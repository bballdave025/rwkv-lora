{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d7b071-3c52-4189-81e4-2fe25bb2f61d",
   "metadata": {},
   "source": [
    "# First Full LoRA Trial with Transformer\n",
    "\n",
    "Starting with going through what I've done as well as finishing the task of getting my LoRA-fine-tuned model from Hugging Face and running inference on it (i.e. testing it using the test set). See the first timestamp below for the new timing. By the way, I've shut down and rebooted the compy here in the corner with the three screens).\n",
    "\n",
    "## peft (for LoRA) and FLAN-T5-small for the LLM\n",
    "\n",
    "I'm following what seems to be a great tutorial from Mehul Gupta,\n",
    "\n",
    "> https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578\n",
    "> \n",
    "> https://web.archive.org/web/20240522140323/https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578\n",
    "\n",
    "I'm doing this to prepare creating a LoRA for RWKV ( <strike>@todo</strike> @DONE  put links in [here](#Notes-Looking-Forward-to-LoRA-on-RWKV) ) so as to fine-tune it for Pat's OLECT-LM stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab0319e-1d1e-4fa3-9153-e8c26344e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "!powershell -c (Get-Date -UFormat \\\"%s_%Y%m%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8476aae1-0e49-4094-8eb8-6bf0d51acbc7",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`1717405690_20240603T090810-0600`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2a1051-3b8f-442b-85f2-c6609ce45369",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "My `environment.yml` file will have its contents listed below. It should have everything needed for an install anywhere. The directory should have a `full_environment.yml`, which includes everything for the environment on Windows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf082676-868d-46ae-8f4b-3ae1d670e076",
   "metadata": {},
   "source": [
    "You can change `do_want_to_read_realtime` to `True` if you really want to see the file as it is now. One case of this would be that you think `environment.yml` has been changed since this notebook was written. The file contents as of the time of my writing this notebook should be in a markdown cell beneath the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d596bcde-28a3-41a7-8ebf-3676c3005c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_want_to_read_realtime = False\n",
    "\n",
    "if do_want_to_read_realtime:\n",
    "    with open(\"environment.yml\", 'r', encoding='utf-8') as fh:\n",
    "        while True:\n",
    "            line = fh.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            ##endof:  if not line\n",
    "            print(line.replace(\"\\n\", \"\"))\n",
    "        ##endof:  while True\n",
    "    ##endof:  with open ... fh\n",
    "##endof:  if do_want_to_read_realtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49530485-e077-49bb-a5ff-a6fa3bf4cb0c",
   "metadata": {},
   "source": [
    "```\n",
    "# @file: environment.yml\n",
    "# @since 2024-06-03\n",
    "## 1717411989_2024-06-03T105309-0600\n",
    "## IMPORTANT NOTES\n",
    "##\n",
    "##  A couple of installations were made from git repos. \n",
    "##     >pip install git+https://github.com/huggingface/peft.git\n",
    "##     >pip install git+https://github.com/nexplorer-3e/qwqfetch\n",
    "##\n",
    "##  The commit info will be important for reproducibility.\n",
    "##\n",
    "##-----\n",
    "##  qwqfetch   for system info\n",
    "##\n",
    "##   Resolved https://github.com/nexplorer-3e/qwqfetch \\\n",
    "##       to commit f72d222e2fff5ffea9f4e4b3a203e4c4d9e8cf00\n",
    "##   Successfully installed qwqfetch-0.0.0\n",
    "##\n",
    "#\n",
    "##-----\n",
    "##  peft: I installed PEFT among other things, but I'm picking out \n",
    "##+       stuff relevant to peft. PEFT has LoRA in it.\n",
    "##\n",
    "##   Resolved https://github.com/huggingface/peft.git \\\n",
    "##       to commit e7b75070c72a88f0f7926cc6872858a2c5f0090d\n",
    "## Successfully built peft\n",
    "#\n",
    "#\n",
    "\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - python=3.10.14\n",
    "  - pip=24.0\n",
    "  - pip:\n",
    "      - accelerate==0.30.1\n",
    "      - bitsandbytes==0.43.1\n",
    "      - datasets==2.19.1\n",
    "      - evaluate==0.4.2\n",
    "      - huggingface-hub==0.23.2\n",
    "      - humanfriendly==10.0\n",
    "      - jupyter==1.0.0\n",
    "      - nltk==3.8.1\n",
    "      - peft==0.11.2.dev0\n",
    "      - py-cpuinfo==9.0.0\n",
    "      - pylspci==0.4.3\n",
    "      - qwqfetch==0.0.0\n",
    "      - rouge-score==0.1.2\n",
    "      - tensorflow-cpu==2.16.1\n",
    "      - torch==2.3.0\n",
    "      - transformers==4.41.1\n",
    "      - trl==0.8.6\n",
    "      - wmi==1.5.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb51da5-6c05-4870-931c-7068e575ac99",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7c2afe-9b5f-4ee6-9db6-9f8d2cf6b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "from random import randrange\n",
    "import torch\n",
    "from transformers import AutoTokenizer, \\\n",
    "                         AutoModelForSeq2SeqLM, \\\n",
    "                         AutoModelForCausalLM, \\\n",
    "                         TrainingArguments, \\\n",
    "                         pipeline\n",
    "from transformers.utils import logging\n",
    "from peft import LoraConfig, \\\n",
    "                 prepare_model_for_kbit_training, \\\n",
    "                 get_peft_model, \\\n",
    "                 AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login, notebook_login\n",
    "\n",
    "from datasets import load_metric\n",
    "from evaluate import load as evaluate_dot_load\n",
    "import nltk\n",
    "import rouge_score\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "\n",
    "import pickle\n",
    "import pprint\n",
    "import re\n",
    "import timeit\n",
    "from humanfriendly import format_timespan\n",
    "import os\n",
    "\n",
    "## my module(s), now just in the working directory as .PY files\n",
    "import system_info_as_script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bfe2f4-b1b4-4158-b3c6-048c487514d2",
   "metadata": {},
   "source": [
    "## Load the training and test dataset along with the LLM with its tokenizer\n",
    "\n",
    "The LLM will be fine-tuned. It seems the tokenizer will also be fine-tuned, \n",
    "but I'm not sure \n",
    "\n",
    "<b>Why aren't we loading the validation set?</b> (I don't know; that's not a teaching question.)\n",
    "\n",
    "I've tried to make use of it (the validation set) with the `trainer`. We'll see how it goes.\n",
    "\n",
    "<b>Update:</b> It worked fine, though its loss is lower than the training set's loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6043d426-bcee-4539-a9ea-4b7c4c3dccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Need to install  datasets  from pip, not conda. I'll do all from pip. \n",
    "#+ I'll get rid of the current conda environment and make it anew.\n",
    "#+ Actually, I'll make sure  conda  and  pip  are updated, then do what\n",
    "#+ I discussed above.\n",
    "#+\n",
    "#+ cf. \n",
    "#+     arch_ref_1 = \"https://web.archive.org/web/20240522150357/\" + \\\n",
    "#+                  \"https://stackoverflow.com/questions/77433096/\" + \\\n",
    "#+                  \"notimplementederror-loading-a-dataset-\" + \\\n",
    "#+                  \"cached-in-a-localfilesystem-is-not-suppor\"\n",
    "#+\n",
    "#+ Also useful might be\n",
    "#+     arch_ref_2 = \"https://web.archive.org/web/20240522150310/\" + \\\n",
    "#+                  \"https://stackoverflow.com/questions/76340743/\" + \\\n",
    "#+                  \"huggingface-load-datasets-gives-\" + \\\n",
    "#+                  \"notimplementederror-cannot-error\"\n",
    "#\n",
    "data_files = {'train':'samsum-train.json', \n",
    "              'evaluation':'samsum-validation.json',\n",
    "              'test':'samsum-test.json'}\n",
    "dataset = load_dataset('json', data_files=data_files)\n",
    "\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "model_load_tic = timeit.default_timer()\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model_load_toc = timeit.default_timer()\n",
    "\n",
    "model_load_duration = model_load_toc - model_load_tic\n",
    "\n",
    "print(f\"Loading the original model, {model_name}\")\n",
    "print(f\"took {model_load_toc - model_load_tic:0.4f} seconds.\")\n",
    "\n",
    "model_load_time_str = format_timespan(model_load_duration)\n",
    "\n",
    "print(f\"which equates to {model_load_time_str}\")\n",
    "\n",
    "#  Next line makes training faster but a little less accurate\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer_tic = timeit.default_timer()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                          trust_remote_code=True)\n",
    "tokenizer_toc = timeit.default_timer()\n",
    "\n",
    "tokenizer_duration = tokenizer_toc - tokenizer_tic\n",
    "\n",
    "print(\"Getting original tokenizer\")\n",
    "print(f\"took {tokenizer_toc - tokenizer_tic:0.4f} seconds.\")\n",
    "\n",
    "tokenizer_time_str = format_timespan(tokenizer_duration)\n",
    "\n",
    "print(f\"which equates to {tokenizer_time_str}\")\n",
    "\n",
    "#  padding instructions for the tokenizer\n",
    "#+   ??? !!! What about for RWKV !!! ???\n",
    "#+ Will it be the same?\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc6b24-d617-48a1-8236-c3be16402c4a",
   "metadata": {},
   "source": [
    "#### Trying some things I've been learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a107cbdc-5a73-4371-8afe-faa2dc623c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aaf9b8-19fe-4255-81bb-47dfa40cfcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arch_str = str(model)\n",
    "\n",
    "with open(\"google_-flan-t5-small.model-architecture.txt\", 'w', encoding='utf-8') as fh:\n",
    "    fh.write(model_arch_str)\n",
    "##endof:  with open ... fh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ee78d-a0ba-47c0-8e5b-1206452e7005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some other saves\n",
    "pickle_filename = \"lora_flan_t5_cpu_objects.pkl\"\n",
    "objects_to_pickle = []\n",
    "objects_to_pickle.append(model_arch_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59156663-4b5b-4b60-8b37-627a9f297fc9",
   "metadata": {},
   "source": [
    "## Prompt and Trainer\n",
    "\n",
    "For our SFT (<b>S</b>upervised <b>F</b>ine <b>T</b>uning) model, we use the `class trl.SFTTrainer`.\n",
    "\n",
    "I want to research this a bit, especially the `formatting_func` that we'll be passing to the `SFTTrainer`.\n",
    "\n",
    "First, though, some information about SFT. From the Hugging Face Documentation at https://huggingface.co/docs/trl/en/sft_trainer ([archived](https://web.archive.org/web/20240529140717/https://huggingface.co/docs/trl/en/sft_trainer))\n",
    "\n",
    "> Supervised fine-tuning (or SFT for short) is a crucial step in RLHF. In TRL we provide an easy-to-use API to create your SFT models and train them with few lines of code on your dataset.\n",
    "\n",
    "Though I won't be using the examples unless I get even more stuck, the next paragraph _has_ examples, and I'll put the paragraph here.\n",
    "\n",
    "> Check out a complete flexible example at [examples/scripts/sft.py](https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py) \\[[archived](https://web.archive.org/web/20240529140740/https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py)\\]. Experimental support for Vision Language Models is also included in the example [examples/scripts/vsft_llava.py](https://github.com/huggingface/trl/blob/main/examples/scripts/vsft_llava.py) \\[[archived](https://web.archive.org/web/20240529140738/https://github.com/huggingface/trl/blob/main/examples/scripts/vsft_llava.py)\\].\n",
    "\n",
    "RLHF ([archived wikipedia page](https://web.archive.org/web/20240529142205/https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback)) is <b>R</b>einforcement <b>L</b>earning from <b>H</b>uman <b>F</b>eedback. [TRL](https://huggingface.co/docs/trl/en/index#:~:text=TRL%20is%20a%20full%20stack,Policy%20Optimization%20(PPO)%20step.) ([archived]())       <b>T</b>ransfer <b>R</b>einforcement <b>L</b>earning, a library from Hugging Face.\n",
    "\n",
    "For the parameter, `formatting_func`, I can look ath the documentation site above (specifically [here](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=formatting_func%20(Optional)), at the GitHub repo for [the code](https://github.com/huggingface/trl/blob/main/trl/trainer/sft_trainer.py) (in the docstrings), or from my local `conda` environment, at `C:\\Users\\bballdave025\\.conda\\envs\\rwkv-lora-pat\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py`.\n",
    "\n",
    "Pulling code from the last one, I get\n",
    "\n",
    ">         formatting_func (`Optional[Callable]`):\n",
    ">            The formatting function to be used for creating the `ConstantLengthDataset`.\n",
    "\n",
    "That matches the first very well\n",
    "\n",
    "> <b>formatting_func</b> (`Optional[Callable]`) — The formatting function to be used for creating the `ConstantLengthDataset`.\n",
    "\n",
    "(A quick note: In this Jupyter Notebook environment, I could have typed `trainer = SFTTrainer(` and then <kbd>Shift</kbd> + <kbd>Tab</kbd> to find that same documentation.\n",
    "\n",
    "However, I think that more clarity is found at the [documentation for `ConstantLengthDataset](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=class%20trl.trainer.ConstantLengthDataset)\n",
    "\n",
    "> <b>formatting_func</b> (`Callable`, <b>optional</b>) — Function that formats the text before tokenization. Usually it is recommended to have follows a certain pattern such as `\"### Question: {question} ### Answer: {answer}\"`\n",
    "\n",
    "So, as we'll see the next code from  the tutorial, it basically is a prompt templater/formatter that matches the JSON. For example, we use `sample['dialogue']` to access the `dialogue` key/pair. That's what I got from all this stuff.\n",
    "\n",
    "Mehul Gupta himself stated\n",
    "\n",
    "> Next, using the Input and Output, we will create a prompt template which is a requirement by the SFTTrainer we will be using later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cdd3ac-f714-41ed-a1fb-c27395b9251d",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb36fc-783f-4f19-b213-69cc3dbf05af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_instruction_format(sample):\n",
    "    return f\"\"\" Instruction:\n",
    "      Use the Task below and the Input given to write the Response:\n",
    "\n",
    "      ### Task:\n",
    "      Summarize the Input\n",
    "\n",
    "      ### Input:\n",
    "      {sample['dialogue']}\n",
    "\n",
    "      ### Response:\n",
    "      {sample['summary']}\n",
    "      \"\"\"\n",
    "##endof:  prompt_instruction_format(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3622673e-df80-43da-9d5c-efd6085d301d",
   "metadata": {},
   "source": [
    "### Trainer - the LoRA Setup Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da39f5e-4e14-4eb5-ac66-c90a77472c64",
   "metadata": {},
   "source": [
    "#### Arguments and Configuration\n",
    "\n",
    "See [this section](#The-final-TrainingArguments-call---with-parameter-list) to see what I changed from the tutorial to get the evaluation set as part of training and to get a customized repo name. The couple of sections before it will give more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac040cf-2eb5-4eb5-91c7-2e47c54b990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Some arguments to pass to the trainer\n",
    "training_args = TrainingArguments( \n",
    "                        output_dir='output',\n",
    "                        num_train_epochs=1,\n",
    "                        per_device_train_batch_size=4,\n",
    "                        save_strategy='epoch',\n",
    "                        learning_rate=2e-4,\n",
    "                        do_eval=True,\n",
    "                        per_device_eval_batch_size=4,\n",
    "                        eval_strategy='epoch',\n",
    "                        hub_model_id=\"dwb-flan-t5-small-lora-finetune\",\n",
    ")\n",
    "\n",
    "# the fine-tuning (peft for LoRA) stuff\n",
    "peft_config = LoraConfig( lora_alpha=16,\n",
    "                          lora_dropout=0.1,\n",
    "                          r=64,\n",
    "                          bias='none',\n",
    "                          task_type='CAUSAL_LM'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa2a84-c5f6-45ad-9d09-fb9f25fae9da",
   "metadata": {},
   "source": [
    "`task_type`, cf. https://github.com/huggingface/peft/blob/main/src/peft/config.py#L222 ([archived](https://web.archive.org/web/20240603151908/https://github.com/huggingface/peft/blob/main/src/peft/config.py))\n",
    "\n",
    ">        Args:\n",
    ">            peft_type (Union[[`~peft.utils.config.PeftType`], `str`]): The type of Peft method to use.\n",
    ">            task_type (Union[[`~peft.utils.config.TaskType`], `str`]): The type of task to perform.\n",
    ">            inference_mode (`bool`, defaults to `False`): Whether to use the Peft model in inference mode.\n",
    "\n",
    "After some searching using Cygwin\n",
    "\n",
    "```\n",
    "bballdave025@MYMACHINE /cygdrive/c/Users/bballdave025/.conda/envs/rwkv-lora-pat/Lib/site-packages/peft/utils\n",
    "$ ls -lah\n",
    "total 116K\n",
    "drwx------+ 1 bballdave025 bballdave025    0 May 28 21:09 .\n",
    "drwx------+ 1 bballdave025 bballdave025    0 May 28 21:09 ..\n",
    "-rwx------+ 1 bballdave025 bballdave025 2.0K May 28 21:09 __init__.py\n",
    "drwx------+ 1 bballdave025 bballdave025    0 May 28 21:09 __pycache__\n",
    "-rwx------+ 1 bballdave025 bballdave025 8.0K May 28 21:09 constants.py\n",
    "-rwx------+ 1 bballdave025 bballdave025 3.8K May 28 21:09 integrations.py\n",
    "-rwx------+ 1 bballdave025 bballdave025  17K May 28 21:09 loftq_utils.py\n",
    "-rwx------+ 1 bballdave025 bballdave025 9.7K May 28 21:09 merge_utils.py\n",
    "-rwx------+ 1 bballdave025 bballdave025  25K May 28 21:09 other.py\n",
    "-rwx------+ 1 bballdave025 bballdave025 2.2K May 28 21:09 peft_types.py\n",
    "-rwx------+ 1 bballdave025 bballdave025  21K May 28 21:09 save_and_load.py\n",
    "\n",
    "bballdave025@MYMACHINE /cygdrive/c/Users/bballdave025/.conda/envs/rwkv-lora-pat/Lib/site-packages/peft/utils\n",
    "$ grep -iIRHn \"TaskType\" .\n",
    "peft_types.py:60:class TaskType(str, enum.Enum):\n",
    "__init__.py:20:# from .config import PeftConfig, PeftType, PromptLearningConfig, TaskType\n",
    "__init__.py:22:from .peft_types import PeftType, TaskType\n",
    "\n",
    "bballdave025@MYMACHINE /cygdrive/c/Users/bballdave025/.conda/envs/rwkv-lora-pat/Lib/site-packages/peft/utils\n",
    "$\n",
    "```\n",
    "\n",
    "So, let's look at the `peft_types.py` file.\n",
    "\n",
    "The docstring for `class TaskType(str, enum.Enum)` is\n",
    "\n",
    "```\n",
    "    Enum class for the different types of tasks supported by PEFT.\n",
    "    \n",
    "    Overview of the supported task types:\n",
    "    - SEQ_CLS: Text classification.\n",
    "    - SEQ_2_SEQ_LM: Sequence-to-sequence language modeling.\n",
    "    - CAUSAL_LM: Causal language modeling.\n",
    "    - TOKEN_CLS: Token classification.\n",
    "    - QUESTION_ANS: Question answering.\n",
    "    - FEATURE_EXTRACTION: Feature extraction. Provides the hidden states which can be used as embeddings or features\n",
    "      for downstream tasks.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e178f3f-9dd6-409e-9a4d-c1c85611a8b9",
   "metadata": {},
   "source": [
    "### We're going to start timing stuff, so here's some system info\n",
    "\n",
    "`system_info_as_script.py` is a script I wrote with the help\n",
    "of a variety of StackOverflow and documentation sources.\n",
    "It should be in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14193c4-9040-47ea-9504-37ccb408de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "!powershell -c (Get-Date -UFormat \\\"%s_%Y%m%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e51b0-8f09-4693-b991-e853ca3cfccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_info_as_script.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161c58b-3ef9-40e4-bcc4-eca0daab17e1",
   "metadata": {},
   "source": [
    "Before I rebooted (and I did reboot this morning by shutting down and restarting, though the script output it still shows the reboot date as `2024-5-26` and the uptime as `7 days, 18 hours, 53 minutes`), I ran this from an elevated command prompt. The result are in the file,\n",
    "\n",
    "[`system_info_win_compy_admin_2024-06-03T070700-0600.txt`](./system_info_win_compy_admin_2024-06-03T070700-0600.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b9483-f8ff-47d7-ad94-b833f27d6e9e",
   "metadata": {},
   "source": [
    "### ROUGE Metrics\n",
    "\n",
    "Some references from the Microsoft/Google (who?) implementation\n",
    "\n",
    "https://pypi.org/project/rouge-score/\n",
    "\n",
    "https://web.archive.org/web/20240530231357/https://pypi.org/project/rouge-score/\n",
    "\n",
    "<br/>\n",
    "\n",
    "https://github.com/google-research/google-research/tree/master/rouge\n",
    "\n",
    "https://web.archive.org/web/20240530231412/https://github.com/google-research/google-research/tree/master/rouge\n",
    "\n",
    "<br/>\n",
    "\n",
    "Not the one I used:\n",
    "\n",
    "https://github.com/microsoft/nlp-recipes/blob/master/examples/text_summarization/summarization_evaluation.ipynb\n",
    "\n",
    "https://web.archive.org/web/20240530231709/https://github.com/microsoft/nlp-recipes/blob/master/examples/text_summarization/summarization_evaluation.ipynb\n",
    "\n",
    "<br/>\n",
    "\n",
    "Someone else made this other one, which I inspected but didn't use.\n",
    "\n",
    "https://pypi.org/project/rouge/\n",
    "\n",
    "https://web.archive.org/web/20240530232029/https://pypi.org/project/rouge/\n",
    "\n",
    "https://github.com/pltrdy/rouge\n",
    "\n",
    "https://web.archive.org/web/20240530232023/https://github.com/pltrdy/rouge\n",
    "\n",
    "but I think he defers to the rouge_score from Google."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907cdf67-55ac-41c6-a81c-7e89b4ddade8",
   "metadata": {},
   "source": [
    "#### My ROUGE Metrics\n",
    "\n",
    "I want to use the skip-grams score. Thanks to\n",
    "\n",
    "https://www.bomberbot.com/machine-learning/skip-bigrams-in-system/\n",
    "\n",
    "https://web.archive.org/web/20240530230949/https://www.bomberbot.com/machine-learning/skip-bigrams-in-system/\n",
    "\n",
    "I can do this as well as writing the code for the other metrics.\n",
    "\n",
    "##### Not used for now\n",
    "\n",
    "Focusing on the main goal. Quick and Reckless. My therapist would be so proud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9287dd25-f5fe-45a2-b47c-7f8baaea0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dwb_rouge_scores\n",
    "\n",
    "#help(dwb_rouge_scores.dwb_rouge_n)\n",
    "\n",
    "# print(\"SEPARATOR\")\n",
    "\n",
    "#help(dwb_rouge_scores.dwb_rouge_L)\n",
    "\n",
    "# print(\"SMALLER-SEPARATOR\\nwhich needs\")\n",
    "\n",
    "#help(dwb_rouge_scores.dwb_lcs)\n",
    "\n",
    "# print(\"SEPARATOR\")\n",
    "\n",
    "#help(dwb_rouge_scores.dwb_rouge_s)\n",
    "\n",
    "# print(\"SMALLER-SEPARATOR\\nwhich needs\")\n",
    "\n",
    "#help(dwb_rouge_scores.dwb_skipngrams)\n",
    "\n",
    "# print(\"SEPARATOR\")\n",
    "\n",
    "#help(dwb_rouge_scores.dwb_rouge_Lsum)\n",
    "\n",
    "# print(\"which just wraps google-research's rouge_score's version\")\n",
    "\n",
    "#help()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a84433-dbb3-4e97-9b51-e0074b020f1c",
   "metadata": {},
   "source": [
    "#### Other useful ROUGE code\n",
    "\n",
    "(found as I go along)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eb2eab-ee60-4970-9b32-314439e1b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_rouge_score_rough(this_rouge_str,\n",
    "                             do_debug_rouge_fmt=False):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    \n",
    "    rouge_ret_str = this_rouge_str\n",
    "    \n",
    "    if do_debug_rouge_fmt:\n",
    "        print(\" #DEBUG 1#\")\n",
    "        print(rouge_ret_str)\n",
    "    ##endof:  do_debug_rouge_fmt\n",
    "    \n",
    "    rouge_ret_str = re.sub(r\"([(,][ ]?)([0-9A-Za-z_]+[=])\",\n",
    "                            \"\\g<1>\\n     \\g<2>\",\n",
    "                           rouge_ret_str,\n",
    "                           flags=re.I|re.M\n",
    "    )\n",
    "    \n",
    "    if do_debug_rouge_fmt:\n",
    "        print(\" #DEBUG 2#\")\n",
    "        print(rouge_ret_str)\n",
    "    ##endof:  do_debug_rouge_fmt\n",
    "    \n",
    "    rouge_ret_str = re.sub(r\"(.)([)])$\",\n",
    "                            \"\\g<1>\\n\\g<2>\",\n",
    "                           rouge_ret_str\n",
    "    )\n",
    "    \n",
    "    if do_debug_rouge_fmt:\n",
    "        print(\" #DEBUG 3#\")\n",
    "        print(rouge_ret_str)\n",
    "    ##endof:  do_debug_rouge_fmt\n",
    "\n",
    "    rouge_ret_str = rouge_ret_str.replace(\n",
    "                                   \"precision=\",\n",
    "                                   \"     precision=\"\n",
    "                                ).replace(\n",
    "                                   \"recall=\",\n",
    "                                   \"     recall=\"\n",
    "                                ).replace(\n",
    "                                   \"fmeasure=\",\n",
    "                                   \"     fmeasure=\"\n",
    "    )\n",
    "    \n",
    "    return rouge_ret_str\n",
    "    \n",
    "##endof:  format_rouge_score_rough(<params>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fbbbdc-e2f8-4bf4-a3e7-e3b409c82f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rouge_scores(result, sample_num=None):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    \n",
    "    print(\"\\n\\n---------- ROUGE SCORES ----------\")\n",
    "    if sample_num is None:\n",
    "        print(\"  --------- dialogue ----------\")\n",
    "    elif type(sample_num) is int:\n",
    "        print(f\"  --------- dialogue {sample_num+1} ----------\")\n",
    "    else:\n",
    "        print(f\"  --------- {sample_num} ----------\")\n",
    "    ##endof:  if/else sample_num is None\n",
    "    print(\"ROUGE-1 results\")\n",
    "    rouge1_str = str(result['rouge1'])\n",
    "    print(format_rouge_score_rough(rouge1_str))\n",
    "    print(\"ROUGE-2 results\")\n",
    "    rouge2_str = str(result['rouge2'])\n",
    "    print(format_rouge_score_rough(rouge2_str))\n",
    "    print(\"ROUGE-L results\")\n",
    "    rougeL_str = str(result['rougeL'])\n",
    "    print(format_rouge_score_rough(rougeL_str))\n",
    "    print(\"ROUGE-Lsum results\")\n",
    "    rougeLsum_str = str(result['rougeLsum'])\n",
    "    print(format_rouge_score_rough(rougeLsum_str))\n",
    "##endof:  print_rouge_scores(<params>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afae864-111f-48e0-b477-f8583442b685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# #  From https://github.com/google-research/google-research/tree/master/rouge\n",
    "# #+ <strike>I can't see how to aggregate it, though I may have</strike>\n",
    "# #+ I found a resource at\n",
    "# #+  ref_gg_rg=\"https://github.com/huggingface/datasets/blob/\" + \\\n",
    "# #+            \"main/metrics/rouge/rouge.py\"\n",
    "# #+\n",
    "# #+ arch_gg_rg=\"https://web.archive.org/web/20240603192938/\" + \\\n",
    "# #+            \"https://github.com/huggingface/datasets/blob/\" + \\\n",
    "# #+            \"main/metrics/rouge/rouge.py\"\n",
    "#\n",
    "def compute_google_rouge_score(predictions, \n",
    "                               references, \n",
    "                               rouge_types=None, \n",
    "                               use_aggregator=True, \n",
    "                               use_stemmer=False):\n",
    "    '''\n",
    "    Figuring out the nice format of the deprecated method\n",
    "    '''\n",
    "    if rouge_types is None:\n",
    "        rouge_types = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "    ##endof:  if rouge_types is None\n",
    "    scorer = rouge_scorer.RougeScorer(rouge_types=rouge_types, \n",
    "                                      use_stemmer=use_stemmer\n",
    "    )\n",
    "    if use_aggregator:\n",
    "        aggregator = scoring.BootstrapAggregator()\n",
    "    else:\n",
    "        scores = []\n",
    "    ##endof:  if/else use_aggregator\n",
    "    for ref, pred in zip(references, predictions):\n",
    "        score = scorer.score(ref, pred)\n",
    "        if use_aggregator:\n",
    "            aggregator.add_scores(score)\n",
    "        else:\n",
    "            scores.append(score)\n",
    "    ##endof:  for\n",
    "    if use_aggregator:\n",
    "        result = aggregator.aggregate()\n",
    "    else:\n",
    "        result = {}\n",
    "        for key in scores[0]:\n",
    "            result[key] = [score[key] for score in scores]\n",
    "        ##endof:  for\n",
    "    ##endof:  if/else\n",
    "    return result\n",
    "##endof:  compute_google_rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2c07f3-1066-48ac-a243-8ec5af8f528e",
   "metadata": {},
   "source": [
    "Extra cell.\n",
    "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5cd858-f6e7-4dba-b689-75e3d950f26d",
   "metadata": {},
   "source": [
    "### Try for a baseline (for out-of-the-box, pretrained model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba0b4db-cd1c-466c-91be-cc0e2a8a14c1",
   "metadata": {},
   "source": [
    "#### Just one summarization to begin with, randomly picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68250f44-daf6-436e-a7d2-dcb7a8d97ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Just one summarization to begin with, randomly picked ... but\n",
    "#+ now with th possibility of a known seed, to allow visual \n",
    "#+ comparison with after-training results.\n",
    "#+ I'M NOT GOING TO USE THIS REPEATED SEED, I'm just going to\n",
    "#+ use the datum at the first index to compare.\n",
    "\n",
    "do_seed_for_repeatable = True\n",
    "\n",
    "summarizer = pipeline('summarization', \n",
    "                      model=model, \n",
    "                      tokenizer=tokenizer)\n",
    "\n",
    "if do_seed_for_repeatable:\n",
    "    rand_seed_for_randrange = 137\n",
    "    random.seed(rand_seed_for_randrange)\n",
    "##endof:  if do_seed_for_repeatable\n",
    "\n",
    "sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    "\n",
    "res = summarizer(sample[\"dialogue\"])\n",
    "\n",
    "print(f\"flan-t5-small summary:\\n{res[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f318d3d-7593-495e-810d-7937ce085d08",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21170b7-0685-4e39-b9d6-2eaf5624d476",
   "metadata": {},
   "source": [
    "#### Now, a couple summarizations with comparison to ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a760e7-6270-4e65-93f0-1af0c3dd559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_list = []\n",
    "ref_test_list = []\n",
    "\n",
    "sample_num = 0\n",
    "\n",
    "this_sample = dataset['test'][sample_num]\n",
    "\n",
    "print(f\"dialogue: \\n{this_sample['dialogue']}\\n---------------\")\n",
    "\n",
    "ground_summary = this_sample['summary']\n",
    "res = summarizer(this_sample['dialogue'])\n",
    "res_summary = res[0]['summary_text']\n",
    "\n",
    "print(f\"human-genratd summary:\\n{ground_summary}\")\n",
    "print(f\"flan-t5-small summary:\\n{res_summary}\")\n",
    "\n",
    "ref_test_list.append(ground_summary)\n",
    "pred_test_list.append(res_summary)\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#  datasets.load_metric\n",
    "#+ Supposed to be deprecated, but it's the only one I found that aggregates \n",
    "#+ the scores. Also, it gives more than just an f-score\n",
    "rouge = load_metric('rouge', trust_remote_code=True)\n",
    "\n",
    "#  Yes, I have just one datum, but I'm setting things up to\n",
    "#+ work well with a loop.\n",
    "results_test_0 = rouge.compute(\n",
    "                    predictions=pred_test_list,\n",
    "                    references=ref_test_list,\n",
    "                    use_aggregator=False\n",
    ")\n",
    "\n",
    "# >>> print(list(results_test.keys()))\n",
    "# ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b834a5f-057f-43a8-a16b-ddaaa70b3244",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ab55d2-33b6-4c6f-802f-2ab97e0f8b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rouge_scores(results_test_0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c97b482-020c-47f9-834f-50000cd594b1",
   "metadata": {},
   "source": [
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84099d2-41e4-4266-a5e9-b7b7cac59504",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 224\n",
    "\n",
    "this_sample = dataset['test'][sample_num]\n",
    "\n",
    "print(f\"dialogue: \\n{this_sample['dialogue']}\\n---------------\")\n",
    "\n",
    "ground_summary = this_sample['summary']\n",
    "res = summarizer(this_sample['dialogue'])\n",
    "res_summary = res[0]['summary_text']\n",
    "\n",
    "print(f\"human-genratd summary:\\n{ground_summary}\")\n",
    "print(f\"flan-t5-small summary:\\n{res_summary}\")\n",
    "\n",
    "# Now, we'll have two data\n",
    "ref_test_list = [ground_summary]\n",
    "pred_test_list = [res_summary]\n",
    "\n",
    "results_test_224 = rouge.compute(\n",
    "                      predictions=pred_test_list,\n",
    "                      references=ref_test_list,\n",
    "                      use_aggregator=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575767f6-56ed-4480-aa9e-117c22a83f24",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf7d444-6d03-400c-b7ac-551d6aa4f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rouge_scores(results_test_224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c32f25d-8168-4e86-b011-fbad9bf49ad7",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f9c213-e96b-48de-8050-4eff2b571695",
   "metadata": {},
   "source": [
    "##### Note on ROUGE Scores\n",
    "\n",
    "```\n",
    "# @todo : Run the ROUGE analysis from the Python package\n",
    "#         (after running with  trust_remote_code=False\n",
    "#          to find the deprecation it mentioned).\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# #  From https://github.com/google-research/google-research/tree/master/rouge\n",
    "# #+ I can't see how to aggregate it, though I may have found a resource at\n",
    "# #+  ref_gg_rg=\"https://github.com/huggingface/datasets/blob/\" + \\\n",
    "# #+            \"main/metrics/rouge/rouge.py\"\n",
    "# #+\n",
    "# #+ arch_gg_rg=\"https://web.archive.org/web/20240603192938/\" + \\\n",
    "# #+            \"https://github.com/huggingface/datasets/blob/\" + \\\n",
    "# #+            \"main/metrics/rouge/rouge.py\"\n",
    "#\n",
    "\n",
    "#  It turns out that the deprecated one is preferable in \n",
    "#+ output, at least until I can debug the aggregation of\n",
    "#+ scores with another version: compute_google_rouge_score\n",
    "```\n",
    "\n",
    "That should come from the `compute_google_rouge_score`, above. I was able\n",
    "to look through the code for `datasets.load_metric('rouge')` code and\n",
    "put together that method.\n",
    "\n",
    "\n",
    "For now, I used ...\n",
    "\n",
    "```\n",
    "# Using the deprecated-but-aggregating-and-not-only-f-score one\n",
    "rouge = load_metric('rouge', trust_remote_code=False)\n",
    "```\n",
    "\n",
    "\n",
    "This next one is what the warning message said to use, but it only returns\n",
    "an f-measure (f-score)\n",
    "\n",
    "```\n",
    "# #  Replacement for the load_metric - evaluate.load(metric_name)\n",
    "# #+ Docs said:\n",
    "# #+\n",
    "# #+> Returns:\n",
    "# #+>    rouge1: rouge_1 (f1),\n",
    "# #+>    rouge2: rouge_2 (f1),\n",
    "# #+>    rougeL: rouge_l (f1),\n",
    "# #+>    rougeLsum: rouge_lsum (f1)\n",
    "# #+>\n",
    "# #+> Meaning we only get the f-score. I want more to compare.\n",
    "# #-v- code \n",
    "# rouge = evaluate_dot_load('rouge')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f099cc0c-bce3-49e5-8ca8-afd212adbc70",
   "metadata": {},
   "source": [
    "#### Verbosity stuff - get rid of the nice advice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7a763-7427-4054-ab79-ee8d0931f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "!powershell -c (Get-Date -UFormat \\\"%s_%Y%m%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb761d-d836-49a4-b585-29aae74f0c86",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`1717411179_20240603T103939-0600`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49d3a5-b58e-468a-836a-84a3908fd27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_verbosity_is_critical = \\\n",
    "  logging.get_verbosity() == logging.CRITICAL # alias FATAL, 50\n",
    "log_verbosity_is_error = \\\n",
    "  logging.get_verbosity() == logging.ERROR # 40\n",
    "log_verbosity_is_warn = \\\n",
    "  logging.get_verbosity() == logging.WARNING # alias WARN, 30\n",
    "log_verbosity_is_info = \\\n",
    "  logging.get_verbosity() == logging.INFO # 20\n",
    "log_verbosity_is_debug = \\\n",
    "  logging.get_verbosity() == logging.DEBUG # 10\n",
    "\n",
    "print( \"The statement, 'logging verbosity is CRITICAL' \" + \\\n",
    "      f\"is {log_verbosity_is_critical}\")\n",
    "print( \"The statement, 'logging verbosity is    ERROR' \" + \\\n",
    "      f\"is {log_verbosity_is_error}\")\n",
    "print( \"The statement, 'logging verbosity is  WARNING' \" + \\\n",
    "      f\"is {log_verbosity_is_warn}\")\n",
    "print( \"The statement, 'logging verbosity is     INFO' \" + \\\n",
    "      f\"is {log_verbosity_is_info}\")\n",
    "print( \"The statement, 'logging verbosity is    DEBUG' \" + \\\n",
    "      f\"is {log_verbosity_is_debug}\")\n",
    "\n",
    "print()\n",
    "\n",
    "init_log_verbosity = logging.get_verbosity()\n",
    "print(f\"The value of logging.get_verbosity() is: {init_log_verbosity}\")\n",
    "\n",
    "print()\n",
    "\n",
    "init_t_n_a_w = os.environ.get('TRANSFORMERS_NO_ADVISORY_WARNINGS')\n",
    "print(f\"TRANSFORMERS_NO_ADIVSORY_WARNINGS: {init_t_n_a_w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a66b11-e35e-4663-9bcc-602bd3cd060d",
   "metadata": {},
   "source": [
    "### Actual Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671b535-247a-4510-9a09-d8db4fbeab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "# !powershell -c (Get-Date -UFormat \\\"%s_%Y%m%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5dca6b-d4ca-440e-bdd2-71869263c6d7",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`1717411242_20240603T104042-0600`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08031760-1551-41f3-8a1e-1a4b8fef5513",
   "metadata": {},
   "source": [
    "<b>!!! NOTE</b> You'd better <b>make \n",
    "dang sure you want the lots of output</b> \n",
    "before you set this next boolean to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af21e6ae-43c6-4ea5-97e2-019d11e31abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_have_lotta_output_from_all_dialogs_summaries_1 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3db0ad-b6df-4124-aad7-74be50b62b9a",
   "metadata": {},
   "source": [
    "# Are you sure about the value of that last boolean? 1\n",
    "\n",
    "There could be megabytes (maybe gigabytes) worth of text output if you've changed it to `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c6a677-f405-4dc7-b42f-07976a2aa00e",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a86024-3f8f-4eb3-9377-1543c99e2a37",
   "metadata": {},
   "source": [
    "### Actual Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c550c2-eee7-4393-9f92-1a25f4fcc2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ref1 = \"https://web.archive.org/web/20240530051418/\" + \\\n",
    "#+        \"https://stackoverflow.com/questions/73221277/\" + \\\n",
    "#+        \"python-hugging-face-warning\"\n",
    "#  ref2 = \"https://web.archive.org/web/20240530051559/\" + \\\n",
    "#+        \"https://huggingface.co/docs/transformers/en/\" + \\\n",
    "#+        \"main_classes/logging\"\n",
    "\n",
    "##  Haven't tried this, because the logging seemed easier,\n",
    "##+ and the logging worked\n",
    "#os.environ(\"TRANSFORMERS_NO_ADVISORY_WARNINGS\") = 1\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "summarizer = pipeline('summarization', \n",
    "                      model=model, \n",
    "                      tokenizer=tokenizer)\n",
    "\n",
    "#*p*#baseline_sample_dialog_list = []\n",
    "baseline_prediction_list = []\n",
    "baseline_reference_list = []\n",
    "\n",
    "baseline_tic = timeit.default_timer()\n",
    "\n",
    "for sample_num in range(len(dataset['test'])):\n",
    "    this_sample = dataset['test'][sample_num]\n",
    "    \n",
    "    if do_have_lotta_output_from_all_dialogs_summaries_1:\n",
    "        print(f\"dialogue: \\n{this_sample['dialogue']}\\n---------------\")\n",
    "    ##endof:  if do_have_lotta_output_from_all_dialogs_summaries_1\n",
    "    \n",
    "    ground_summary = this_sample['summary']\n",
    "    res = summarizer(this_sample['dialogue'])\n",
    "    res_summary = res[0]['summary_text']\n",
    "    \n",
    "    if do_have_lotta_output_from_all_dialogs_summaries_1:\n",
    "        print(f\"human-genratd summary:\\n{ground_summary}\")\n",
    "        print(f\"flan-t5-small summary:\\n{res_summary}\")\n",
    "    ##endof:  if do_have_lotta_output_from_all_dialogs_summaries_1\n",
    "    \n",
    "    #*p*#    baseline_sample_dialog_list.append(this_sample)\n",
    "    baseline_reference_list.append(ground_summary)\n",
    "    baseline_prediction_list.append(res_summary)\n",
    "##endof:  for sample_num in range(len(dataset['test']))\n",
    "\n",
    "baseline_toc = timeit.default_timer()\n",
    "\n",
    "baseline_duration = baseline_toc - baseline_tic\n",
    "\n",
    "print( \"Getting things ready for scoring\")\n",
    "print(f\"took {baseline_toc - baseline_tic:0.4f} seconds.\")\n",
    "\n",
    "#  It turns out that the deprecated one is preferable in \n",
    "#+ output, at least until I can debug the aggregation of\n",
    "#+ scores with another version\n",
    "#+ That should come with the\n",
    "\n",
    "rouge = load_metric('rouge', trust_remote_code=False)\n",
    "\n",
    "baseline_results = rouge.compute(\n",
    "                      predictions=baseline_prediction_list,\n",
    "                      references=baseline_reference_list,\n",
    "                      use_aggregator=True\n",
    ")\n",
    "\n",
    "# >>> print(list(baseline_results.keys()))\n",
    "# ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "\n",
    "#*p*# objects_to_pickle.append(baseline_sample_dialog_list)\n",
    "#*p*# objects_to_pickle.append(baseline_prediction_list)\n",
    "#*p*# objects_to_pickle.append(baseline_reference_list)\n",
    "#*p*# objects_to_pickle.append(baseline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac9cc41-8657-4813-a0a6-a2aab23f875f",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bda0d0c-0d62-4ad4-aa66-a05895732720",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rouge_scores(baseline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb88143c-de3f-4b95-8863-54dd3b1b419d",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7144dba-f685-492d-a33a-5da8fb5a2acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Haven't tried this, because the logging seemed easier,\n",
    "##+ and the logging worked\n",
    "# os.environ(\"TRANSFORMERS_NO_ADVISORY_WARNINGS\") = init_t_n_a_w\n",
    "\n",
    "logging.set_verbosity(init_log_verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4df533-4ce4-45d6-9966-50cc2d862108",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_enter_duration_manually = False\n",
    "NUM_TO_CATCH_NO_MANUAL_ENTRY = -137.\n",
    "is_a_manual_entry_skip = False # innocent until proven guilty\n",
    "\n",
    "\n",
    "if do_enter_duration_manually:\n",
    "    # !!! remember to type in your number, if needed !!! #\n",
    "    baseline_duration = NUM_TO_CATCH_NO_MANUAL_ENTRY\n",
    "    # !!! UNCOMMENT THE NEXT LINE IF YOU WANT TO ENTER MANUALLY !!!\n",
    "    #baseline_duration = 1162.5236\n",
    "##endof:  if do_enter_duration_manually\n",
    "\n",
    "print(\"Running baseline inference (using the test set)\")\n",
    "if ( ( do_enter_duration_manually ) and \\\n",
    "     ( baseline_duration == -137. ) \\\n",
    "   ):\n",
    "    print(\"took AN UNKNOWN AMOUNT OF TIME.\")\n",
    "    print(\"You didn't manually enter in your real time,\")\n",
    "    print(\"as you should have.\")\n",
    "    is_a_manual_entry_skip = True\n",
    "elif ( ( do_enter_duration_manually ) and \\\n",
    "       ( baseline_duration != -137. ) \n",
    "     ):\n",
    "    print(\"(and using your manually entered time)\")\n",
    "else:\n",
    "    pass\n",
    "##endof:  if <check manual entry>\n",
    "\n",
    "if not is_a_manual_entry_skip:\n",
    "    print(f\"took {format_timespan(baseline_duration)}\")\n",
    "##endof:  if not is_a_manual_entry_skip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df187315-53a9-4f66-a9f3-17db67512ddd",
   "metadata": {},
   "source": [
    "### Trainer - the Actual Trainer Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc41e49-b3b8-4ea0-a3e0-aa40e4860be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "# !powershell -c (Get-Date -UFormat \\\"%s_%Y-%m-%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5503fa3b-09b3-4b6e-b18c-3fb9791e715f",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`1717096214_2024-05-30T191014-0600`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4feb5c-bb3c-444d-8814-6efa2317ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer( model=model,\n",
    "                      train_dataset=dataset['train'],\n",
    "                      eval_dataset=dataset['evaluation'],\n",
    "                      peft_config=peft_config,\n",
    "                      tokenizer=tokenizer,\n",
    "                      packing=True,\n",
    "                      formatting_func=prompt_instruction_format,\n",
    "                      args=training_args,\n",
    "                    )\n",
    "##  Warnings are below output.\n",
    "\n",
    "##  Ended up not using this.\n",
    "#                      max_seq_length=675\n",
    "#          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25010c2f-3f36-47b8-aad4-d5beec083d7c",
   "metadata": {},
   "source": [
    "First time warnings from the code above (as it still is).\n",
    "\n",
    "        \n",
    ">        WARNING:bitsandbytes.cextension:The installed version of bitsandbytes \\\n",
    ">         was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, \\\n",
    ">         and GPU quantization are unavailable.\n",
    ">        C:\\Users\\bballdave025\\.conda\\envs\\rwkv-lora-pat\\lib\\site-packages\\trl\\\\\n",
    ">         trainer\\sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` \\\n",
    ">        argument to the SFTTrainer, this will default to 512\n",
    ">         warnings.warn(\n",
    ">        \n",
    ">        [ > Generating train split: 6143/0 [00:04<00:00, 2034.36 examples/s] ]\n",
    ">        \n",
    ">        Token indices sequence length is longer than the specified maximum sequence \\\n",
    ">         length for this model (657 > 512). Running this sequence through the model \\\n",
    ">         will result in indexing errors\n",
    ">        \n",
    ">        [ > Generating train split: 355/0 [00:00<00:00, 6.10 examples/s] ]\n",
    "\n",
    "<b>DWB Note</b> and possible\n",
    "\n",
    "\\# @todo:\n",
    "\n",
    "<strike>So, I'm changing the `max_seq_length`.</strike> \n",
    "Maybe I should just throw out the offender(s) \n",
    "(along with the blank one that's in there somewhere),\n",
    "but I'll just continue as is.\n",
    "\n",
    "Actually, it appears I didn't run the updated cell, \n",
    "(with `max_seq_length=675`), since the\n",
    "Warning and Advice are still there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d556f5-92e7-4bdc-bbd0-73d643050446",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "## Let's Train This LoRA Thing and See How It Does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376a82f8-7f45-454c-b301-a591298171c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "# !powershell -c (Get-Date -UFormat \\\"%s_%Y-%m-%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd1014-d324-4293-930d-0ead8fdba381",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`1717096271_2024-05-30T191111-0600`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c6264-aab2-4a36-aa8b-71aa69c9719f",
   "metadata": {},
   "source": [
    "At about `1717063394_2024-05-30T100314-0600`, DWB went in and \n",
    "renamed `profile.ps1` to `NOT-USING_-_pro_file_-_now.ps1.bak`\n",
    "That should get rid of our errors from `powershell`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c15de9-adb4-433d-8a01-e633571bb076",
   "metadata": {},
   "source": [
    "### The long-time-taking training code is just below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4971f967-9d95-4606-9cc7-748834836b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timeit.default_timer()\n",
    "trainer.train()\n",
    "toc = timeit.default_timer()\n",
    "print(f\"tic: {tic}\")\n",
    "print(f\"toc: {toc}\")\n",
    "training_duration = toc - tic\n",
    "print(f\"Training took {toc - tic:0.4f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749bd2db-709b-45bb-b9c4-acb820cbd5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_by_hand = False\n",
    "NUM_TO_CATCH_NO_DO_BY_HAND = -137.\n",
    "is_a_do_by_hand_skip = False # innocent until proven guilty\n",
    "\n",
    "\n",
    "if do_by_hand:\n",
    "    # !!! remember to type in your number, if needed !!! #\n",
    "    training_duration = NUM_TO_CATCH_NO_MANUAL_ENTRY\n",
    "    # !!! UNCOMMENT THE NEXT LINE IF YOU WANT TO ENTER MANUALLY !!!\n",
    "    #training_duration = 11081.7024\n",
    "##endof:  if do_by_hand\n",
    "\n",
    "print(\"Running training with LoRA\")\n",
    "print(\"(using the training and eval sets)\")\n",
    "if ( ( do_by_hand ) and \\\n",
    "     ( training_duration == -137. ) \\\n",
    "   ):\n",
    "    print(\"took AN UNKNOWN AMOUNT OF TIME.\")\n",
    "    print(\"You didn't manually enter in your real time,\")\n",
    "    print(\"as you should have.\")\n",
    "    is_a_do_by_hand_skip = True\n",
    "elif ( ( do_by_hand ) and \\\n",
    "       ( training_duration != -137. ) \n",
    "     ):\n",
    "    print(\"(and using your manually entered time)\")\n",
    "else:\n",
    "    pass\n",
    "##endof:  if <check manual entry>\n",
    "\n",
    "if not is_a_do_by_hand_skip:\n",
    "    print(f\"took {format_timespan(training_duration)}\")\n",
    "##endof:  if not is_a_do_by_hand_skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aaea49-d34a-4ab1-89f1-65c7de627ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "# !powershell -c (Get-Date -UFormat \\\"%s_%Y-%m-%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0520fa-6516-42cf-888f-08b0717e7d83",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`1717107458_2024-05-30T221738-0600`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5127700-78f2-4f17-b677-8d29789d30db",
   "metadata": {},
   "source": [
    "#### @todo : consolidate \"the other info as above\"\n",
    "\n",
    "I'm talking about the numbers of data points, tokens, whatever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ca0bc-360c-4708-a1a8-c33c5f3f731f",
   "metadata": {},
   "source": [
    "#### Any Comments / Things to Try (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d76966d-3466-463a-83fa-e21381b5365a",
   "metadata": {},
   "source": [
    "We passed an evaluation set (parameter ``) to the `trainer`.\n",
    "How can we see information about that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934771bd-3201-4393-be21-7036e49b0843",
   "metadata": {},
   "source": [
    "#### How to get the evaluation set used by the trainer\n",
    "\n",
    "I added the following parameters to the \n",
    "`training_args = TrainingArguments(<args>)`\n",
    "call.\n",
    "\n",
    "- `do_eval=True`\n",
    "- `per_device_eval_batch_size=4`\n",
    "- `eval_strategy='epoch'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c056cac-441f-42ae-9600-8db0bc991f82",
   "metadata": {},
   "source": [
    "#### How to specify your repo name\n",
    "\n",
    "I also added this next parameter to the arguments for\n",
    "`training_args = TrainingArguments(<args>)`\n",
    "\n",
    "- `hub_model_id=\"dwb-flan-t5-small-lora-finetune\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b246e-c7bd-4fd7-8eab-b0ba46a2a922",
   "metadata": {},
   "source": [
    "#### The final TrainingArguments call - with parameter list\n",
    "\n",
    "```\n",
    "training_args = TrainingArguments( \n",
    "                        output_dir='output',\n",
    "                        num_train_epochs=1,\n",
    "                        per_device_train_batch_size=4,\n",
    "                        save_strategy='epoch',\n",
    "                        learning_rate=2e-4,\n",
    "                        do_eval=True,\n",
    "                        per_device_eval_batch_size=4,\n",
    "                        eval_strategy='epoch',\n",
    "                        hub_model_id=\"dwb-flan-t5-small-lora-finetune\",\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e62ce34-a826-4a83-8b2d-9acede4ff2a2",
   "metadata": {},
   "source": [
    "## Save the Trainer to Hugging Face and Get Our Updated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80265d1c-b65b-4863-8ac3-34e066c86522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "# !powershell -c (Get-Date -UFormat \\\"%s_%Y-%m-%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c105c8-8182-4c12-9ce6-1016ee4787c5",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`1717145367_2024-05-31T084927-0600`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e0693e-579b-4a82-a2c2-2d3dc1e6f900",
   "metadata": {},
   "source": [
    "I'm following the [(archived) tutorial from Mehul Gupta on Medium](https://web.archive.org/web/20240522140323/https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578); since it's archived, you can follow exactly what I'm doing.\n",
    "\n",
    "Running this next line of code will come up with a dialog box with text entry,\n",
    "and I'm now using the `@thebballdave025` for Hugging Face stuff.\n",
    "\n",
    "<b>Make sure to use the WRITE token, here.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e5a45e-1890-415c-9da3-9f43fa0969df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This will come up with a dialog box with text entry.\n",
    "#+ and I'm now using @thebballdave025 for Hugging Face.\n",
    "\n",
    "# Use the write token, here.\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c99a5c-2739-49f6-9a4b-02c087e4c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer and create a tokenizer model card\n",
    "tokenizer.save_pretrained('testing')\n",
    "  #  used 'testing' first - I think I can make a repo according\n",
    "  #+ to the first getting-started cli instructions, but let's\n",
    "  #+ use what Mehul Gupta used, first\n",
    "  #  Actually, I think 'testing' is the local directory\n",
    "\n",
    "# Create the trainer model card\n",
    "trainer.create_model_card()\n",
    "\n",
    "# Push the results to the Hugging Face Hub\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe8898-7a51-4826-8328-4db9cea25886",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "Part of the output included the URL,\n",
    "\n",
    "https://huggingface.co/thebballdave025/dwb-flan-t5-small-lora-finetune/commit/c87d34b398f3801ceb1e18c819a7c8fc894989c7\n",
    "\n",
    "Hooray! The repo name I used in constructing the trainer worked!\n",
    "\n",
    "I can get to the general repo with the URL,\n",
    "\n",
    "https://huggingface.co/thebballdave025/dwb-flan-t5-small-lora-finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511b4c50-2882-4fdd-afa5-3daefbb78cf3",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "## Info on the Fine-Tuned Model from the Repo's README - Model Card(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1121b2f-91df-4912-b42f-a3e6803180a9",
   "metadata": {},
   "source": [
    "### [thebballdave025/dwb-flan-t5-small-lora-finetune](https://huggingface.co/thebballdave025/dwb-flan-t5-small-lora-finetune)\n",
    "\n",
    "\\[archived\\] The archiving attempt at archive.org (Wayback Machine) failed.\n",
    "I'm not sure why, as the model is set as public.\n",
    "\n",
    "`PEFT  TensorBoard  Safetensors       generator  trl  sft  generated_from_trainer       License: apache-2.0`\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<b>@todo</b> : [Edit Model Card](https://huggingface.co/thebballdave025/dwb-flan-t5-small-lora-finetune/edit/main/README.md)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;\n",
    "Unable to determine this model’s pipeline type. Check the docs \n",
    "[(i)](https://huggingface.co/docs/hub/models-widgets#enabling-a-widget).\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Adapter for\n",
    "[google/flan-t5-small](https://huggingface.co/google/flan-t5-small)\n",
    "\n",
    "#### dwb-flan-t5-small-lora-finetune\n",
    "\n",
    "This model is a fine-tuned version of \n",
    "[google/flan-t5-small](https://huggingface.co/google/flan-t5-small) on the \n",
    "generator dataset \\[DWB note: I don't know why it says \"generator dataset\".\n",
    "I used the samsum dataset, which I will link here and on the\n",
    "model card, eventually\\]. \n",
    "\n",
    "It achieves the following results on the evaluation set:\n",
    "\n",
    "- Loss: 0.0226\n",
    "- <i>DWB Note: I don't know which metric was used to calculate loss. If this were more important, I'd dig through code to find out and evaluate with the same metric. If I'm really lucky, they somehow used the ROUGE scores in the loss function, so we match.</i>\n",
    "\n",
    "#### Model description\n",
    "\n",
    "More information needed\n",
    "\n",
    "#### Intended uses & limitations\n",
    "\n",
    "More information needed\n",
    "\n",
    "#### Training and evaluation data\n",
    "\n",
    "More information needed\n",
    "\n",
    "#### Training procedure\n",
    "\n",
    "#### Training hyperparameters\n",
    "\n",
    "The following hyperparameters were used during training:\n",
    "\n",
    "- learning_rate: 0.0002\n",
    "- train_batch_size: 4\n",
    "- eval_batch_size: 4\n",
    "- seed: 42\n",
    "- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n",
    "- lr_scheduler_type: linear\n",
    "- num_epochs: 1\n",
    "\n",
    "#### Training results\n",
    "\n",
    "```\n",
    "\n",
    "  Training Loss | Epoch | Step | Validation Loss\n",
    " ---------------+-------+------+-----------------\n",
    "      0.0685    |  1.0  | 1536 |     0.0226\n",
    "```\n",
    "\n",
    "#### Framework versions\n",
    "\n",
    "- PEFT 0.11.2.dev0\n",
    "- Transformers 4.41.1\n",
    "- Pytorch 2.3.0+cpu\n",
    "- Datasets 2.19.1\n",
    "- Tokenizers 0.19.1\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c81523-7faf-40d9-806a-919038f6e6f1",
   "metadata": {},
   "source": [
    "## Actually Get the Model from Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62333ba9-c416-4f04-b8ff-ff27be3a359b",
   "metadata": {},
   "source": [
    "Running this next line of code will come up with a dialog box with text entry,\n",
    "and I'm now using the `@thebballdave025` for Hugging Face stuff.\n",
    "\n",
    "<b>Make sure to use the READ token, here.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a022c0-760d-4fcb-bb71-a05e322598cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read token. Will bring up text entry to paste token string\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207707c8-1920-446a-bfa9-9c8ee9480a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "!powershell -c (Get-Date -UFormat \\\"%s_%Y-%m-%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b4db3b-4516-4900-b640-ac498f0308bc",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`1717491686_2024-06-04T090126-0600`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e79207-23e2-4d73-ac6f-90a40a489574",
   "metadata": {},
   "source": [
    "I have restarted the kernel since defining these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca3e660-c8f3-4f95-92ab-33f7751f1f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_files = {'train':'samsum-train.json', \n",
    "#              'evaluation':'samsum-validation.json',\n",
    "#              'test':'samsum-test.json'}\n",
    "#dataset = load_dataset('json', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca447ad1-943e-420a-9898-8404fbb8b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My trained model from Hugging Face\n",
    "\n",
    "new_model_name = \"thebballdave025/dwb-flan-t5-small-lora-finetune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d1a19-78ae-4e67-9125-ae81a0f4d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_load_tic = timeit.default_timer()\n",
    "# do we need\n",
    "new_model = AutoModelForSeq2SeqLM.from_pretrained(new_model_name)\n",
    "# # or, do we need\n",
    "# new_model = AutoModelForCausalLM.from_pretrained(new_model_name)\n",
    "new_model_load_toc = timeit.default_timer()\n",
    "\n",
    "new_model_load_duration = new_model_load_toc - new_model_load_tic\n",
    "\n",
    "print(f\"Loading the LoRA-fine-tuned model, {new_model_name}\")\n",
    "print(f\"took {new_model_load_toc - new_model_load_tic:0.4f} seconds.\")\n",
    "\n",
    "new_model_load_time_str = format_timespan(new_model_load_duration)\n",
    "\n",
    "print(f\"which equates to {new_model_load_time_str}\")\n",
    "\n",
    "#  Next line makes training faster but a little less accurate\n",
    "new_model.config.pretraining_tp = 1\n",
    "\n",
    "new_tokenizer_tic = timeit.default_timer()\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(new_model_name, \n",
    "                                          trust_remote_code=True)\n",
    "new_tokenizer_toc = timeit.default_timer()\n",
    "\n",
    "new_tokenizer_duration = new_tokenizer_toc - new_tokenizer_tic\n",
    "\n",
    "print()\n",
    "print(\"Getting fine-turned tokenizer\")\n",
    "print(f\"took {new_tokenizer_toc - new_tokenizer_tic:0.4f} seconds.\")\n",
    "\n",
    "new_tokenizer_time_str = format_timespan(new_tokenizer_duration)\n",
    "\n",
    "print(f\"which equates to {new_tokenizer_time_str}\")\n",
    "\n",
    "#  padding instructions for the tokenizer\n",
    "#+   ??? !!! What about for RWKV !!! ???\n",
    "#+ Will it be the same?\n",
    "new_tokenizer.pad_token = new_tokenizer.eos_token\n",
    "new_tokenizer.padding_side = \"right\"\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Got some weird results, so I'm doing the old tokenizer\n",
    "old_model_name = \"google/flan-t5-small\"\n",
    "\n",
    "##old_model_load_tic = timeit.default_timer()\n",
    "##old_model = \\\n",
    "##     AutoModelForSeq2SeqLM.from_pretrained(old_model_name)\n",
    "##old_model_load_toc = timeit.default_timer()\n",
    "\n",
    "##old_model_load_duration = \\\n",
    "##           old_model_load_toc - old_model_load_tic\n",
    "\n",
    "##print(f\"Loading the original model, {old_model_name}\")\n",
    "##print(\"took \" + \\\n",
    "##      f\"{old_model_load_toc - old_model_load_tic:0.4f}\" + \\\n",
    "##      \" seconds.\"\n",
    "##)\n",
    "\n",
    "##old_model_load_time_str = format_timespan(old_model_load_duration)\n",
    "\n",
    "##print(f\"which equates to {old_model_load_time_str}\")\n",
    "\n",
    "## #  Next line makes training faster but a little less accurate\n",
    "##old_model.config.pretraining_tp = 1\n",
    "\n",
    "old_tokenizer_tic = timeit.default_timer()\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\n",
    "                                       old_model_name, \n",
    "                                       trust_remote_code=True\n",
    ")\n",
    "old_tokenizer_toc = timeit.default_timer()\n",
    "\n",
    "old_tokenizer_duration = old_tokenizer_toc - old_tokenizer_tic\n",
    "\n",
    "print(\"Getting old tokenizer\")\n",
    "print( \"took \" + \\\n",
    "      f\"{old_tokenizer_toc - old_tokenizer_tic:0.4f}\"\n",
    "       \" seconds.\"\n",
    ")\n",
    "\n",
    "tokenizer_time_str = format_timespan(tokenizer_duration)\n",
    "\n",
    "print(f\"which equates to {tokenizer_time_str}\")\n",
    "\n",
    "#  padding instructions for the tokenizer\n",
    "#+   ??? !!! What about for RWKV !!! ???\n",
    "#+ Will it be the same?\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8792fdf-eb40-4b73-b810-770227db7476",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b45927-f379-40b8-bf9a-e18171037c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_arch_str = str(new_model)\n",
    "\n",
    "with open(\"dwb-flan-t5-small-lora-finetune.model-architecture.txt\", 'w', encoding='utf-8') as fhn:\n",
    "    fhn.write(new_model_arch_str)\n",
    "##endof:  with open ... fhn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd1c76-f3b8-44ff-8d6e-1b735155d546",
   "metadata": {},
   "source": [
    "@todo : get some Python version of `diff` going on here. I'm just using Cygwin/bash to see the LoRA additions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87701971-8342-470b-8f23-66f4494206e8",
   "metadata": {},
   "source": [
    "### Let's start by doing the single-dialogue summaries we used before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e84c7c-d959-4565-b89d-de270cbea072",
   "metadata": {},
   "source": [
    "### Prompt (repeated for new kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac4925-0497-4ee6-97b0-229ea7b06d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def prompt_instruction_format(sample):\n",
    "#    return f\"\"\" Instruction:\n",
    "#      Use the Task below and the Input given to write the Response:\n",
    "#\n",
    "#      ### Task:\n",
    "#      Summarize the Input\n",
    "#\n",
    "#      ### Input:\n",
    "#      {sample['dialogue']}\n",
    "#\n",
    "#      ### Response:\n",
    "#      {sample['summary']}\n",
    "#      \"\"\"\n",
    "###endof:  prompt_instruction_format(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db3cc0e-e311-4076-bb17-e62a7e469d80",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70b7022-54f8-4efa-a8d6-16fe1c9a204f",
   "metadata": {},
   "source": [
    "#### Try one picked at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99bcc51-f9a4-41f0-b7bf-1d5398cac38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Just one summarization to begin with, randomly picked ... but\n",
    "#+ now with th possibility of a known seed, to allow visual \n",
    "#+ comparison with after-training results.\n",
    "#+ I'M NOT GOING TO USE THIS REPEATED SEED, I'm just going to\n",
    "#+ use the datum at the first index to compare.\n",
    "\n",
    "do_seed_for_repeatable = True\n",
    "\n",
    "summarizer = pipeline('summarization', \n",
    "                      model=new_model,\n",
    "                      tokenizer=new_tokenizer)\n",
    "\n",
    "if do_seed_for_repeatable:\n",
    "    rand_seed_for_randrange = 137\n",
    "    random.seed(rand_seed_for_randrange)\n",
    "##endof:  if do_seed_for_repeatable\n",
    "\n",
    "sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    "\n",
    "res = summarizer(sample[\"dialogue\"])\n",
    "\n",
    "print(f\"dwb-flan-t5-small-lora-finetune summary:\\n{res[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f843119d-1d6f-4a3f-bd1e-b8d064f10394",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e7a31-de22-4dca-be9c-bff1f8705c45",
   "metadata": {},
   "source": [
    "#### Now, a couple summarizations with comparison to ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e1948-8eb4-41e2-84eb-be2cd71d9b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_list = []\n",
    "ref_test_list = []\n",
    "\n",
    "sample_num = 0\n",
    "\n",
    "this_sample = dataset['test'][sample_num]\n",
    "\n",
    "print(f\"dialogue: \\n{this_sample['dialogue']}\\n---------------\")\n",
    "\n",
    "ground_summary = this_sample['summary']\n",
    "res = summarizer(this_sample['dialogue'])\n",
    "res_summary = res[0]['summary_text']\n",
    "\n",
    "print(f\"human-genratd summary:\\n{ground_summary}\")\n",
    "print(f\"dwb-flan-t5-small-lora-finetune summary:\\n{res_summary}\")\n",
    "\n",
    "ref_test_list.append(ground_summary)\n",
    "pred_test_list.append(res_summary)\n",
    "\n",
    "# deprecated, blah blah blah\n",
    "rouge = load_metric('rouge', trust_remote_code=True)\n",
    "\n",
    "#  Yes, I have just one datum, but I'm setting things up to\n",
    "#+ work well with a loop (meaning lists for pred and ref).\n",
    "results_test_0 = rouge.compute(\n",
    "                    predictions=pred_test_list,\n",
    "                    references=ref_test_list,\n",
    "                    use_aggregator=False\n",
    ")\n",
    "\n",
    "# >>> print(list(results_test.keys()))\n",
    "# ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d75d3-79f7-46b7-9159-cfdda7dea069",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b13b61-4b8a-484f-b3c6-1128ec6f6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rouge_scores(results_test_0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7f1ec7-e2a8-4df3-b9db-22abf8c27b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 224\n",
    "\n",
    "this_sample = dataset['test'][sample_num]\n",
    "\n",
    "print(f\"dialogue: \\n{this_sample['dialogue']}\\n---------------\")\n",
    "\n",
    "ground_summary = this_sample['summary']\n",
    "res = summarizer(this_sample['dialogue'])\n",
    "res_summary = res[0]['summary_text']\n",
    "\n",
    "print(f\"human-genratd summary:\\n{ground_summary}\")\n",
    "print(f\"dwb-flan-t5-small-lora-finetune summary:\\n{res_summary}\")\n",
    "\n",
    "ref_test_list = [ground_summary]\n",
    "pred_test_list = [res_summary]\n",
    "\n",
    "rouge = load_metric('rouge', trust_remote_code=True)\n",
    "\n",
    "results_test_224 = rouge.compute(\n",
    "                     predictions=pred_test_list,\n",
    "                     references=ref_test_list,\n",
    "                     use_aggregator=False\n",
    ")\n",
    "\n",
    "# >>> print(list(results_test.keys()))\n",
    "# ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d2b0a6-ff52-4e5a-b849-823350dc2d59",
   "metadata": {},
   "source": [
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bebf17-d3ca-46b7-afb3-941418ab36a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rouge_scores(results_test_224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a4bc8-47b3-4022-9168-2d21174dac09",
   "metadata": {},
   "source": [
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eafafa-e0d2-40ff-a779-4b3271def94d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0369324f-d29b-4a49-b6d9-3106caf103f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eef5412-bf00-4e63-aeef-0083934bfcbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88281b15-7070-44dd-84e4-aedefa00293d",
   "metadata": {},
   "source": [
    "## Evaluation on the Test Set and Comparison to Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d123c-412a-4195-8820-838590b814a2",
   "metadata": {},
   "source": [
    "#### Verbosity stuff - get rid of the nice advice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a7666e-206c-418a-aec8-27b0fd4de3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde59af6-c15a-4494-b49e-87019a70715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_verbosity_is_critical = \\\n",
    "  logging.get_verbosity() == logging.CRITICAL # alias FATAL, 50\n",
    "log_verbosity_is_error = \\\n",
    "  logging.get_verbosity() == logging.ERROR # 40\n",
    "log_verbosity_is_warn = \\\n",
    "  logging.get_verbosity() == logging.WARNING # alias WARN, 30\n",
    "log_verbosity_is_info = \\\n",
    "  logging.get_verbosity() == logging.INFO # 20\n",
    "log_verbosity_is_debug = \\\n",
    "  logging.get_verbosity() == logging.DEBUG # 10\n",
    "\n",
    "print( \"The statement, 'logging verbosity is CRITICAL' \" + \\\n",
    "      f\"is {log_verbosity_is_critical}\")\n",
    "print( \"The statement, 'logging verbosity is    ERROR' \" + \\\n",
    "      f\"is {log_verbosity_is_error}\")\n",
    "print( \"The statement, 'logging verbosity is  WARNING' \" + \\\n",
    "      f\"is {log_verbosity_is_warn}\")\n",
    "print( \"The statement, 'logging verbosity is     INFO' \" + \\\n",
    "      f\"is {log_verbosity_is_info}\")\n",
    "print( \"The statement, 'logging verbosity is    DEBUG' \" + \\\n",
    "      f\"is {log_verbosity_is_debug}\")\n",
    "\n",
    "print()\n",
    "\n",
    "init_log_verbosity = logging.get_verbosity()\n",
    "print(f\"The value of logging.get_verbosity() is: {init_log_verbosity}\")\n",
    "\n",
    "print()\n",
    "\n",
    "init_t_n_a_w = os.environ.get('TRANSFORMERS_NO_ADVISORY_WARNINGS')\n",
    "print(f\"TRANSFORMERS_NO_ADIVSORY_WARNINGS: {init_t_n_a_w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf8db54-8003-429f-8abd-cfb09a1af80b",
   "metadata": {},
   "source": [
    "### Here's the actual evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d3eeb-e238-43bf-ada2-3d7ccaf10b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a522d7-6285-4b9d-b619-ea0f454f2cae",
   "metadata": {},
   "source": [
    "<b>!!! NOTE !!!</b> I'm going to use `tat` (with an underscore\n",
    "or undescores before, after, or surrounding the variable names)\n",
    "to indicate 'testing-after-training'.\n",
    "\n",
    "I guess I could have used `inference`, but I didn't."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523785e0-4ece-49cc-bb8a-64d7c3b0bb01",
   "metadata": {},
   "source": [
    "<b>!!! another NOTE</b> You'd better <b>make \n",
    "dang sure you want the lots of output</b> \n",
    "before you set this next boolean to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a638f1-5abb-4c3e-8683-dcc49557a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_have_lotta_output_from_all_dialogs_summaries = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdfd04c-675b-4e8c-b293-560617b5651a",
   "metadata": {},
   "source": [
    "# Are you sure about the value of that last boolean?\n",
    "\n",
    "There could be megabytes (maybe gigabytes) worth of text output if you've changed it to `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670fbde9-0651-4723-9ff8-09264467b27d",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205d54d1-e4db-4851-97ed-e56e90276285",
   "metadata": {},
   "source": [
    "### Evaluation on the Test Set and Comparison to Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ad3bd-0921-4753-8c6c-175bffc23b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()\n",
    "\n",
    "tat_summarizer = pipeline('summarization', \n",
    "                          model=new_model, \n",
    "                          tokenizer=new_tokenizer)\n",
    "\n",
    "#*p*#tat_sample_dialog_list = []\n",
    "prediction_tat_list = []\n",
    "reference_tat_list = []\n",
    "\n",
    "tat_tic = timeit.default_timer()\n",
    "\n",
    "for sample_num in range(len(dataset['test'])):\n",
    "    this_sample = dataset['test'][sample_num]\n",
    "    \n",
    "    if do_have_lotta_output_from_all_dialogs_summaries:\n",
    "        print(\"=\"*75)\n",
    "        print(f\"dialogue: \\n{this_sample['dialogue']}\\n---------------\")\n",
    "    ##endof:  if do_have_lotta_output_from_all_dialogs_summaries\n",
    "    \n",
    "    ground_tat_summary = this_sample['summary']\n",
    "    res_tat = summarizer(this_sample['dialogue'])\n",
    "    res_tat_summary = res_tat[0]['summary_text']\n",
    "    \n",
    "    if do_have_lotta_output_from_all_dialogs_summaries:\n",
    "        print(\"-\"*70)\n",
    "        print(f\"human-genratd summary:\\n{ground_tat_summary}\")\n",
    "        print(\"-\"*70)\n",
    "        print( \"dwb-flan-t5-small-lora-finetune summary:\" + \\\n",
    "              f\"\\n{res_tat_summary}\")\n",
    "        print(\"-\"*70)\n",
    "    ##endof:  if do_have_lotta_output_from_all_dialogs_summaries\n",
    "\n",
    "    #*p*#    tat_sample_dialog_list.append(this_sample)\n",
    "    reference_tat_list.append(ground_tat_summary)\n",
    "    prediction_tat_list.append(res_tat_summary)\n",
    "##endof:  for sample_num in range(len(dataset['test']))\n",
    "\n",
    "tat_toc = timeit.default_timer()\n",
    "\n",
    "tat_duration = tat_toc - tat_tic\n",
    "\n",
    "print( \"Getting things ready for scoring (after training)\")\n",
    "print(f\"took {tat_toc - tat_tic:0.4f} seconds.\")\n",
    "\n",
    "tat_time_str = format_timespan(tat_duration)\n",
    "\n",
    "print(f\"which equates to {tat_time_str}\")\n",
    "\n",
    "\n",
    "rouge = load_metric('rouge', trust_remote_code=True)\n",
    "\n",
    "results_tat = rouge.compute(\n",
    "                  predictions=prediction_tat_list,\n",
    "                  references=reference_tat_list,\n",
    "                  use_aggregator=True\n",
    ")\n",
    "\n",
    "# >>> print(list(results_tat.keys()))\n",
    "# ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "\n",
    "#*p*# objects_to_pickle.append(tat_sample_dialog_list)\n",
    "#*p*# objects_to_pickle.append(prediction_tat_list)\n",
    "#*p*# objects_to_pickle.append(reference_tat_list)\n",
    "#*p*# objects_to_pickle.append(results_tat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ac26a-b895-4622-a4d7-821c4e22588f",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61791378-d26a-4399-a2eb-dba17715791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rouge_scores(results_tat, \"TEST AFTER TRAINING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b25dc-d7ff-4166-8cc3-779d24d49c0c",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be13e82f-3a50-45b9-b636-85ca33c9dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Haven't tried this, because the logging seemed easier,\n",
    "##+ and the logging worked\n",
    "# os.environ(\"TRANSFORMERS_NO_ADVISORY_WARNINGS\") = init_t_n_a_w\n",
    "\n",
    "logging.set_verbosity(init_log_verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff65b106-c9d2-4936-9d5a-fa1d5fa1d10d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fb63f34-c6d8-4a64-8712-99137b5a9ddb",
   "metadata": {},
   "source": [
    "### Pickle things to pickle save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5526ab-3d35-4b79-a18a-0cca17e5f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pickle_filename, 'wb') as pfh:\n",
    "    pickle.dump(objects_to_pickle , pfh)\n",
    "##endof:  with open ... as pfh # (pickle file handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5af6893-b04d-42d1-8694-aeca3b33f993",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66346d54-06bb-47da-8d54-a9dee43b408a",
   "metadata": {},
   "source": [
    "## Notes Looking Forward to LoRA on RWKV\n",
    "\n",
    "Hugging Face Community, seems to have a good portion of their models\n",
    "\n",
    "https://huggingface.co/RWKV\n",
    "\n",
    "https://web.archive.org/web/20240530232509/https://huggingface.co/RWKV\n",
    "\n",
    "<br/>\n",
    "\n",
    "GitHub has even more versions/models, including the `v4-neo` that\n",
    "I think will be important (the LoRA project)\n",
    "\n",
    "https://github.com/BlinkDL/RWKV-LM/tree/main\n",
    "\n",
    "https://web.archive.org/web/20240530232637/https://github.com/BlinkDL/RWKV-LM/tree/main\n",
    "\n",
    "<br/>\n",
    "\n",
    "The main RWKV website (?!)\n",
    "\n",
    "https://www.rwkv.com/\n",
    "\n",
    "https://web.archive.org/web/20240529120904/https://www.rwkv.com/\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "GOOD STUFF. A project doing LoRA with RWKV\n",
    "\n",
    "https://github.com/Blealtan/RWKV-LM-LoRA/\n",
    "\n",
    "https://web.archive.org/web/20240530232823/https://github.com/Blealtan/RWKV-LM-LoRA\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "The official blog, I guess, with some good coding examples\n",
    "\n",
    "https://huggingface.co/blog/rwkv\n",
    "\n",
    "https://web.archive.org/web/20240530233025/https://huggingface.co/blog/rwkv\n",
    "\n",
    "It includes something that's similar to what I'm doing here in the\n",
    "`First_Full_LoRA_Trial_with_Transformer_Again.ipynb` tutorial, etc.\n",
    "\n",
    "```\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"RWKV/rwkv-raven-1b5\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "```\n",
    "\n",
    "The `AutoModelForCausalLM` is the same as the tutorial I'm following,\n",
    "but I don't know what the `.to(0)` is for.\n",
    "\n",
    "Really quickly, also looking at\n",
    "\n",
    "https://huggingface.co/RWKV/rwkv-4-world-7b\n",
    "\n",
    "https://web.archive.org/web/20240530234438/https://huggingface.co/RWKV/rwkv-4-world-7b\n",
    "\n",
    "I see an example for CPU.\n",
    "\n",
    "```\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "              \"RWKV/rwkv-4-world-7b\",\n",
    "              trust_remote_code=True\n",
    ").to(torch.float32)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "              \"RWKV/rwkv-4-world-7b\",\n",
    "              trust_remote_code=True)\n",
    "```\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "(Old version? Unofficial, it seems)\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/model_doc/rwkv\n",
    "\n",
    "https://web.archive.org/web/20240530232341/https://huggingface.co/docs/transformers/en/model_doc/rwkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e92f0e-4c74-44b5-bec1-f0161ee84266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
