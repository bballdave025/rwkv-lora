{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d7b071-3c52-4189-81e4-2fe25bb2f61d",
   "metadata": {},
   "source": [
    "# First Full LoRA Trial with Transformer\n",
    "\n",
    "## peft (for LoRA) and FLAN-T5-small for the LLM\n",
    "\n",
    "I'm following what seems to be a great tutorial from Mehul Gupta,\n",
    "\n",
    "> https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578\n",
    "> \n",
    "> https://web.archive.org/web/20240522140323/https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578\n",
    "\n",
    "I'm doing this to prepare creating a LoRA for RWKV ( @todo  put links in here ) so as to fine-tune it for Pat's OLECT-LM stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab0319e-1d1e-4fa3-9153-e8c26344e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  No need to run this again\n",
    "# !powershell -c (Get-Date -UFormat \\\"%s_%Y%m%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8476aae1-0e49-4094-8eb8-6bf0d51acbc7",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`1716367147_20240522T083907-0600`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb51da5-6c05-4870-931c-7068e575ac99",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7c2afe-9b5f-4ee6-9db6-9f8d2cf6b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "import torch\n",
    "from transformers import AutoTokenizer, \\\n",
    "                         AutoModelForSeq2SeqLM, \\\n",
    "                         TrainingArguments, \\\n",
    "                         pipeline\n",
    "from transformers.utils import logging\n",
    "from peft import LoraConfig, \\\n",
    "                 prepare_model_for_kbit_training, \\\n",
    "                 get_peft_model, \\\n",
    "                 AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login, notebook_login\n",
    "\n",
    "from datasets import load_metric\n",
    "import nltk\n",
    "import rouge_score\n",
    "\n",
    "import pickle\n",
    "import pprint\n",
    "import timeit\n",
    "from humanfriendly import format_timespan\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bfe2f4-b1b4-4158-b3c6-048c487514d2",
   "metadata": {},
   "source": [
    "## Load the training and test dataset along with the LLM with its tokenizer\n",
    "\n",
    "The LLM will be fine-tuned. It seems the tokenizer will also be fine-tuned, \n",
    "but I'm not sure \n",
    "\n",
    "<b>Why aren't we loading the validation set?</b> (I don't know; that's not a teaching question.)\n",
    "\n",
    "I've tried to make use of it with the `trainer`. We'll see how it goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6043d426-bcee-4539-a9ea-4b7c4c3dccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Need to install  datasets  from pip, not conda. I'll do all from pip. \n",
    "#+ I'll get rid of the current conda environment and make it anew.\n",
    "#+ Actually, I'll make sure  conda  and  pip  are updated, then do what\n",
    "#+ I discussed above.\n",
    "#+\n",
    "#+ cf. \n",
    "#+     arch_ref_1 = \"https://web.archive.org/web/20240522150357/\" + \\\n",
    "#+                  \"https://stackoverflow.com/questions/77433096/\" + \\\n",
    "#+                  \"notimplementederror-loading-a-dataset-\" + \\\n",
    "#+                  \"cached-in-a-localfilesystem-is-not-suppor\"\n",
    "#+\n",
    "#+ Also useful might be\n",
    "#+     arch_ref_2 = \"https://web.archive.org/web/20240522150310/\" + \\\n",
    "#+                  \"https://stackoverflow.com/questions/76340743/\" + \\\n",
    "#+                  \"huggingface-load-datasets-gives-\" + \\\n",
    "#+                  \"notimplementederror-cannot-error\"\n",
    "#\n",
    "data_files = {'train':'samsum-train.json', \n",
    "              'evaluation':'samsum-validation.json',\n",
    "              'test':'samsum-test.json'}\n",
    "dataset = load_dataset('json', data_files=data_files)\n",
    "\n",
    "model_name = \"google/flan-t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "#  Next line makes training faster but a little less accurate\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                          trust_remote_code=True)\n",
    "\n",
    "#  padding instructions for the tokenizer\n",
    "#+   ??? !!! What about for RWKV !!! ???\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc6b24-d617-48a1-8236-c3be16402c4a",
   "metadata": {},
   "source": [
    "#### Trying some things I've been learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a107cbdc-5a73-4371-8afe-faa2dc623c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59156663-4b5b-4b60-8b37-627a9f297fc9",
   "metadata": {},
   "source": [
    "## Prompt and Trainer\n",
    "\n",
    "For our SFT (<b>S</b>upervised <b>F</b>ine <b>T</b>uning) model, we use the `class trl.SFTTrainer`.\n",
    "\n",
    "I want to research this a bit, especially the `formatting_func` that we'll be passing to the `SFTTrainer`.\n",
    "\n",
    "First, though, some information about SFT. From the Hugging Face Documentation at https://huggingface.co/docs/trl/en/sft_trainer ([archived](https://web.archive.org/web/20240529140717/https://huggingface.co/docs/trl/en/sft_trainer))\n",
    "\n",
    "> Supervised fine-tuning (or SFT for short) is a crucial step in RLHF. In TRL we provide an easy-to-use API to create your SFT models and train them with few lines of code on your dataset.\n",
    "\n",
    "Though I won't be using the examples unless I get even more stuck, the next paragraph _has_ examples, and I'll put the paragraph here.\n",
    "\n",
    "> Check out a complete flexible example at [examples/scripts/sft.py](https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py) \\[[archived](https://web.archive.org/web/20240529140740/https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py)\\]. Experimental support for Vision Language Models is also included in the example [examples/scripts/vsft_llava.py](https://github.com/huggingface/trl/blob/main/examples/scripts/vsft_llava.py) \\[[archived](https://web.archive.org/web/20240529140738/https://github.com/huggingface/trl/blob/main/examples/scripts/vsft_llava.py)\\].\n",
    "\n",
    "RLHF ([archived wikipedia page](https://web.archive.org/web/20240529142205/https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback)) is <b>R</b>einforcement <b>L</b>earning from <b>H</b>uman <b>F</b>eedback. [TRL](https://huggingface.co/docs/trl/en/index#:~:text=TRL%20is%20a%20full%20stack,Policy%20Optimization%20(PPO)%20step.) ([archived]())       <b>T</b>ransfer <b>R</b>einforcement <b>L</b>earning, a library from Hugging Face.\n",
    "\n",
    "For the parameter, `formatting_func`, I can look ath the documentation site above (specifically [here](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=formatting_func%20(Optional)), at the GitHub repo for [the code](https://github.com/huggingface/trl/blob/main/trl/trainer/sft_trainer.py) (in the docstrings), or from my local `conda` environment, at `C:\\Users\\bballdave025\\.conda\\envs\\rwkv-lora-pat\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py`.\n",
    "\n",
    "Pulling code from the last one, I get\n",
    "\n",
    ">         formatting_func (`Optional[Callable]`):\n",
    ">            The formatting function to be used for creating the `ConstantLengthDataset`.\n",
    "\n",
    "That matches the first very well\n",
    "\n",
    "> <b>formatting_func</b> (`Optional[Callable]`) — The formatting function to be used for creating the `ConstantLengthDataset`.\n",
    "\n",
    "(A quick note: In this Jupyter Notebook environment, I could have typed `trainer = SFTTrainer(` and then <kbd>Shift</kbd> + <kbd>Tab</kbd> to find that same documentation.\n",
    "\n",
    "However, I think that more clarity is found at the [documentation for `ConstantLengthDataset](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=class%20trl.trainer.ConstantLengthDataset)\n",
    "\n",
    "> <b>formatting_func</b> (`Callable`, <b>optional</b>) — Function that formats the text before tokenization. Usually it is recommended to have follows a certain pattern such as `\"### Question: {question} ### Answer: {answer}\"`\n",
    "\n",
    "So, as we'll see the next code from  the tutorial, it basically is a prompt templater/formatter that matches the JSON. For example, we use `sample['dialogue']` to access the `dialogue` key/pair. That's what I got from all this stuff.\n",
    "\n",
    "Mehul Gupta himself stated\n",
    "\n",
    "> Next, using the Input and Output, we will create a prompt template which is a requirement by the SFTTrainer we will be using later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cdd3ac-f714-41ed-a1fb-c27395b9251d",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb36fc-783f-4f19-b213-69cc3dbf05af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_instruction_format(sample):\n",
    "    return f\"\"\" Instruction:\n",
    "      Use the Task below and the Input given to write the Response:\n",
    "\n",
    "      ### Task:\n",
    "      Summarize the Input\n",
    "\n",
    "      ### Input:\n",
    "      {sample['dialogue']}\n",
    "\n",
    "      ### Response:\n",
    "      {sample['summary']}\n",
    "      \"\"\"\n",
    "##endof:  prompt_instruction_format(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3622673e-df80-43da-9d5c-efd6085d301d",
   "metadata": {},
   "source": [
    "### Trainer - the LoRA Setup Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da39f5e-4e14-4eb5-ac66-c90a77472c64",
   "metadata": {},
   "source": [
    "#### Arguments and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac040cf-2eb5-4eb5-91c7-2e47c54b990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Some arguments to pass to the trainer\n",
    "training_args = TrainingArguments( output_dir='output',\n",
    "                                   num_train_epochs=1,\n",
    "                                   per_device_train_batch_size=4,\n",
    "                                   save_strategy='epoch',\n",
    "                                   learning_rate=2e-4\n",
    ")\n",
    "\n",
    "# the fine-tuning (peft for LoRA) stuff\n",
    "peft_config = LoraConfig( lora_alpha=16,\n",
    "                          lora_dropout=0.1,\n",
    "                          r=64,\n",
    "                          bias='none',\n",
    "                          task_type='CAUSAL_LM'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa2a84-c5f6-45ad-9d09-fb9f25fae9da",
   "metadata": {},
   "source": [
    "`task_type`, cf. https://github.com/huggingface/peft/blob/main/src/peft/config.py#L222\n",
    "\n",
    ">        Args:\n",
    ">            peft_type (Union[[`~peft.utils.config.PeftType`], `str`]): The type of Peft method to use.\n",
    ">            task_type (Union[[`~peft.utils.config.TaskType`], `str`]): The type of task to perform.\n",
    ">            inference_mode (`bool`, defaults to `False`): Whether to use the Peft model in inference mode.\n",
    "\n",
    "After some searching using Cygwin\n",
    "\n",
    "```\n",
    "bballdave025@MYMACHINE /cygdrive/c/Users/bballdave025/.conda/envs/rwkv-lora-pat/Lib/site-packages/peft/utils\n",
    "$ ls -lah\n",
    "total 116K\n",
    "drwx------+ 1 bballdave025 bballdave025    0 May 28 21:09 .\n",
    "drwx------+ 1 bballdave025 bballdave025    0 May 28 21:09 ..\n",
    "-rwx------+ 1 bballdave025 bballdave025 2.0K May 28 21:09 __init__.py\n",
    "drwx------+ 1 bballdave025 bballdave025    0 May 28 21:09 __pycache__\n",
    "-rwx------+ 1 bballdave025 bballdave025 8.0K May 28 21:09 constants.py\n",
    "-rwx------+ 1 bballdave025 bballdave025 3.8K May 28 21:09 integrations.py\n",
    "-rwx------+ 1 bballdave025 bballdave025  17K May 28 21:09 loftq_utils.py\n",
    "-rwx------+ 1 bballdave025 bballdave025 9.7K May 28 21:09 merge_utils.py\n",
    "-rwx------+ 1 bballdave025 bballdave025  25K May 28 21:09 other.py\n",
    "-rwx------+ 1 bballdave025 bballdave025 2.2K May 28 21:09 peft_types.py\n",
    "-rwx------+ 1 bballdave025 bballdave025  21K May 28 21:09 save_and_load.py\n",
    "\n",
    "bballdave025@MYMACHINE /cygdrive/c/Users/bballdave025/.conda/envs/rwkv-lora-pat/Lib/site-packages/peft/utils\n",
    "$ grep -iIRHn \"TaskType\" .\n",
    "peft_types.py:60:class TaskType(str, enum.Enum):\n",
    "__init__.py:20:# from .config import PeftConfig, PeftType, PromptLearningConfig, TaskType\n",
    "__init__.py:22:from .peft_types import PeftType, TaskType\n",
    "\n",
    "bballdave025@MYMACHINE /cygdrive/c/Users/bballdave025/.conda/envs/rwkv-lora-pat/Lib/site-packages/peft/utils\n",
    "$\n",
    "```\n",
    "\n",
    "So, let's look at the `peft_types.py` file.\n",
    "\n",
    "The docstring for `class TaskType(str, enum.Enum)` is\n",
    "\n",
    "```\n",
    "    Enum class for the different types of tasks supported by PEFT.\n",
    "    \n",
    "    Overview of the supported task types:\n",
    "    - SEQ_CLS: Text classification.\n",
    "    - SEQ_2_SEQ_LM: Sequence-to-sequence language modeling.\n",
    "    - CAUSAL_LM: Causal language modeling.\n",
    "    - TOKEN_CLS: Token classification.\n",
    "    - QUESTION_ANS: Question answering.\n",
    "    - FEATURE_EXTRACTION: Feature extraction. Provides the hidden states which can be used as embeddings or features\n",
    "      for downstream tasks.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e178f3f-9dd6-409e-9a4d-c1c85611a8b9",
   "metadata": {},
   "source": [
    "### We're going to start timing stuff, so here's some system info\n",
    "\n",
    "`win_system_info_as_script.py` is a script I wrote with the help\n",
    "of a variety of StackOverflow and documentation sources.\n",
    "It should be in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e51b0-8f09-4693-b991-e853ca3cfccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import win_system_info_as_script as winsysinfo\n",
    "winsysinfo.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5cd858-f6e7-4dba-b689-75e3d950f26d",
   "metadata": {},
   "source": [
    "### Try for a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba0b4db-cd1c-466c-91be-cc0e2a8a14c1",
   "metadata": {},
   "source": [
    "#### Just one summarization to begin with, randomly picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e54f9-6693-454d-b4a1-fca681baf39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!powershell -c (Get-Date -UFormat \\\"%s_%Y%m%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d892f4f-d28f-4371-bcb1-9ce214c6f3b3",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68250f44-daf6-436e-a7d2-dcb7a8d97ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Just one summarization to begin with, randomly picked ... but\n",
    "#+ now with th possibility of a known seed, to allow visual \n",
    "#+ comparison with after-training results.\n",
    "#+ I'M NOT GOING TO USE THIS REPEATED SEED, I'm just going to\n",
    "#+ use the datum at the first index to compare.\n",
    "\n",
    "do_seed_for_repeatable = False\n",
    "\n",
    "summarizer = pipeline('summarization', model=model, tokenizer=tokenizer)\n",
    "\n",
    "if do_seed_for_repeatable:\n",
    "  rand_seed_for_randrange = 137\n",
    "  random.seed(rand_seed_for_randrange)\n",
    "##endof:  if do_seed_for_repeatable\n",
    "\n",
    "sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    "\n",
    "res = summarizer(sample[\"dialogue\"])\n",
    "\n",
    "print(f\"flan-t5-small summary:\\n{res[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21170b7-0685-4e39-b9d6-2eaf5624d476",
   "metadata": {},
   "source": [
    "#### Now, one summarization with comparison to ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a760e7-6270-4e65-93f0-1af0c3dd559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now one summarization with comparison to ground truth\n",
    "\n",
    "summarizer = pipeline('summarization', model=model, tokenizer=tokenizer)\n",
    "\n",
    "pred_test_list = []\n",
    "ref_test_list = []\n",
    "\n",
    "sample_num = 0\n",
    "\n",
    "this_sample = dataset['test'][sample_num]\n",
    "\n",
    "print(f\"dialogue: \\n{this_sample['dialogue']}\\n---------------\")\n",
    "\n",
    "grnd_summary = this_sample['summary']\n",
    "res = summarizer(this_sample['dialogue'])\n",
    "res_summary = res[0]['summary_text']\n",
    "\n",
    "# humgen is for human-generated\n",
    "\n",
    "print(f\"human-genratd summary:\\n{grnd_summary}\")\n",
    "print(f\"flan-t5-small summary:\\n{res_summary}\")\n",
    "\n",
    "ref_test_list.append(grnd_summary)\n",
    "pred_test_list.append(res_summary)\n",
    "\n",
    "print(\"\\n\\n---------- ROUGE SCORES ----------\")\n",
    "\n",
    "rouge = load_metric('rouge', trust_remote_code=True)\n",
    "\n",
    "results = rouge.compute(predictions=pred_test_list,\n",
    "                        references=ref_test_list,\n",
    "                        use_aggregator=True)\n",
    "\n",
    "# >>> print(list(results.keys()))\n",
    "# ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "\n",
    "print()\n",
    "print(\"ROUGE-1 results\")\n",
    "pprint.pp(results['rouge1'])\n",
    "print()\n",
    "print(\"ROUGE-2 results\")\n",
    "pprint.pp(results['rouge2'])\n",
    "print()\n",
    "print(\"ROUGE-L results\")\n",
    "pprint.pp(results['rougeL'])\n",
    "print()\n",
    "print(\"ROUGE-Lsum results\")\n",
    "pprint.pp(results['rougeLsum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f099cc0c-bce3-49e5-8ca8-afd212adbc70",
   "metadata": {},
   "source": [
    "#### Verbosity stuff - get rid of the nice advice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7a763-7427-4054-ab79-ee8d0931f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!powershell -c (Get-Date -UFormat \\\"%s_%Y-%m-%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb761d-d836-49a4-b585-29aae74f0c86",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49d3a5-b58e-468a-836a-84a3908fd27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bballdave025@MYMACHINE /cygdrive/c/Users/bballdave025/.conda/envs/rwkv-lora-pat/Lib/site-packages/peft/utils\n",
    "# $ date +'%s_%Y-%m-%dT%H%M%S%z'\n",
    "# 1717049876_2024-05-30T001756-0600\n",
    "\n",
    "log_verbosity_is_critical = \\\n",
    "  logging.get_verbosity() == logging.CRITICAL # alias FATAL, 50\n",
    "log_verbosity_is_error = \\\n",
    "  logging.get_verbosity() == logging.ERROR # 40\n",
    "log_verbosity_is_warn = \\\n",
    "  logging.get_verbosity() == logging.WARNING # alias WARN, 30\n",
    "log_verbosity_is_info = \\\n",
    "  logging.get_verbosity() == logging.INFO # 20\n",
    "log_verbosity_is_debug = \\\n",
    "  logging.get_verbosity() == logging.DEBUG # 10\n",
    "\n",
    "print( \"The statement, 'logging verbosity is CRITICAL' \" + \\\n",
    "      f\"is {log_verbosity_is_critical}\")\n",
    "print( \"The statement, 'logging verbosity is    ERROR' \" + \\\n",
    "      f\"is {log_verbosity_is_error}\")\n",
    "print( \"The statement, 'logging verbosity is  WARNING' \" + \\\n",
    "      f\"is {log_verbosity_is_warn}\")\n",
    "print( \"The statement, 'logging verbosity is     INFO' \" + \\\n",
    "      f\"is {log_verbosity_is_info}\")\n",
    "print( \"The statement, 'logging verbosity is    DEBUG' \" + \\\n",
    "      f\"is {log_verbosity_is_debug}\")\n",
    "\n",
    "print()\n",
    "\n",
    "init_log_verbosity = logging.get_verbosity()\n",
    "print(f\"The value of logging.get_verbosity() is: {init_log_verbosity}\")\n",
    "\n",
    "print()\n",
    "\n",
    "init_t_n_a_w = os.environ.get('TRANSFORMERS_NO_ADVISORY_WARNINGS')\n",
    "print(f\"TRANSFORMERS_NO_ADIVSORY_WARNINGS: {init_t_n_a_w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a66b11-e35e-4663-9bcc-602bd3cd060d",
   "metadata": {},
   "source": [
    "### Actual Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671b535-247a-4510-9a09-d8db4fbeab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "!powershell -c (Get-Date -UFormat \\\"%s_%Y-%m-%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5dca6b-d4ca-440e-bdd2-71869263c6d7",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c550c2-eee7-4393-9f92-1a25f4fcc2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ref1 = \"https://web.archive.org/web/20240530051418/\" + \\\n",
    "#+        \"https://stackoverflow.com/questions/73221277/\" + \\\n",
    "#+        \"python-hugging-face-warning\"\n",
    "#  ref2 = \"https://web.archive.org/web/20240530051559/\" + \\\n",
    "#+        \"https://huggingface.co/docs/transformers/en/\" + \\\n",
    "#+        \"main_classes/logging\"\n",
    "\n",
    "##  Haven't tried this, because the logging seemed easier,\n",
    "##+ and the logging worked\n",
    "#os.environ(\"TRANSFORMERS_NO_ADVISORY_WARNINGS\") = 1\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "summarizer = pipeline('summarization', model=model, tokenizer=tokenizer)\n",
    "\n",
    "prediction_list = []\n",
    "reference_list = []\n",
    "\n",
    "tic = timeit.default_timer()\n",
    "\n",
    "for sample_num in range(len(dataset['test'])):\n",
    "  this_sample = dataset['test'][sample_num]\n",
    "  \n",
    "  #print(f\"dialogue: \\n{this_sample['dialogue']}\\n---------------\")\n",
    "\n",
    "  grnd_summary = this_sample['summary']\n",
    "  res = summarizer(this_sample['dialogue'])\n",
    "  res_summary = res[0]['summary_text']\n",
    "  \n",
    "  #print(f\"human-genratd summary:\\n{grnd_summary}\")\n",
    "  #print(f\"flan-t5-small summary:\\n{res_summary}\")\n",
    "  \n",
    "  reference_list.append(grnd_summary)\n",
    "  prediction_list.append(res_summary)\n",
    "##endof:  for sample_num in range(len(dataset['test']))\n",
    "\n",
    "toc = timeit.default_timer()\n",
    "\n",
    "baseline_duration = toc - tic\n",
    "\n",
    "print( \"Getting things ready for scoring\")\n",
    "print(f\"took {toc - tic:0.4f} seconds.\")\n",
    "\n",
    "print(\"\\n\\n---------- ROUGE SCORES ----------\")\n",
    "\n",
    "rouge = load_metric('rouge', trust_remote_code=True)\n",
    "  #  Set trust_remote_code=False to see the warning,\n",
    "  #+ deprecation, and what to change to.\n",
    "\n",
    "results = rouge.compute(predictions=prediction_list,\n",
    "                        references=reference_list,\n",
    "                        use_aggregator=True)\n",
    "\n",
    "# >>> print(list(results.keys()))\n",
    "# ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "\n",
    "print()\n",
    "print(\"ROUGE-1 results\")\n",
    "pprint.pp(results['rouge1'])\n",
    "print()\n",
    "print(\"ROUGE-2 results\")\n",
    "pprint.pp(results['rouge2'])\n",
    "print()\n",
    "print(\"ROUGE-L results\")\n",
    "pprint.pp(results['rougeL'])\n",
    "print()\n",
    "print(\"ROUGE-Lsum results\")\n",
    "pprint.pp(results['rougeLsum'])\n",
    "\n",
    "##  Haven't tried this, because the logging seemed easier,\n",
    "##+ and the logging worked\n",
    "# os.environ(\"TRANSFORMERS_NO_ADVISORY_WARNINGS\") = init_t_n_a_w\n",
    "\n",
    "logging.set_verbosity(init_log_verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4df533-4ce4-45d6-9966-50cc2d862108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  this time, put it in manually\n",
    "do_enter_duration_manually = True\n",
    "\n",
    "if do_enter_duration_manually:\n",
    "    baseline_duration = 1161.2583 # remember to type in your number, if needed\n",
    "##endof:  if do_enter_duration_manually\n",
    "\n",
    "print(\"Running baseline inference (using the test set)\")\n",
    "print(f\"took {format_timespan(baseline_duration)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df187315-53a9-4f66-a9f3-17db67512ddd",
   "metadata": {},
   "source": [
    "### Trainer - the Actual Trainer Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4feb5c-bb3c-444d-8814-6efa2317ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer( model=model,\n",
    "                      train_dataset=dataset['train'],\n",
    "                      eval_dataset=dataset['evaluation'],\n",
    "                      peft_config=peft_config,\n",
    "                      tokenizer=tokenizer,\n",
    "                      packing=True,\n",
    "                      formatting_func=prompt_instruction_format,\n",
    "                      args=training_args,\n",
    "                    )\n",
    "##  Warnings are below output.\n",
    "\n",
    "##  Ended up not using this.\n",
    "#                      max_seq_length=675\n",
    "#          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25010c2f-3f36-47b8-aad4-d5beec083d7c",
   "metadata": {},
   "source": [
    "First time warnings from the code above (as it still is).\n",
    "\n",
    "        \n",
    ">        WARNING:bitsandbytes.cextension:The installed version of bitsandbytes \\\n",
    ">         was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, \\\n",
    ">         and GPU quantization are unavailable.\n",
    ">        C:\\Users\\bballdave025\\.conda\\envs\\rwkv-lora-pat\\lib\\site-packages\\trl\\\\\n",
    ">         trainer\\sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` \\\n",
    ">        argument to the SFTTrainer, this will default to 512\n",
    ">         warnings.warn(\n",
    ">        \n",
    ">        [ > Generating train split: 6143/0 [00:04<00:00, 2034.36 examples/s] ]\n",
    ">        \n",
    ">        Token indices sequence length is longer than the specified maximum sequence \\\n",
    ">         length for this model (657 > 512). Running this sequence through the model \\\n",
    ">         will result in indexing errors\n",
    ">        \n",
    ">        [ > Generating train split: 355/0 [00:00<00:00, 6.10 examples/s] ]\n",
    "\n",
    "DWB Note\n",
    "\n",
    "<strike>So, I'm changing the `max_seq_length`.</strike> \n",
    "Maybe I should just throw out the offender(s) \n",
    "(along with the blank one that's in there somewhere),\n",
    "but I'll just continue as is.\n",
    "\n",
    "Actually, it appears I didn't run the updated cell, \n",
    "(with `max_seq_length=675`), since the\n",
    "Warning and Advice are still there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d556f5-92e7-4bdc-bbd0-73d643050446",
   "metadata": {},
   "source": [
    "## Let's Train This LoRA Thing and See How It Does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376a82f8-7f45-454c-b301-a591298171c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # No need to do this again\n",
    "# !powershell -c (Get-Date -UFormat \\\"%s_%Y-%m-%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'\n",
    "###  DWB went in and renamed `profile.ps1` to `NOT-USING_-_pro_file_-_now.ps1.bak`\n",
    "###+ That should get rid of our errors from `powershell`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd1014-d324-4293-930d-0ead8fdba381",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c6264-aab2-4a36-aa8b-71aa69c9719f",
   "metadata": {},
   "source": [
    "At about `1717063394_2024-05-30T100314-0600`, DWB went in and \n",
    "renamed `profile.ps1` to `NOT-USING_-_pro_file_-_now.ps1.bak`\n",
    "That should get rid of our errors from `powershell`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c15de9-adb4-433d-8a01-e633571bb076",
   "metadata": {},
   "source": [
    "### The long-time-taking training code is just below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4971f967-9d95-4606-9cc7-748834836b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timeit.default_timer()\n",
    "trainer.train()\n",
    "toc = timeit.default_timer()\n",
    "print(f\"tic: {tic}\")\n",
    "print(f\"toc: {toc}\")\n",
    "training_duration = toc - tic\n",
    "print(f\"Training took {toc - tic:0.4f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749bd2db-709b-45bb-b9c4-acb820cbd5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time, put it in by hand\n",
    "do_by_hand = True\n",
    "\n",
    "if do_by_hand:\n",
    "  training_duration = 11087.7452 ## make sure to enter your value\n",
    "##endof:  if do_by_hand\n",
    "print( \"Training with LoRA (and with the other info as above)\")\n",
    "print(f\"took {format_timespan(training_duration)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5127700-78f2-4f17-b677-8d29789d30db",
   "metadata": {},
   "source": [
    "#### @todo : consolidate \"the other info as above\"\n",
    "\n",
    "I'm talking about the numbers of data points, tokens, whatever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ca0bc-360c-4708-a1a8-c33c5f3f731f",
   "metadata": {},
   "source": [
    "#### Any Comments / Things to Try (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d76966d-3466-463a-83fa-e21381b5365a",
   "metadata": {},
   "source": [
    "We passed an evaluation set (parameter ``) to the `trainer`.\n",
    "How can we see information about that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab8c97-c9c2-45fa-9522-824ab8593630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# space for doing trainer stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c752dbc0-235a-4d49-9e2d-e999b05b35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more space for doing trainer stuff, if that happens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e62ce34-a826-4a83-8b2d-9acede4ff2a2",
   "metadata": {},
   "source": [
    "## Save the Trainer to Hugging Face and Get Our Updated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80265d1c-b65b-4863-8ac3-34e066c86522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Don't need this again\n",
    "# !powershell -c (Get-Date -UFormat \\\"%s_%Y-%m-%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c105c8-8182-4c12-9ce6-1016ee4787c5",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`1717078324_2024-05-30T141204-0600`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e0693e-579b-4a82-a2c2-2d3dc1e6f900",
   "metadata": {},
   "source": [
    "I'm following the [(archived) tutorial from Mehul Gupta on Medium](https://web.archive.org/web/20240522140323/https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578); since it's archived, you can follow exactly what I'm doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e5a45e-1890-415c-9da3-9f43fa0969df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will come up with a dialog box with text entry.\n",
    "\n",
    "# Use the write token, here.\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c99a5c-2739-49f6-9a4b-02c087e4c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer and create a tokenizer model card\n",
    "tokenizer.save_pretrained('testing')\n",
    "  #  used 'testing' first - I don't like how vague that is.\n",
    "  #+ I deleted the repo\n",
    "  # using 'dwb-first-lora-test-flan-t5-guptal' broke it\n",
    "  #  actually, I think deleting output from my main model\n",
    "  #+ page broke it.\n",
    "\n",
    "# Create the trainer model card\n",
    "trainer.create_model_card()\n",
    "\n",
    "# Push the results to the Hugging Face Hub\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f21397-d453-4434-bbb1-85135d4a76c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378b7818-fb90-4517-8b80-c7eb6acaa6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b44f56-58df-4abb-b279-72d469d75b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a022c0-760d-4fcb-bb71-a05e322598cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207707c8-1920-446a-bfa9-9c8ee9480a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73a6d8-1a9c-4f37-85c8-f97a9664804e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88281b15-7070-44dd-84e4-aedefa00293d",
   "metadata": {},
   "source": [
    "## Evaluation on the Test Set and Comparison to Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d123c-412a-4195-8820-838590b814a2",
   "metadata": {},
   "source": [
    "#### Verbosity stuff - get rid of the nice advice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a7666e-206c-418a-aec8-27b0fd4de3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!powershell -c (Get-Date -UFormat \\\"%s_%Y-%m-%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f710cdea-0123-4f96-8f29-e2ea8144e318",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde59af6-c15a-4494-b49e-87019a70715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bballdave025@MYMACHINE /cygdrive/c/Users/bballdave025/.conda/envs/rwkv-lora-pat/Lib/site-packages/peft/utils\n",
    "# $ date +'%s_%Y-%m-%dT%H%M%S%z'\n",
    "# 1717049876_2024-05-30T001756-0600\n",
    "\n",
    "log_verbosity_is_critical = \\\n",
    "  logging.get_verbosity() == logging.CRITICAL # alias FATAL, 50\n",
    "log_verbosity_is_error = \\\n",
    "  logging.get_verbosity() == logging.ERROR # 40\n",
    "log_verbosity_is_warn = \\\n",
    "  logging.get_verbosity() == logging.WARNING # alias WARN, 30\n",
    "log_verbosity_is_info = \\\n",
    "  logging.get_verbosity() == logging.INFO # 20\n",
    "log_verbosity_is_debug = \\\n",
    "  logging.get_verbosity() == logging.DEBUG # 10\n",
    "\n",
    "print( \"The statement, 'logging verbosity is CRITICAL' \" + \\\n",
    "      f\"is {log_verbosity_is_critical}\")\n",
    "print( \"The statement, 'logging verbosity is    ERROR' \" + \\\n",
    "      f\"is {log_verbosity_is_error}\")\n",
    "print( \"The statement, 'logging verbosity is  WARNING' \" + \\\n",
    "      f\"is {log_verbosity_is_warn}\")\n",
    "print( \"The statement, 'logging verbosity is     INFO' \" + \\\n",
    "      f\"is {log_verbosity_is_info}\")\n",
    "print( \"The statement, 'logging verbosity is    DEBUG' \" + \\\n",
    "      f\"is {log_verbosity_is_debug}\")\n",
    "\n",
    "print()\n",
    "\n",
    "init_log_verbosity = logging.get_verbosity()\n",
    "print(f\"The value of logging.get_verbosity() is: {init_log_verbosity}\")\n",
    "\n",
    "print()\n",
    "\n",
    "init_t_n_a_w = os.environ.get('TRANSFORMERS_NO_ADVISORY_WARNINGS')\n",
    "print(f\"TRANSFORMERS_NO_ADIVSORY_WARNINGS: {init_t_n_a_w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf8db54-8003-429f-8abd-cfb09a1af80b",
   "metadata": {},
   "source": [
    "### Here's the actual evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d3eeb-e238-43bf-ada2-3d7ccaf10b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!powershell -c (Get-Date -UFormat \\\"%s_%Y-%m-%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d0f187-2402-44a4-8322-f899df42fab4",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a522d7-6285-4b9d-b619-ea0f454f2cae",
   "metadata": {},
   "source": [
    "<b>!!! NOTE !!!</b> I'm going to use `tat` (with an underscore\n",
    "or undescores before, after, or surrounding the variable names)\n",
    "to indicate 'testing-after-training'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ad3bd-0921-4753-8c6c-175bffc23b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  I'm going to use 'tat' for testing-after-training\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "summarizer = pipeline('summarization', model=model, tokenizer=tokenizer)\n",
    "\n",
    "prediction_tat_list = []\n",
    "reference_tat_list = []\n",
    "\n",
    "tic = timeit.default_timer()\n",
    "\n",
    "for sample_num in range(len(dataset['test'])):\n",
    "  this_sample = dataset['test'][sample_num]\n",
    "  \n",
    "  #print(f\"dialogue: \\n{this_sample['dialogue']}\\n---------------\")\n",
    "\n",
    "  grnd_tat_summary = this_sample['summary']\n",
    "  res_tat = summarizer(this_sample['dialogue'])\n",
    "  res_tat_summary = res_tat[0]['summary_text']\n",
    "  \n",
    "  #print(f\"human-genratd summary:\\n{grnd_tat_summary}\")\n",
    "  #print(f\"flan-t5-small summary:\\n{res_tat_summary}\")\n",
    "  \n",
    "  reference_tat_list.append(grnd_tat_summary)\n",
    "  prediction_tat_list.append(res_tat_summary)\n",
    "##endof:  for sample_num in range(len(dataset['test']))\n",
    "\n",
    "toc = timeit.default_timer()\n",
    "\n",
    "print( \"Getting things ready for scoring (after training)\")\n",
    "print(f\"took {toc - tic:0.4f} seconds.\")\n",
    "\n",
    "print(\"\\n\\n---------- ROUGE SCORES ----------\")\n",
    "\n",
    "rouge = load_metric('rouge', trust_remote_code=True)\n",
    "  #  Set trust_remote_code=False to see the warning,\n",
    "  #+ deprecation, and what to change to.\n",
    "\n",
    "results_tat = rouge.compute(\n",
    "                  predictions=prediction_tat_list,\n",
    "                  references=reference_tat_list,\n",
    "                  use_aggregator=True\n",
    ")\n",
    "\n",
    "# >>> print(list(results_tat.keys()))\n",
    "# ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "\n",
    "print()\n",
    "print(\"ROUGE-1 results\")\n",
    "pprint.pp(results_tat['rouge1'])\n",
    "print()\n",
    "print(\"ROUGE-2 results\")\n",
    "pprint.pp(results_tat['rouge2'])\n",
    "print()\n",
    "print(\"ROUGE-L results\")\n",
    "pprint.pp(results_tat['rougeL'])\n",
    "print()\n",
    "print(\"ROUGE-Lsum results\")\n",
    "pprint.pp(results_tat['rougeLsum'])\n",
    "\n",
    "logging.set_verbosity(init_log_verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61791378-d26a-4399-a2eb-dba17715791f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff65b106-c9d2-4936-9d5a-fa1d5fa1d10d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b8950-a5ba-4270-9053-5b154d1529bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5526ab-3d35-4b79-a18a-0cca17e5f7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be8033-0a75-4941-aa99-dddc3f972b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9568b7-9ee5-4019-8781-89570b04f947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e92f0e-4c74-44b5-bec1-f0161ee84266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
