{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d7b071-3c52-4189-81e4-2fe25bb2f61d",
   "metadata": {},
   "source": [
    "# First Full LoRA Trial with Transformer Now on Google CoLab\n",
    "\n",
    "Starting with going through what I've done as well as finishing the task of getting my LoRA-fine-tuned model from Hugging Face and running inference on it (i.e. testing it using the test set). See the first timestamp below for the new timing. By the way, I've shut down and rebooted the compy here in the corner with the three screens).\n",
    "\n",
    "## peft (for LoRA) and FLAN-T5-small for the LLM\n",
    "\n",
    "I'm following what seems to be a great tutorial from Mehul Gupta,\n",
    "\n",
    "> https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578\n",
    "> \n",
    "> https://web.archive.org/web/20240522140323/https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578\n",
    "\n",
    "I'm doing this to prepare creating a LoRA for RWKV ( <strike>@todo</strike> @DONE  put links in [here](#Notes-Looking-Forward-to-LoRA-on-RWKV) ) so as to fine-tune it for Pat's OLECT-LM stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab0319e-1d1e-4fa3-9153-e8c26344e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "!date +'%s_%Y%m%dT%H%M%S%z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8476aae1-0e49-4094-8eb8-6bf0d51acbc7",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2a1051-3b8f-442b-85f2-c6609ce45369",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "(Feel free to drop down to the [TL;DR](#Install-TL;DR) section.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dca441-c60d-4f05-8cf3-9f3271c28fb8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Detailed stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61348879-fe55-4100-9375-93a516c31630",
   "metadata": {},
   "source": [
    "My `environment.yml` file will have its contents listed below. It should have everything needed for an install anywhere. The directory should have a `full_environment.yml`, which includes everything for the environment on Windows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf082676-868d-46ae-8f4b-3ae1d670e076",
   "metadata": {},
   "source": [
    "You can change `do_want_to_read_realtime` to `True` if you really want to see the file as it is now. One case of this would be that you think `environment.yml` has been changed since this notebook was written. The file contents as of the time of my writing this notebook should be in a markdown cell beneath the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d596bcde-28a3-41a7-8ebf-3676c3005c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_want_to_read_realtime = False\n",
    "\n",
    "if do_want_to_read_realtime:\n",
    "    with open(\"environment.yml\", 'r', encoding='utf-8') as fh:\n",
    "        while True:\n",
    "            line = fh.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            ##endof:  if not line\n",
    "            print(line.replace(\"\\n\", \"\"))\n",
    "        ##endof:  while True\n",
    "    ##endof:  with open ... fh\n",
    "##endof:  if do_want_to_read_realtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1cf350-15a1-4c21-a969-e4751fe770b6",
   "metadata": {},
   "source": [
    "For the CPU, I got the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49530485-e077-49bb-a5ff-a6fa3bf4cb0c",
   "metadata": {},
   "source": [
    "```\n",
    "# @file: environment.yml\n",
    "# @since 2024-06-03\n",
    "## 1717411989_2024-06-03T105309-0600\n",
    "## IMPORTANT NOTES\n",
    "##\n",
    "##  A couple of installations were made from git repos. \n",
    "##     >pip install git+https://github.com/huggingface/peft.git\n",
    "##     >pip install git+https://github.com/nexplorer-3e/qwqfetch\n",
    "##\n",
    "##  The commit info will be important for reproducibility.\n",
    "##\n",
    "##-----\n",
    "##  qwqfetch   for system info\n",
    "##\n",
    "##   Resolved https://github.com/nexplorer-3e/qwqfetch \\\n",
    "##       to commit f72d222e2fff5ffea9f4e4b3a203e4c4d9e8cf00\n",
    "##   Successfully installed qwqfetch-0.0.0\n",
    "##\n",
    "#\n",
    "##-----\n",
    "##  peft: I installed PEFT among other things, but I'm picking out \n",
    "##+       stuff relevant to peft. PEFT has LoRA in it.\n",
    "##\n",
    "##   Resolved https://github.com/huggingface/peft.git \\\n",
    "##       to commit e7b75070c72a88f0f7926cc6872858a2c5f0090d\n",
    "## Successfully built peft\n",
    "#\n",
    "#\n",
    "\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - python=3.10.14\n",
    "  - pip=24.0\n",
    "  - pip:\n",
    "      - accelerate==0.30.1\n",
    "      - bitsandbytes==0.43.1\n",
    "      - datasets==2.19.1\n",
    "      - evaluate==0.4.2\n",
    "      - huggingface-hub==0.23.2\n",
    "      - humanfriendly==10.0\n",
    "      - jupyter==1.0.0\n",
    "      - nltk==3.8.1\n",
    "      - peft==0.11.2.dev0\n",
    "      - py-cpuinfo==9.0.0\n",
    "      - pylspci==0.4.3\n",
    "      - qwqfetch==0.0.0\n",
    "      - rouge-score==0.1.2\n",
    "      - tensorflow-cpu==2.16.1\n",
    "      - torch==2.3.0\n",
    "      - transformers==4.41.1\n",
    "      - trl==0.8.6\n",
    "      - wmi==1.5.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45020ba-4a6a-4b77-8cff-558f1d9aba0c",
   "metadata": {},
   "source": [
    "For CoLab, I got the following"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02130710-14a2-4c37-8b78-acb85e85d702",
   "metadata": {},
   "source": [
    "Put CoLab `environment-colab.yml` that gets `conda env export`-ed here\n",
    "\n",
    "```\n",
    "blah\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1a7e50-0f7e-4299-8741-daa3c7b1e256",
   "metadata": {},
   "source": [
    "What should probably work for any install on CoLab comes in the next executable cells - the ones with `!pip install` ... I hope that it doesn't automatically read my `environment.yml` file and build it, because my `environment.yml` file is made for running on a CPU. What's more, I ran it on a CPU on Windows, so I'm not sure how it will perform with Linux(R)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b765b467-62c1-477f-9bfd-36b762b3b669",
   "metadata": {},
   "source": [
    "\\[Doing some stuff.\\]\n",
    "\n",
    "<br/>\n",
    "\n",
    "Okay, I'm going to commit this stuff with the `environment.yml` renamed to `environment-cpu.yml`. I'll create (and commit) a new `environment.yml` exactly the same as the one above, except with `tensorflow-cpu` replaced with `tensorflow`. This new, `tensorflow`-not-`tensorflow-cpu` environment file will also be copied to `environment-colab.yml`. (There will also be `full_environment-cpu.yml` and `full_environment-colab.yml` files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ef4532-71f6-41f8-91af-4c561bdcd69d",
   "metadata": {},
   "source": [
    "If nothing happened with the `environment.yml` (i.e. no environment was built, no packages were loaded), run the installs below. That should get you set up nicely for CoLab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5cc6f3-3cd1-492c-8104-f8537589887f",
   "metadata": {},
   "source": [
    "If packages aren't installed, you can either:\n",
    "\n",
    "1) (not preferable) install from `environment-colab.yml`.\n",
    "\n",
    "From a terminal/command prompt, run\n",
    "\n",
    "`conda env create -f environment-colab.yml`\n",
    "\n",
    "<b>OR</b> \n",
    "\n",
    "2) you can run the `!pip install` commands below for running things on Google CoLab (or any *NIX-type system, I think). <b>In my experience, at least some of the packages need to come from `pip` rather than from `conda` to work. I suggest using all from `pip`.</b> For the whole project, my suggestion is to run the Jupyter Notebook from inside a `conda environment`. For complete reproducibility, the Python version should be 3.10.14 and the `pip` version 24.0. The commands for the shell/command prompt could be\n",
    "\n",
    "```\n",
    "conda create -n my-env-lora python=3.10.14`\n",
    "\n",
    "conda activate my-env-lora\n",
    "```\n",
    "\n",
    "After the environment is activated, you can then run\n",
    "\n",
    "```\n",
    "pip install --upgrade pip==24.0\n",
    "  #  note that you should use the `--upgrade` flag whether\n",
    "  #+ upgrading or downgrading pip\n",
    "```\n",
    "\n",
    "To get started with this notebook stuff,\n",
    "\n",
    "`pip install jupyter`\n",
    "\n",
    "And run `jupyter notebook` to start using a blank notebook, or `jupyter notebook <notebook_base_name>.ipynb` to use an already-created notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257fb6a2-5828-41c3-b11c-6303ec6c6d1e",
   "metadata": {},
   "source": [
    "<b>Note</b>: if you are in this Jupyter Notebook but aren't in a conda environment ... and if you know enough to realize that and to know what the following commands do, you can uncomment the commands below to get your `conda` environment set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b474f89-e7a0-4698-9284-93fb37cba89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## not going to do complicated subprocess stuff here. Sorry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dfa050-0b52-4bc9-8c2f-138abaf79f68",
   "metadata": {},
   "source": [
    "### Install TL;DR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d97161-cab9-457b-b4f1-34befe9f9131",
   "metadata": {},
   "source": [
    "Once you have things ready and a jupyter notebook running, you can do the `!pip install` commands that follow. That is, you should run the commands unless you used \n",
    "\n",
    "`conda env create -f <environment-filename>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edac2e99-bb43-4c5f-943e-9e60993471ef",
   "metadata": {},
   "source": [
    "These commands below should get you set up nicely for CoLab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613563b3-4594-4ae9-aaa7-da1665ea832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --update pip==24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac73dc63-02ac-4071-8219-872dd541285e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate bitsandbytes evaluate datasets huggingface-hub\n",
    "!pip install humanfriendly nltk py-cpuinfo pylspci rouge-score\n",
    "!pip install tensorflow torch transformers trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af69848-fc20-4533-ad81-9e569c07fc2c",
   "metadata": {},
   "source": [
    "Trying this next one on its own, since it might fail (we're not on Windows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e98a12a-36b4-4d51-9050-1925ac72b91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wmi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7660f15a-189d-4532-8229-e458547dda66",
   "metadata": {},
   "source": [
    "And now, for the installs from GitHub repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a93b44-38de-4961-82dd-48bc5c1a4d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f5d1f-1839-479a-84a8-5a15c17fb664",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/nexplorer-3e/qwqfetch.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb51da5-6c05-4870-931c-7068e575ac99",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7c2afe-9b5f-4ee6-9db6-9f8d2cf6b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "from random import randrange\n",
    "import torch\n",
    "from transformers import AutoTokenizer, \\\n",
    "                         AutoModelForSeq2SeqLM, \\\n",
    "                         AutoModelForCausalLM, \\\n",
    "                         TrainingArguments, \\\n",
    "                         pipeline\n",
    "from transformers.utils import logging\n",
    "from peft import LoraConfig, \\\n",
    "                 prepare_model_for_kbit_training, \\\n",
    "                 get_peft_model, \\\n",
    "                 AutoPeftModelForSeq2SeqLM, \\\n",
    "                 AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login, notebook_login\n",
    "\n",
    "from datasets import load_metric\n",
    "from evaluate import load as evaluate_dot_load\n",
    "import nltk\n",
    "import rouge_score\n",
    "from rouge_score import rouge_scorer, scoring\n",
    "\n",
    "import pickle\n",
    "import pprint\n",
    "import re\n",
    "import timeit\n",
    "from humanfriendly import format_timespan\n",
    "import os\n",
    "\n",
    "## my module(s), now just in the working directory as .PY files\n",
    "import system_info_as_script\n",
    "import dwb_rouge_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bfe2f4-b1b4-4158-b3c6-048c487514d2",
   "metadata": {},
   "source": [
    "## Load the training and test dataset along with the LLM and its tokenizer\n",
    "\n",
    "The LLM will be fine-tuned. It seems the tokenizer will also be fine-tuned, \n",
    "but I'm not sure \n",
    "\n",
    "<b>Why aren't we loading the validation set?</b> <strike>(I don't know; that's not a teaching question.)</strike> \n",
    "\n",
    "<b>Update:</b> It seems that validation-set use with the trainer wasn't part of the example.\n",
    "\n",
    "I've tried to make use of it (the validation set) with the `trainer`. We'll see how it goes.\n",
    "\n",
    "<b>Update:</b> It worked fine, though its loss is lower than the training set's loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6043d426-bcee-4539-a9ea-4b7c4c3dccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Need to install  datasets  (i.e. the `datasets` module/package)\n",
    "#+ from `pip`, not `conda`. I'll do all from `pip`. \n",
    "#+\n",
    "#+ cf. \n",
    "#+     arch_ref_1 = \"https://web.archive.org/web/20240522150357/\" + \\\n",
    "#+                  \"https://stackoverflow.com/questions/77433096/\" + \\\n",
    "#+                  \"notimplementederror-loading-a-dataset-\" + \\\n",
    "#+                  \"cached-in-a-localfilesystem-is-not-suppor\"\n",
    "#+\n",
    "#+ Also useful might be\n",
    "#+     arch_ref_2 = \"https://web.archive.org/web/20240522150310/\" + \\\n",
    "#+                  \"https://stackoverflow.com/questions/76340743/\" + \\\n",
    "#+                  \"huggingface-load-datasets-gives-\" + \\\n",
    "#+                  \"notimplementederror-cannot-error\"\n",
    "#\n",
    "data_files = {'train':'samsum-train.json', \n",
    "              'evaluation':'samsum-validation.json',\n",
    "              'test':'samsum-test.json'}\n",
    "dataset = load_dataset('json', data_files=data_files)\n",
    "\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "model_load_tic = timeit.default_timer()\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model_load_toc = timeit.default_timer()\n",
    "\n",
    "model_load_duration = model_load_toc - model_load_tic\n",
    "\n",
    "print(f\"Loading the original model, {model_name}\")\n",
    "print(f\"took {model_load_toc - model_load_tic:0.4f} seconds.\")\n",
    "\n",
    "model_load_time_str = format_timespan(model_load_duration)\n",
    "\n",
    "print(f\"which equates to {model_load_time_str}\")\n",
    "\n",
    "#  Next line makes training faster but a little less accurate\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer_tic = timeit.default_timer()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                          trust_remote_code=True)\n",
    "tokenizer_toc = timeit.default_timer()\n",
    "\n",
    "tokenizer_duration = tokenizer_toc - tokenizer_tic\n",
    "\n",
    "print()\n",
    "print(\"Getting the original tokenizer\")\n",
    "print(f\"took {tokenizer_toc - tokenizer_tic:0.4f} seconds.\")\n",
    "\n",
    "tokenizer_time_str = format_timespan(tokenizer_duration)\n",
    "\n",
    "print(f\"which equates to {tokenizer_time_str}\")\n",
    "\n",
    "#  padding instructions for the tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43c61a9-aeb0-4f71-a5a4-7a96d6307083",
   "metadata": {},
   "source": [
    "I wonder if those lines,\n",
    "\n",
    "```\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "```\n",
    "\n",
    "will be the same for RWKV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9703511a-dd55-4ff7-bbfa-9894175daf04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Notes from trying to get rid of weird output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8303ce71-fb16-4ee6-bbdb-04ad31012762",
   "metadata": {},
   "source": [
    "I've thought about changing the line\n",
    "\n",
    "`model = AutoModelForSeq2SeqLM.from_pretrained(model_name)`\n",
    "\n",
    "to match the `peft` configuration, i.e.\n",
    "\n",
    "> `peft_config = LoraConfig( lora_alpha=16,`<br/>\n",
    "> `                          lora_dropout=0.1,`<br/>\n",
    "> `                          r=64,`<br/>\n",
    "> `                          bias='none',`<br/>\n",
    "> `                          task_type='CAUSAL_LM',`<br/>\n",
    "> `)`\n",
    "\n",
    "I've thought about using\n",
    "\n",
    "`model = AutoModelForCausalLM.from_pretrained(model_name)`\n",
    "\n",
    "but every documentation I've\n",
    "consulted uses the `Seq2SeqLM`. e.g.\n",
    "\n",
    "```\n",
    "doc1 = \"https://web.archive.org/web/20240506213344/\" + \\\\\n",
    "       \"https://huggingface.co/docs/transformers/en/\" + \\\\\n",
    "       \"model_doc/flan-t5\"\n",
    "```\n",
    "\n",
    "Also, there is the info from\n",
    "\n",
    "```\n",
    "doc2=\"https://huggingface.co/transformers/v3.0.2/model_doc/t5.html\"\n",
    "```\n",
    "\n",
    "> T5 is an encoder-decoder model pre-trained on a multi-task\n",
    "> mixture of unsupervised and supervised tasks and for which \n",
    "> <b>each task is converted into a text-to-text format.</b>\n",
    "\n",
    "Something similar is in the paper abstract for\n",
    "\n",
    "https://arxiv.org/pdf/1910.10683.pdf\n",
    "\n",
    "Colin Raffel et al. \n",
    "\"Exploring the Limits of Transfer Learning with a Unified \n",
    "<b>Text-to-Text</b> Transformer\".\n",
    "online. arXiv:cs.LG.1910.10683v4. 19 Sep 2023.\n",
    "retrieved 06 June 2024\n",
    "\n",
    "which is cited in `doc2`\n",
    "\n",
    "> In this paper, we explore the landscape of transfer learning\n",
    "> techniques for NLP by introducing a unified framework that\n",
    "> converts all text-based language problems into a\n",
    "> <b>text-to-text format</b>.\n",
    "\n",
    "(All emphasis is mine, DWB.)\n",
    "\n",
    "#### Google Results\n",
    "\n",
    "As of today (2024-06-06), a Google search for\n",
    "\n",
    "`\"AutoModelForCausalLM from_pretrained google flan-t5-small\"`\n",
    "\n",
    "(with quotes) returns\n",
    "\n",
    "> Your search - \"AutoModelForCausalLM from_pretrained google flan-t5-small\" - did not match any documents.\n",
    ">\n",
    "> Suggestions:\n",
    ">\n",
    "> - Make sure all words are spelled correctly.\n",
    "> - Try different keywords.\n",
    "> - Try more general keywords.\n",
    "\n",
    "whereas a Google search (again with quotes) for\n",
    "\n",
    "`\"AutoModelForSeq2SeqLM from_pretrained google flan-t5-small\"`\n",
    "\n",
    "returns\n",
    "\n",
    "> About 119 results (0.22 seconds)\n",
    "\n",
    "#### Trying the experiment\n",
    "\n",
    "With all that, I tried the line anyway. Using just the important lines\n",
    "\n",
    "`IN:`\n",
    "\n",
    "```\n",
    "model_name = \"google/flan-t5-small\"\n",
    "```\n",
    "...\n",
    "```\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "```\n",
    "\n",
    "`OUT:`\n",
    "\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "Cell In[4], line 29\n",
    "     25 model_load_tic = timeit.default_timer()\n",
    "     27 #model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "---> 29 model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "     30 model_load_toc = timeit.default_timer()\n",
    "     32 model_load_duration = model_load_toc - model_load_tic\n",
    "\n",
    "File ~\\.conda\\envs\\rwkv-lora-pat\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:566, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)\n",
    "    562     model_class = _get_model_class(config, cls._model_mapping)\n",
    "    563     return model_class.from_pretrained(\n",
    "    564         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n",
    "    565     )\n",
    "--> 566 raise ValueError(\n",
    "    567     f\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\n",
    "    568     f\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\n",
    "    569 )\n",
    "\n",
    "ValueError: Unrecognized configuration class <class 'transformers.models.t5.configuration_t5.T5Config'> for this kind of AutoModel: AutoModelForCausalLM.\n",
    "Model type should be one of BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, ElectraConfig, ErnieConfig, FalconConfig, FuyuConfig, GemmaConfig, GitConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, JambaConfig, JetMoeConfig, LlamaConfig, MambaConfig, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, OlmoConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc6b24-d617-48a1-8236-c3be16402c4a",
   "metadata": {},
   "source": [
    "### Trying some things I've been learning (architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a107cbdc-5a73-4371-8afe-faa2dc623c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aaf9b8-19fe-4255-81bb-47dfa40cfcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arch_str = str(model)\n",
    "\n",
    "with open(\"google_-flan-t5-small.model-architecture.txt\", 'w', encoding='utf-8') as fh:\n",
    "    fh.write(model_arch_str)\n",
    "##endof:  with open ... fh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6685b823-98dc-4b44-931a-6ead83f9b138",
   "metadata": {},
   "source": [
    "#### Some other saves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ee78d-a0ba-47c0-8e5b-1206452e7005",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_filename = \"lora_flan_t5_cpu_objects.pkl\"\n",
    "objects_to_pickle = []\n",
    "objects_to_pickle.append(model_arch_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39910a52-ded5-4be2-83ed-f6ba7b48fa0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Prompt and Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59156663-4b5b-4b60-8b37-627a9f297fc9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "For our SFT (<b>S</b>upervised <b>F</b>ine <b>T</b>uning) model, we use the `class trl.SFTTrainer`.\n",
    "\n",
    "I want to research this a bit, especially the `formatting_func` that we'll be passing to the `SFTTrainer`.\n",
    "\n",
    "First, though, some information about SFT. From the Hugging Face Documentation at https://huggingface.co/docs/trl/en/sft_trainer ([archived](https://web.archive.org/web/20240529140717/https://huggingface.co/docs/trl/en/sft_trainer))\n",
    "\n",
    "> Supervised fine-tuning (or SFT for short) is a crucial step in RLHF. In TRL we provide an easy-to-use API to create your SFT models and train them with few lines of code on your dataset.\n",
    "\n",
    "Though I won't be using the examples unless I get even more stuck, the next paragraph _has_ examples, and I'll put the paragraph here.\n",
    "\n",
    "> Check out a complete flexible example at [examples/scripts/sft.py](https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py) \\[[archived](https://web.archive.org/web/20240529140740/https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py)\\]. Experimental support for Vision Language Models is also included in the example [examples/scripts/vsft_llava.py](https://github.com/huggingface/trl/blob/main/examples/scripts/vsft_llava.py) \\[[archived](https://web.archive.org/web/20240529140738/https://github.com/huggingface/trl/blob/main/examples/scripts/vsft_llava.py)\\].\n",
    "\n",
    "RLHF ([archived wikipedia page](https://web.archive.org/web/20240529142205/https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback)) is <b>R</b>einforcement <b>L</b>earning from <b>H</b>uman <b>F</b>eedback. [TRL](https://huggingface.co/docs/trl/en/index#:~:text=TRL%20is%20a%20full%20stack,Policy%20Optimization%20(PPO)%20step.) ([archived]())       <b>T</b>ransfer <b>R</b>einforcement <b>L</b>earning, a library from Hugging Face.\n",
    "\n",
    "For the parameter, `formatting_func`, I can look ath the documentation site above (specifically [here](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=formatting_func%20(Optional)), at the GitHub repo for [the code](https://github.com/huggingface/trl/blob/main/trl/trainer/sft_trainer.py) (in the docstrings), or from my local `conda` environment, at `C:\\Users\\bballdave025\\.conda\\envs\\rwkv-lora-pat\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py`.\n",
    "\n",
    "Pulling code from the last one, I get\n",
    "\n",
    ">         formatting_func (`Optional[Callable]`):\n",
    ">            The formatting function to be used for creating the `ConstantLengthDataset`.\n",
    "\n",
    "That matches the first very well\n",
    "\n",
    "> <b>formatting_func</b> (`Optional[Callable]`) — The formatting function to be used for creating the `ConstantLengthDataset`.\n",
    "\n",
    "(A quick note: In this Jupyter Notebook environment, I could have typed `trainer = SFTTrainer(` and then <kbd>Shift</kbd> + <kbd>Tab</kbd> to find that same documentation.\n",
    "\n",
    "However, I think that more clarity is found at the [documentation for `ConstantLengthDataset](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=class%20trl.trainer.ConstantLengthDataset)\n",
    "\n",
    "> <b>formatting_func</b> (`Callable`, <b>optional</b>) — Function that formats the text before tokenization. Usually it is recommended to have follows a certain pattern such as `\"### Question: {question} ### Answer: {answer}\"`\n",
    "\n",
    "So, as we'll see the next code from  the tutorial, it basically is a prompt templater/formatter that matches the JSON. For example, we use `sample['dialogue']` to access the `dialogue` key/pair. That's what I got from all this stuff.\n",
    "\n",
    "Mehul Gupta himself stated\n",
    "\n",
    "> Next, using the Input and Output, we will create a prompt template which is a requirement by the SFTTrainer we will be using later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cdd3ac-f714-41ed-a1fb-c27395b9251d",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb36fc-783f-4f19-b213-69cc3dbf05af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_instruction_format(sample):\n",
    "    return f\"\"\" Instruction:\n",
    "      Use the Task below and the Input given to write the Response:\n",
    "\n",
    "      ### Task:\n",
    "      Summarize the Input\n",
    "\n",
    "      ### Input:\n",
    "      {sample['dialogue']}\n",
    "\n",
    "      ### Response:\n",
    "      {sample['summary']}\n",
    "      \"\"\"\n",
    "##endof:  prompt_instruction_format(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3622673e-df80-43da-9d5c-efd6085d301d",
   "metadata": {},
   "source": [
    "## Trainer - the LoRA Setup Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da39f5e-4e14-4eb5-ac66-c90a77472c64",
   "metadata": {},
   "source": [
    "#### Arguments and Configuration\n",
    "\n",
    "See [this section](#The-final-TrainingArguments-call---with-parameter-list) to see what I changed from the tutorial to get the evaluation set as part of training and to get a customized repo name. The couple of sections before it will give more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac040cf-2eb5-4eb5-91c7-2e47c54b990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  some arguments to pass to the trainer\n",
    "training_args = TrainingArguments( \n",
    "                    output_dir='output',\n",
    "                    num_train_epochs=1,\n",
    "                    per_device_train_batch_size=4,\n",
    "                    save_strategy='epoch',\n",
    "                    learning_rate=2e-4,\n",
    "                    do_eval=True,\n",
    "                    per_device_eval_batch_size=4,\n",
    "                    eval_strategy='epoch',\n",
    "                    hub_model_id=\"dwb-flan-t5-small-lora-ft-colab\",\n",
    "                    run_name=\"dwb-flan-samsum-run-colab-20240606-02\",\n",
    "                    #  has nodename (machine), when this param is\n",
    "                    #+ unset\n",
    "                    overwrite_output_dir=False,\n",
    "                    logging_strategy='steps',\n",
    "                    logging_steps=32,\n",
    ")\n",
    "\n",
    "#  the fine-tuning (peft for LoRA) stuff\n",
    "peft_config = LoraConfig( lora_alpha=16,\n",
    "                          lora_dropout=0.1,\n",
    "                          r=64,\n",
    "                          bias='none',\n",
    "                          task_type='CAUSAL_LM',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa2a84-c5f6-45ad-9d09-fb9f25fae9da",
   "metadata": {},
   "source": [
    "`task_type`, cf. https://github.com/huggingface/peft/blob/main/src/peft/config.py#L222 ([archived](https://web.archive.org/web/20240603151908/https://github.com/huggingface/peft/blob/main/src/peft/config.py))\n",
    "\n",
    ">        Args:\n",
    ">            peft_type (Union[[`~peft.utils.config.PeftType`], `str`]): The type of Peft method to use.\n",
    ">            task_type (Union[[`~peft.utils.config.TaskType`], `str`]): The type of task to perform.\n",
    ">            inference_mode (`bool`, defaults to `False`): Whether to use the Peft model in inference mode.\n",
    "\n",
    "After some searching using Cygwin\n",
    "\n",
    "```\n",
    "bballdave025@MYMACHINE /cygdrive/c/Users/bballdave025/.conda/envs/rwkv-lora-pat/Lib/site-packages/peft/utils\n",
    "$ ls -lah\n",
    "total 116K\n",
    "drwx------+ 1 bballdave025 bballdave025    0 May 28 21:09 .\n",
    "drwx------+ 1 bballdave025 bballdave025    0 May 28 21:09 ..\n",
    "-rwx------+ 1 bballdave025 bballdave025 2.0K May 28 21:09 __init__.py\n",
    "drwx------+ 1 bballdave025 bballdave025    0 May 28 21:09 __pycache__\n",
    "-rwx------+ 1 bballdave025 bballdave025 8.0K May 28 21:09 constants.py\n",
    "-rwx------+ 1 bballdave025 bballdave025 3.8K May 28 21:09 integrations.py\n",
    "-rwx------+ 1 bballdave025 bballdave025  17K May 28 21:09 loftq_utils.py\n",
    "-rwx------+ 1 bballdave025 bballdave025 9.7K May 28 21:09 merge_utils.py\n",
    "-rwx------+ 1 bballdave025 bballdave025  25K May 28 21:09 other.py\n",
    "-rwx------+ 1 bballdave025 bballdave025 2.2K May 28 21:09 peft_types.py\n",
    "-rwx------+ 1 bballdave025 bballdave025  21K May 28 21:09 save_and_load.py\n",
    "\n",
    "bballdave025@MYMACHINE /cygdrive/c/Users/bballdave025/.conda/envs/rwkv-lora-pat/Lib/site-packages/peft/utils\n",
    "$ grep -iIRHn \"TaskType\" .\n",
    "peft_types.py:60:class TaskType(str, enum.Enum):\n",
    "__init__.py:20:# from .config import PeftConfig, PeftType, PromptLearningConfig, TaskType\n",
    "__init__.py:22:from .peft_types import PeftType, TaskType\n",
    "\n",
    "bballdave025@MYMACHINE /cygdrive/c/Users/bballdave025/.conda/envs/rwkv-lora-pat/Lib/site-packages/peft/utils\n",
    "$\n",
    "```\n",
    "\n",
    "So, let's look at the `peft_types.py` file.\n",
    "\n",
    "The docstring for `class TaskType(str, enum.Enum)` is\n",
    "\n",
    "```\n",
    "    Enum class for the different types of tasks supported by PEFT.\n",
    "    \n",
    "    Overview of the supported task types:\n",
    "    - SEQ_CLS: Text classification.\n",
    "    - SEQ_2_SEQ_LM: Sequence-to-sequence language modeling.\n",
    "    - CAUSAL_LM: Causal language modeling.\n",
    "    - TOKEN_CLS: Token classification.\n",
    "    - QUESTION_ANS: Question answering.\n",
    "    - FEATURE_EXTRACTION: Feature extraction. Provides the hidden states which can be used as embeddings or features\n",
    "      for downstream tasks.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e178f3f-9dd6-409e-9a4d-c1c85611a8b9",
   "metadata": {},
   "source": [
    "### We're going to start timing stuff, so here's some system info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9606ed-e100-42e3-9ffc-cb1cd03004c8",
   "metadata": {},
   "source": [
    "`system_info_as_script.py` is a script I wrote with the help\n",
    "of a variety of StackOverflow and documentation sources.\n",
    "It should be in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14193c4-9040-47ea-9504-37ccb408de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "!date +'%s_%Y%m%dT%H%M%S%z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c21dc0c-1b97-45d5-b436-ff516116c7c5",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e51b0-8f09-4693-b991-e853ca3cfccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_info_as_script.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161c58b-3ef9-40e4-bcc4-eca0daab17e1",
   "metadata": {},
   "source": [
    "(Maybe try that `system_info_as_script.run()` command as `sudo` to see if we get any more info ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c5069-11b6-4041-8bce-7fa6b0dcf2f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ROUGE Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b9483-f8ff-47d7-ad94-b833f27d6e9e",
   "metadata": {},
   "source": [
    "Some references from the Google Research implementation\n",
    "\n",
    "https://pypi.org/project/rouge-score/\n",
    "\n",
    "https://web.archive.org/web/20240530231357/https://pypi.org/project/rouge-score/\n",
    "\n",
    "<br/>\n",
    "\n",
    "https://github.com/google-research/google-research/tree/master/rouge\n",
    "\n",
    "https://web.archive.org/web/20240530231412/https://github.com/google-research/google-research/tree/master/rouge\n",
    "\n",
    "<br/>\n",
    "\n",
    "Not the one I used:\n",
    "\n",
    "https://github.com/microsoft/nlp-recipes/blob/master/examples/text_summarization/summarization_evaluation.ipynb\n",
    "\n",
    "https://web.archive.org/web/20240530231709/https://github.com/microsoft/nlp-recipes/blob/master/examples/text_summarization/summarization_evaluation.ipynb\n",
    "\n",
    "<br/>\n",
    "\n",
    "Someone else made this other one, which I inspected but didn't use.\n",
    "\n",
    "https://pypi.org/project/rouge/\n",
    "\n",
    "https://web.archive.org/web/20240530232029/https://pypi.org/project/rouge/\n",
    "\n",
    "https://github.com/pltrdy/rouge\n",
    "\n",
    "https://web.archive.org/web/20240530232023/https://github.com/pltrdy/rouge\n",
    "\n",
    "but I think he defers to the rouge_score from Google."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bdb691-5847-467b-95b9-8a46c77ec1fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### My ROUGE Metrics incl SkipGrams but Not Using Now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907cdf67-55ac-41c6-a81c-7e89b4ddade8",
   "metadata": {},
   "source": [
    "I want to use the skip-grams score. Thanks to\n",
    "\n",
    "https://www.bomberbot.com/machine-learning/skip-bigrams-in-system/\n",
    "\n",
    "https://web.archive.org/web/20240530230949/https://www.bomberbot.com/machine-learning/skip-bigrams-in-system/\n",
    "\n",
    "I can do this as well as writing the code for the other metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d3d02-5c6d-4257-9604-40b8eeca2ada",
   "metadata": {},
   "source": [
    "##### Not used for now\n",
    "\n",
    "Focusing on the main goal. Quick and Reckless. My therapist would be so proud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0c97c3-8f28-4f86-935b-2cc0eb6171d2",
   "metadata": {},
   "source": [
    "### Documentation for my methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9287dd25-f5fe-45a2-b47c-7f8baaea0fe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import dwb_rouge_scores # done with all other exports\n",
    "\n",
    "sep_banner_1 = \"  \" + \"#\" + \"+\"*60 + \"#\"\n",
    "sep_banner_2 = \"     \" + \"#\" + \"~\"*30 + \"#\"\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(sep_banner_1)\n",
    "\n",
    "help(dwb_rouge_scores.dwb_rouge_n)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(sep_banner_1)\n",
    "print()\n",
    "print()\n",
    "\n",
    "help(dwb_rouge_scores.dwb_rouge_L)\n",
    "\n",
    "print()\n",
    "print(sep_banner_2)\n",
    "print()\n",
    "print(\"dwb_rouge_L needs dwb_lcs\")\n",
    "print()\n",
    "print(sep_banner_2)\n",
    "print()\n",
    "\n",
    "help(dwb_rouge_scores.dwb_lcs)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(sep_banner_1)\n",
    "print()\n",
    "print()\n",
    "\n",
    "help(dwb_rouge_scores.dwb_rouge_s)\n",
    "\n",
    "print()\n",
    "print(sep_banner_2)\n",
    "print()\n",
    "print(\"dwb_rouge_s needs dwb_skipngrams\")\n",
    "print()\n",
    "print(sep_banner_2)\n",
    "print()\n",
    "\n",
    "help(dwb_rouge_scores.dwb_skipngrams)\n",
    "\n",
    "print()\n",
    "print()\n",
    "print(sep_banner_1)\n",
    "print()\n",
    "print()\n",
    "\n",
    "help(dwb_rouge_scores.dwb_rouge_Lsum)\n",
    "\n",
    "print()\n",
    "print(sep_banner_2)\n",
    "print()\n",
    "print(\"dwb_rouge_Lsum just wraps google-research's rouge_score's\")\n",
    "print(\"(from `pip install rouge-score`) version of rougeLsum\")\n",
    "print()\n",
    "print()\n",
    "print(sep_banner_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a84433-dbb3-4e97-9b51-e0074b020f1c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Other useful ROUGE code - Run/Evaluate Code Even if You'll HIde It\n",
    "\n",
    "(found and created as I go along)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eb2eab-ee60-4970-9b32-314439e1b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_rouge_score_rough(this_rouge_str):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    \n",
    "    rouge_ret_str = this_rouge_str\n",
    "    \n",
    "    rouge_ret_str = re.sub(r\"([(,][ ]?)([0-9A-Za-z_]+[=])\",\n",
    "                            \"\\g<1>\\n     \\g<2>\",\n",
    "                           rouge_ret_str,\n",
    "                           flags=re.I|re.M\n",
    "    )\n",
    "    \n",
    "    rouge_ret_str = re.sub(r\"(.)([)])$\",\n",
    "                            \"\\g<1>\\n\\g<2>\",\n",
    "                           rouge_ret_str\n",
    "    )\n",
    "\n",
    "    rouge_ret_str = rouge_ret_str.replace(\n",
    "                                   \"precision=\",\n",
    "                                   \"     precision=\"\n",
    "                                ).replace(\n",
    "                                   \"recall=\",\n",
    "                                   \"     recall=\"\n",
    "                                ).replace(\n",
    "                                   \"fmeasure=\",\n",
    "                                   \"     fmeasure=\"\n",
    "    )\n",
    "    \n",
    "    return rouge_ret_str\n",
    "    \n",
    "##endof:  format_rouge_score_rough(<params>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fbbbdc-e2f8-4bf4-a3e7-e3b409c82f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rouge_scores(result, sample_num_or_header=None):\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    \n",
    "    print(\"\\n\\n---------- ROUGE SCORES ----------\")\n",
    "    if sample_num_or_header is None:\n",
    "        print(\"  --------- dialogue ----------\")\n",
    "    elif type(sample_num_or_header) is int:\n",
    "        print(f\"  --------- dialogue {sample_num_or_header+1} \" + \\\n",
    "               \"----------\")\n",
    "    else:\n",
    "        print(f\"  --------- {sample_num_or_header} ----------\")\n",
    "    ##endof:  if/else sample_num is None\n",
    "    print(\"ROUGE-1 results\")\n",
    "    rouge1_str = str(result['rouge1'])\n",
    "    print(format_rouge_score_rough(rouge1_str))\n",
    "    print(\"ROUGE-2 results\")\n",
    "    rouge2_str = str(result['rouge2'])\n",
    "    print(format_rouge_score_rough(rouge2_str))\n",
    "    print(\"ROUGE-L results\")\n",
    "    rougeL_str = str(result['rougeL'])\n",
    "    print(format_rouge_score_rough(rougeL_str))\n",
    "    print(\"ROUGE-Lsum results\")\n",
    "    rougeLsum_str = str(result['rougeLsum'])\n",
    "    print(format_rouge_score_rough(rougeLsum_str))\n",
    "##endof:  print_rouge_scores(<params>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afae864-111f-48e0-b477-f8583442b685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# #  From https://github.com/google-research/google-research/tree/master/rouge\n",
    "# #+ <strike>I can't see how to aggregate it, though I may have</strike>\n",
    "# #+ I found a resource at\n",
    "# #+  ref_gg_rg=\"https://github.com/huggingface/datasets/blob/\" + \\\n",
    "# #+            \"main/metrics/rouge/rouge.py\"\n",
    "# #+\n",
    "# #+ arch_gg_rg=\"https://web.archive.org/web/20240603192938/\" + \\\n",
    "# #+            \"https://github.com/huggingface/datasets/blob/\" + \\\n",
    "# #+            \"main/metrics/rouge/rouge.py\"\n",
    "#\n",
    "\n",
    "def compute_google_rouge_score(predictions, \n",
    "                               references, \n",
    "                               rouge_types=None, \n",
    "                               use_aggregator=True, \n",
    "                               use_stemmer=False):\n",
    "\n",
    "    '''\n",
    "    Figuring out the nice format of the deprecated method from\n",
    "    the googleresearch/rouge method it claims to be calling.\n",
    "    '''\n",
    "    \n",
    "    if rouge_types is None:\n",
    "        rouge_types = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "    ##endof:  if rouge_types is None\n",
    "    \n",
    "    scorer = rouge_scorer.RougeScorer(rouge_types=rouge_types, \n",
    "                                      use_stemmer=use_stemmer\n",
    "    )\n",
    "    \n",
    "    if use_aggregator:\n",
    "        aggregator = scoring.BootstrapAggregator()\n",
    "    else:\n",
    "        scores = []\n",
    "    ##endof:  if/else use_aggregator\n",
    "    \n",
    "    for ref, pred in zip(references, predictions):\n",
    "        score = scorer.score(ref, pred)\n",
    "        if use_aggregator:\n",
    "            aggregator.add_scores(score)\n",
    "        else:\n",
    "            scores.append(score)\n",
    "    ##endof:  for\n",
    "\n",
    "    result = \"there-is-some-problem\" #  scoping (if we weren't\n",
    "                                     #+ in Python) and having\n",
    "                                     #+ a sort of error message\n",
    "    \n",
    "    if use_aggregator:\n",
    "        result = aggregator.aggregate()\n",
    "    else:\n",
    "        result = {}\n",
    "        for key in scores[0]:\n",
    "            result[key] = [score[key] for score in scores]\n",
    "        ##endof:  for\n",
    "    ##endof:  if/else use_aggregator\n",
    "    \n",
    "    return result\n",
    "    \n",
    "##endof:  compute_google_rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2c07f3-1066-48ac-a243-8ec5af8f528e",
   "metadata": {},
   "source": [
    "I found a nice, short, interesting conversation while doing the random summaries, so I went and found its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46472dc-5f9d-44a4-a959-4f2053d9de58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timeit.default_timer()\n",
    "\n",
    "str_to_find = \"Damien: Omg..I'm glad Sunday is only once a week\"\n",
    "\n",
    "for sample_num in range(len(dataset['test'])):\n",
    "    this_sample = dataset['test'][sample_num]\n",
    "    this_dialogue = this_sample['dialogue']\n",
    "    if str_to_find in this_dialogue:\n",
    "        print(f\"sample_num: {sample_num}\")\n",
    "        print(f\"this_dialogue: \\n{this_dialogue}\")\n",
    "        print()\n",
    "        print(\"this_sample:\")\n",
    "        print(str(this_sample))\n",
    "        print()\n",
    "    ##endof:  if str_to_find in this_dialogue\n",
    "##endof:  for sample_number in range(len(dataset))\n",
    "\n",
    "toc = timeit.default_timer()\n",
    "\n",
    "print(\"Finding the sample in the test dataset (well,\")\n",
    "print(\"actually looking at every sample in the test\")\n",
    "print(\"dataset, regardless of whether we had found\")\n",
    "print(\"something.\")\n",
    "print(f\"took {toc - tic:0.4f} seconds.\")\n",
    "\n",
    "my_duration = toc - tic\n",
    "\n",
    "elapsed_time_str = format_timespan(my_duration)\n",
    "\n",
    "print(f\"which equates to {elapsed_time_str}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"Total size of test dataset: {sample_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0795c5-797b-4592-b417-17d3d4089d56",
   "metadata": {},
   "source": [
    "The interesting conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff709484-f7b1-4868-a87f-12fbbbc47e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_index = 224\n",
    "my_complete_entry = dataset['test'][sample_num]\n",
    "my_cool_str = dataset['test'][sample_num]['dialogue']\n",
    "print(my_cool_str)\n",
    "objects_to_pickle.append(my_cool_str)\n",
    "my_cool_list = [f\"my_index: {my_index}\", my_cool_str, my_complete_entry]\n",
    "pprint.pp(my_cool_list)\n",
    "objects_to_pickle.append(my_cool_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7216a22-a59a-401e-a52e-75a59ee57b97",
   "metadata": {},
   "source": [
    "### Let's get the sizes of all parts of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e975b16-9e5d-4b3f-b412-62267dad18ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_train = len(dataset['train'])\n",
    "size_of_eval = len(dataset['evaluation'])\n",
    "size_of_test = len(dataset['test'])\n",
    "\n",
    "print(f\"size_of_train : {size_of_train}\")\n",
    "print(f\"size_of_eval  : {size_of_eval}\")\n",
    "print(f\"size_of_test  : {size_of_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5cd858-f6e7-4dba-b689-75e3d950f26d",
   "metadata": {},
   "source": [
    "### Try for a baseline (for out-of-the-box, pretrained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d4b1ed-9b22-43df-8c0c-53f6afa88a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "!date +'%s_%Y%m%dT%H%M%S%z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e457a41-5f41-44c7-8b1f-e50a5b007ee7",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba0b4db-cd1c-466c-91be-cc0e2a8a14c1",
   "metadata": {},
   "source": [
    "#### Just one summarization to begin with, randomly picked\n",
    "\n",
    "##### Well, not so randomly, anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68250f44-daf6-436e-a7d2-dcb7a8d97ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Just one summarization to begin with, randomly picked ... but\n",
    "#+ now with th possibility of a known seed, to allow visual \n",
    "#+ comparison with after-training results.\n",
    "#+ I'M NOT GOING TO USE THIS REPEATED SEED, I'm just going to\n",
    "#+ use the datum at the first index to compare.\n",
    "#\n",
    "#  For sharing with Pat, I'm making it repeatable\n",
    "\n",
    "do_seed_for_repeatable = True\n",
    "\n",
    "summarizer = pipeline('summarization', \n",
    "                      model=model, \n",
    "                      tokenizer=tokenizer)\n",
    "\n",
    "if do_seed_for_repeatable:\n",
    "    rand_seed_for_randrange = 137\n",
    "    random.seed(rand_seed_for_randrange)\n",
    "##endof:  if do_seed_for_repeatable\n",
    "\n",
    "sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    "\n",
    "res = summarizer(sample[\"dialogue\"])\n",
    "\n",
    "print(f\"flan-t5-small summary:\\n{res[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21170b7-0685-4e39-b9d6-2eaf5624d476",
   "metadata": {},
   "source": [
    "#### Now, a couple summarizations with comparisons to ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a760e7-6270-4e65-93f0-1af0c3dd559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline('summarization', \n",
    "                      model=model, \n",
    "                      tokenizer=tokenizer)\n",
    "\n",
    "pred_test_list = []\n",
    "ref_test_list = []\n",
    "\n",
    "sample_num = 0\n",
    "\n",
    "this_sample = dataset['test'][sample_num]\n",
    "\n",
    "print(f\"dialogue: \\n{this_sample['dialogue']}\\n---------------\")\n",
    "\n",
    "ground_summary = this_sample['summary']\n",
    "res = summarizer(this_sample['dialogue'])\n",
    "res_summary = res[0]['summary_text']\n",
    "\n",
    "print(f\"human-genratd summary:\\n{ground_summary}\")\n",
    "print(f\"flan-t5-small summary:\\n{res_summary}\")\n",
    "\n",
    "ref_test_list.append(ground_summary)\n",
    "pred_test_list.append(res_summary)\n",
    "\n",
    "#  Yes, I have just one datum, but I'm setting things up to\n",
    "#+ work well with a later loop, i.e. with lists\n",
    "results_test_0 = compute_google_rouge_score(\n",
    "                            predictions=pred_test_list,\n",
    "                            references=ref_test_list,\n",
    "                            use_aggregator=False\n",
    ")\n",
    "\n",
    "# >>> print(list(results_test.keys()))\n",
    "# ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ab55d2-33b6-4c6f-802f-2ab97e0f8b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rouge_scores(results_test_0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84099d2-41e4-4266-a5e9-b7b7cac59504",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline('summarization', \n",
    "                      model=model, \n",
    "                      tokenizer=tokenizer)\n",
    "\n",
    "#  I don't want to aggregate, yet.\n",
    "pred_test_list = []\n",
    "ref_test_list = []\n",
    "\n",
    "sample_num = 224\n",
    "\n",
    "this_sample = dataset['test'][sample_num]\n",
    "\n",
    "print(f\"dialogue: \\n{this_sample['dialogue']}\\n---------------\")\n",
    "\n",
    "ground_summary = this_sample['summary']\n",
    "res = summarizer(this_sample['dialogue'])\n",
    "res_summary = res[0]['summary_text']\n",
    "\n",
    "print(f\"human-genratd summary:\\n{ground_summary}\")\n",
    "print(f\"flan-t5-small summary:\\n{res_summary}\")\n",
    "\n",
    "ref_test_list.append(ground_summary)\n",
    "pred_test_list.append(res_summary)\n",
    "\n",
    "results_test_224 = compute_google_rouge_score(\n",
    "                              predictions=pred_test_list,\n",
    "                              references=ref_test_list,\n",
    "                              use_aggregator=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf7d444-6d03-400c-b7ac-551d6aa4f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rouge_scores(results_test_224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59497375-7a7a-4623-8f52-68b7c5de6823",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Note on ROUGE Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f9c213-e96b-48de-8050-4eff2b571695",
   "metadata": {},
   "source": [
    "<strike>`@todo`</strike> `DONE : Run the ROUGE analysis from the Python package`\n",
    "\n",
    "\n",
    "The package used by the HuggingFace `datasets.load_metric` method is at\n",
    "\n",
    "https://github.com/google-research/google-research/tree/master/rouge\n",
    "\n",
    "I can't see how to aggregate it, though I may have found a resource at\n",
    "\n",
    "```\n",
    "ref_gg_rg = \"https://github.com/huggingface/datasets/blob/\" + \\\\\n",
    "            \"main/metrics/rouge/rouge.py\"\n",
    "\n",
    "arch_gg_rg = \"https://web.archive.org/web/20240603192938/\" + \\\\\n",
    "             \"https://github.com/huggingface/datasets/blob/\" + \\\\\n",
    "             \"main/metrics/rouge/rouge.py\"\n",
    "```\n",
    "\n",
    "<strike>It turns out that the deprecated one is preferable in \n",
    "output, at least until I can debug the aggregation of\n",
    "scores with another version: compute_google_rouge_score</strike>\n",
    "\n",
    "I've now got the `compute_google_rouge_score` method, above. I was able\n",
    "to look through the code for `datasets.load_metric('rouge')` code and\n",
    "put together that new method.\n",
    "\n",
    "So now, I'm not using any `rouge` object, but simply doing\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "these_results = compute_google_rouge_score(\n",
    "                       predictions, references, use_aggregator)\n",
    "```\n",
    "\n",
    "\n",
    "This next one is what the warning/deprecation message for `load_metric` said \n",
    "to use, but it only returns an f-measure (f-score)\n",
    "\n",
    "```\n",
    "# #  Replacement for the load_metric - evaluate.load(metric_name)\n",
    "# #+ Docs said:\n",
    "# #+\n",
    "# #+> Returns:\n",
    "# #+>    rouge1: rouge_1 (f1),\n",
    "# #+>    rouge2: rouge_2 (f1),\n",
    "# #+>    rougeL: rouge_l (f1),\n",
    "# #+>    rougeLsum: rouge_lsum (f1)\n",
    "# #+>\n",
    "# #+> Meaning we only get the f-score. I want more to compare.\n",
    "# #-v- code \n",
    "# rouge = evaluate_dot_load('rouge')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f099cc0c-bce3-49e5-8ca8-afd212adbc70",
   "metadata": {},
   "source": [
    "#### Verbosity stuff - get rid of the nice advice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7a763-7427-4054-ab79-ee8d0931f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "!date +'%s_%Y%m%dT%H%M%S%z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb761d-d836-49a4-b585-29aae74f0c86",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49d3a5-b58e-468a-836a-84a3908fd27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_verbosity_is_critical = \\\n",
    "  logging.get_verbosity() == logging.CRITICAL # alias FATAL, 50\n",
    "log_verbosity_is_error = \\\n",
    "  logging.get_verbosity() == logging.ERROR # 40\n",
    "log_verbosity_is_warn = \\\n",
    "  logging.get_verbosity() == logging.WARNING # alias WARN, 30\n",
    "log_verbosity_is_info = \\\n",
    "  logging.get_verbosity() == logging.INFO # 20\n",
    "log_verbosity_is_debug = \\\n",
    "  logging.get_verbosity() == logging.DEBUG # 10\n",
    "\n",
    "print( \"The statement, 'logging verbosity is CRITICAL' \" + \\\n",
    "      f\"is {log_verbosity_is_critical}\")\n",
    "print( \"The statement, 'logging verbosity is    ERROR' \" + \\\n",
    "      f\"is {log_verbosity_is_error}\")\n",
    "print( \"The statement, 'logging verbosity is  WARNING' \" + \\\n",
    "      f\"is {log_verbosity_is_warn}\")\n",
    "print( \"The statement, 'logging verbosity is     INFO' \" + \\\n",
    "      f\"is {log_verbosity_is_info}\")\n",
    "print( \"The statement, 'logging verbosity is    DEBUG' \" + \\\n",
    "      f\"is {log_verbosity_is_debug}\")\n",
    "\n",
    "print()\n",
    "\n",
    "init_log_verbosity = logging.get_verbosity()\n",
    "print(f\"The value of logging.get_verbosity() is: {init_log_verbosity}\")\n",
    "\n",
    "print()\n",
    "\n",
    "init_t_n_a_w = os.environ.get('TRANSFORMERS_NO_ADVISORY_WARNINGS')\n",
    "print(f\"TRANSFORMERS_NO_ADIVSORY_WARNINGS: {init_t_n_a_w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a66b11-e35e-4663-9bcc-602bd3cd060d",
   "metadata": {},
   "source": [
    "### Actual Baseline on Complete Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c671b535-247a-4510-9a09-d8db4fbeab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "!date +'%s_%Y%m%dT%H%M%S%z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5dca6b-d4ca-440e-bdd2-71869263c6d7",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08031760-1551-41f3-8a1e-1a4b8fef5513",
   "metadata": {},
   "source": [
    "<b>!!! NOTE</b> You'd better <b>make \n",
    "dang sure you want the lots of output</b> \n",
    "before you set this next boolean to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af21e6ae-43c6-4ea5-97e2-019d11e31abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_have_lotta_output_from_all_dialogs_summaries_1 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3db0ad-b6df-4124-aad7-74be50b62b9a",
   "metadata": {},
   "source": [
    "<h1>Are you sure about the value of that last boolean? 1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f4ea4-b239-4c74-a533-0371b28e140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"That last boolean has the value:\")\n",
    "print(f\"{do_have_lotta_output_from_all_dialogs_summaries_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d771a-6fa6-47f3-9743-e3b2096382ab",
   "metadata": {},
   "source": [
    "There could be up to megabytes worth of text output if you've changed it to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c550c2-eee7-4393-9f92-1a25f4fcc2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ref1 = \"https://web.archive.org/web/20240530051418/\" + \\\n",
    "#+        \"https://stackoverflow.com/questions/73221277/\" + \\\n",
    "#+        \"python-hugging-face-warning\"\n",
    "#  ref2 = \"https://web.archive.org/web/20240530051559/\" + \\\n",
    "#+        \"https://huggingface.co/docs/transformers/en/\" + \\\n",
    "#+        \"main_classes/logging\"\n",
    "\n",
    "\n",
    "##  Haven't tried this, because the logging seemed easier,\n",
    "##+ and the logging worked\n",
    "#os.environ(\"TRANSFORMERS_NO_ADVISORY_WARNINGS\") = 1\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "summarizer = pipeline('summarization', \n",
    "                      model=model, \n",
    "                      tokenizer=tokenizer)\n",
    "\n",
    "baseline_sample_dialog_list = [] ##  Keeping for comparison, later\n",
    "                                 ##+ Not needed for scores.\n",
    "baseline_prediction_list = []\n",
    "baseline_reference_list = []\n",
    "\n",
    "baseline_tic = timeit.default_timer()\n",
    "\n",
    "for sample_num in range(len(dataset['test'])):\n",
    "    this_sample = dataset['test'][sample_num]\n",
    "    \n",
    "    if do_have_lotta_output_from_all_dialogs_summaries_1:\n",
    "        print(f\"dialogue: \\n{this_sample['dialogue']}\\n---------------\")\n",
    "    ##endof:  if do_have_lotta_output_from_all_dialogs_summaries_1\n",
    "    \n",
    "    ground_summary = this_sample['summary']\n",
    "    res = summarizer(this_sample['dialogue'])\n",
    "    res_summary = res[0]['summary_text']\n",
    "    \n",
    "    if do_have_lotta_output_from_all_dialogs_summaries_1:\n",
    "        print(f\"human-genratd summary:\\n{ground_summary}\")\n",
    "        print(f\"flan-t5-small summary:\\n{res_summary}\")\n",
    "    ##endof:  if do_have_lotta_output_from_all_dialogs_summaries_1\n",
    "    \n",
    "    baseline_sample_dialog_list.append(this_sample['dialogue'])\n",
    "    baseline_reference_list.append(ground_summary)\n",
    "    baseline_prediction_list.append(res_summary)\n",
    "##endof:  for sample_num in range(len(dataset['test']))\n",
    "\n",
    "baseline_toc = timeit.default_timer()\n",
    "\n",
    "baseline_duration = baseline_toc - baseline_tic\n",
    "\n",
    "print( \"Getting things ready for scoring (doing the baseline)\")\n",
    "print(f\"took {baseline_toc - baseline_tic:0.4f} seconds.\")\n",
    "\n",
    "baseline_time_str = format_timespan(baseline_duration)\n",
    "\n",
    "print(f\"which equates to {baseline_time_str}\")\n",
    "\n",
    "baseline_results = compute_google_rouge_score(\n",
    "                          predictions=baseline_prediction_list,\n",
    "                          references=baseline_reference_list,\n",
    "                          use_aggregator=True\n",
    ")\n",
    "\n",
    "# >>> print(list(baseline_results.keys()))\n",
    "# ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "\n",
    "objects_to_pickle.append(baseline_sample_dialog_list)\n",
    "objects_to_pickle.append(baseline_prediction_list)\n",
    "objects_to_pickle.append(baseline_reference_list)\n",
    "objects_to_pickle.append(baseline_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377fedc3-f857-4ba4-8729-07666336bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Haven't tried this, because the logging seemed easier,\n",
    "##+ and the logging worked\n",
    "# os.environ(\"TRANSFORMERS_NO_ADVISORY_WARNINGS\") = init_t_n_a_w\n",
    "\n",
    "logging.set_verbosity(init_log_verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4949e0-de52-485c-bbc2-559dd0db4c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rouge_scores(baseline_results, \"BASELINE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df187315-53a9-4f66-a9f3-17db67512ddd",
   "metadata": {},
   "source": [
    "### Trainer - the Actual Trainer Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc41e49-b3b8-4ea0-a3e0-aa40e4860be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "!date +'%s_%Y%m%dT%H%M%S%z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5503fa3b-09b3-4b6e-b18c-3fb9791e715f",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4feb5c-bb3c-444d-8814-6efa2317ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer( model=model,\n",
    "                      train_dataset=dataset['train'],\n",
    "                      eval_dataset=dataset['evaluation'],\n",
    "                      peft_config=peft_config,\n",
    "                      tokenizer=tokenizer,\n",
    "                      packing=True,\n",
    "                      formatting_func=prompt_instruction_format,\n",
    "                      args=training_args,\n",
    ")\n",
    "##  Warnings are below output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1896e3-a29c-4df6-b2e7-0d1d762e771b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Warnings I Won't Worry About, Yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25010c2f-3f36-47b8-aad4-d5beec083d7c",
   "metadata": {},
   "source": [
    "First time warnings from the code above (as it still is).\n",
    "\n",
    "        \n",
    ">        WARNING:bitsandbytes.cextension:The installed version of bitsandbytes \\\n",
    ">         was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, \\\n",
    ">         and GPU quantization are unavailable.\n",
    ">        C:\\Users\\bballdave025\\.conda\\envs\\rwkv-lora-pat\\lib\\site-packages\\trl\\\\\n",
    ">         trainer\\sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` \\\n",
    ">        argument to the SFTTrainer, this will default to 512\n",
    ">         warnings.warn(\n",
    ">        \n",
    ">        [ > Generating train split: 6143/0 [00:04<00:00, 2034.36 examples/s] ]\n",
    ">        \n",
    ">        Token indices sequence length is longer than the specified maximum sequence \\\n",
    ">         length for this model (657 > 512). Running this sequence through the model \\\n",
    ">         will result in indexing errors\n",
    ">        \n",
    ">        [ > Generating train split: 355/0 [00:00<00:00, 6.10 examples/s] ]\n",
    "\n",
    "<b>DWB Note</b> and possible\n",
    "\n",
    "<strike>\\# @todo: </strike>\n",
    "\n",
    "<strike>So, I'm changing the `max_seq_length`.</strike> \n",
    "Maybe I should just throw out the offender(s) \n",
    "(along with the blank one that's in there somewhere),\n",
    "but I'll just continue as is.\n",
    "\n",
    "I never ran the updated cell, (with an additional parameter, \n",
    "`max_seq_length=675`), so the Warning and Advice are still there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d556f5-92e7-4bdc-bbd0-73d643050446",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "## Let's Train This LoRA Thing and See How It Does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376a82f8-7f45-454c-b301-a591298171c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "!date +'%s_%Y%m%dT%H%M%S%z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd1014-d324-4293-930d-0ead8fdba381",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c15de9-adb4-433d-8a01-e633571bb076",
   "metadata": {},
   "source": [
    "### The long-time-taking training code is just below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4971f967-9d95-4606-9cc7-748834836b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timeit.default_timer()\n",
    "trainer.train()\n",
    "toc = timeit.default_timer()\n",
    "print(f\"tic: {tic}\")\n",
    "print(f\"toc: {toc}\")\n",
    "training_duration = toc - tic\n",
    "print(f\"Training took {toc - tic:0.4f} seconds.\")\n",
    "training_time_str = format_timespan(training_duration)\n",
    "print(f\"which equates to {training_time_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aaea49-d34a-4ab1-89f1-65c7de627ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "!date +'%s_%Y%m%dT%H%M%S%z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0520fa-6516-42cf-888f-08b0717e7d83",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7684066-010c-40f1-822e-2c54ac673ec6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Thinking about it and learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5127700-78f2-4f17-b677-8d29789d30db",
   "metadata": {},
   "source": [
    "#### @todo : consolidate \"the other info as above\"\n",
    "\n",
    "I'm talking about the numbers of data points, tokens, whatever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ca0bc-360c-4708-a1a8-c33c5f3f731f",
   "metadata": {},
   "source": [
    "#### Any Comments / Things to Try (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d76966d-3466-463a-83fa-e21381b5365a",
   "metadata": {},
   "source": [
    "We passed an evaluation set (parameter `eval_dataset`) to the `trainer`.\n",
    "How can we see information about that?\n",
    "\n",
    "<b>Update:</b> Answer is below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934771bd-3201-4393-be21-7036e49b0843",
   "metadata": {},
   "source": [
    "#### How to get the evaluation set used by the trainer\n",
    "\n",
    "I added the following parameters to the \n",
    "`training_args = TrainingArguments(<args>)`\n",
    "call.\n",
    "\n",
    "- `do_eval=True`\n",
    "- `per_device_eval_batch_size=4`\n",
    "- `eval_strategy='epoch'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c056cac-441f-42ae-9600-8db0bc991f82",
   "metadata": {},
   "source": [
    "#### How to specify your repo name\n",
    "\n",
    "I also added this next parameter to the arguments for\n",
    "`training_args = TrainingArguments(<args>)`\n",
    "\n",
    "- `hub_model_id=\"dwb-flan-t5-small-lora-ft-colab\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b246e-c7bd-4fd7-8eab-b0ba46a2a922",
   "metadata": {},
   "source": [
    "#### The final TrainingArguments call - with parameter list\n",
    "\n",
    "##### Including four additional parameters\n",
    "\n",
    "```\n",
    "training_args = TrainingArguments( \n",
    "                    output_dir='output',\n",
    "                    num_train_epochs=1,\n",
    "                    per_device_train_batch_size=4,\n",
    "                    save_strategy='epoch',\n",
    "                    learning_rate=2e-4,\n",
    "                    do_eval=True,\n",
    "                    per_device_eval_batch_size=4,\n",
    "                    eval_strategy='epoch',\n",
    "                    hub_model_id=\"dwb-flan-t5-small-lora-ft-colab\",\n",
    "                    run_name=\"dwb-flan-samsum-run-colab-20240606-02\",\n",
    "                    #  has nodename (machine), when this param is\n",
    "                    #+ unset\n",
    "                    overwrite_output_dir=False,\n",
    "                    logging_strategy='steps',\n",
    "                    logging_steps=32,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e62ce34-a826-4a83-8b2d-9acede4ff2a2",
   "metadata": {},
   "source": [
    "## Save the Trainer to Hugging Face and Get Our Updated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80265d1c-b65b-4863-8ac3-34e066c86522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "!date +'%s_%Y%m%dT%H%M%S%z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c105c8-8182-4c12-9ce6-1016ee4787c5",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e0693e-579b-4a82-a2c2-2d3dc1e6f900",
   "metadata": {},
   "source": [
    "I'm following the [(archived) tutorial from Mehul Gupta on Medium](https://web.archive.org/web/20240522140323/https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578); since it's archived, you can follow exactly what I'm doing.\n",
    "\n",
    "Running this next line of code will come up with a dialog box with text entry,\n",
    "and I'm now using the `@thebballdave025` for Hugging Face stuff.\n",
    "\n",
    "<b>Make sure to use the WRITE token, here.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e5a45e-1890-415c-9da3-9f43fa0969df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This will come up with a dialog box with text entry.\n",
    "#+ and I'm now using @thebballdave025 for Hugging Face.\n",
    "\n",
    "# Use the write token, here.\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c99a5c-2739-49f6-9a4b-02c087e4c656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenizer and create a tokenizer model card\n",
    "tokenizer.save_pretrained('testing')\n",
    "  #  'testing' is the local directory\n",
    "\n",
    "# Create the trainer model card\n",
    "trainer.create_model_card()\n",
    "\n",
    "# Push the results to the Hugging Face Hub\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341b1b76-7168-43a3-8b70-f2b8245c2970",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hugging Face Repo Info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe8898-7a51-4826-8328-4db9cea25886",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "Part of the output included text giving the URL,\n",
    "\n",
    "https://huggingface.co/thebballdave025/dwb-flan-t5-small-lora-finetune/commit/c87d34b398f3801ceb1e18c819a7c8fc894989c7\n",
    "\n",
    "Hooray! The repo name I used in constructing the trainer worked!\n",
    "\n",
    "I can get to the general repo with the URL,\n",
    "\n",
    "https://huggingface.co/thebballdave025/dwb-flan-t5-small-lora-finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be734832-cf62-43fb-9614-23b5703526d3",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511b4c50-2882-4fdd-afa5-3daefbb78cf3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Info on the Fine-Tuned Model from the Repo's README - Model Card(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1121b2f-91df-4912-b42f-a3e6803180a9",
   "metadata": {},
   "source": [
    "### [thebballdave025/dwb-flan-t5-small-lora-finetune](https://huggingface.co/thebballdave025/dwb-flan-t5-small-lora-finetune)\n",
    "\n",
    "\\[archived\\] The archiving attempt at archive.org (Wayback Machine) failed.\n",
    "I'm not sure why, as the model is set as public.\n",
    "\n",
    "`PEFT TensorBoard Safetensors    generator trl sft generated_from_trainer    License: apache-2.0`\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "\\[<b>@todo</b> :\\] [Edit Model Card](https://huggingface.co/thebballdave025/dwb-flan-t5-small-lora-finetune/edit/main/README.md)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;\n",
    "Unable to determine this model’s pipeline type. Check the docs \n",
    "[(i)](https://huggingface.co/docs/hub/models-widgets#enabling-a-widget).\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Adapter for\n",
    "[google/flan-t5-small](https://huggingface.co/google/flan-t5-small)\n",
    "\n",
    "#### dwb-flan-t5-small-lora-finetune\n",
    "\n",
    "This model is a fine-tuned version of \n",
    "[google/flan-t5-small](https://huggingface.co/google/flan-t5-small) on the \n",
    "generator dataset \\[DWB note: I don't know why it says \"generator dataset\".\n",
    "I used the samsum dataset, which I will link here and on the\n",
    "model card, eventually\\]. \n",
    "\n",
    "It achieves the following results on the evaluation set:\n",
    "\n",
    "- Loss: 0.0226\n",
    "- <i>DWB Note: I don't know which metric was used to calculate loss. If this were more important, I'd dig through code to find out and evaluate with the same metric. If I'm really lucky, they somehow used the ROUGE scores in the loss function, so we match.</i>\n",
    "\n",
    "#### Model description\n",
    "\n",
    "More information needed\n",
    "\n",
    "#### Intended uses & limitations\n",
    "\n",
    "More information needed\n",
    "\n",
    "#### Training and evaluation data\n",
    "\n",
    "More information needed\n",
    "\n",
    "#### Training procedure\n",
    "\n",
    "#### Training hyperparameters\n",
    "\n",
    "The following hyperparameters were used during training:\n",
    "\n",
    "- learning_rate: 0.0002\n",
    "- train_batch_size: 4\n",
    "- eval_batch_size: 4\n",
    "- seed: 42\n",
    "- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n",
    "- lr_scheduler_type: linear\n",
    "- num_epochs: 1\n",
    "\n",
    "#### Training results\n",
    "\n",
    "```\n",
    "\n",
    "  Training Loss | Epoch | Step | Validation Loss\n",
    " ---------------+-------+------+-----------------\n",
    "      0.0685    |  1.0  | 1536 |     0.0226\n",
    "```\n",
    "\n",
    "#### Framework versions\n",
    "\n",
    "- PEFT 0.11.2.dev0\n",
    "- Transformers 4.41.1\n",
    "- Pytorch 2.3.0+cpu\n",
    "- Datasets 2.19.1\n",
    "- Tokenizers 0.19.1\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c81523-7faf-40d9-806a-919038f6e6f1",
   "metadata": {},
   "source": [
    "## Actually Get the Model from Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62333ba9-c416-4f04-b8ff-ff27be3a359b",
   "metadata": {},
   "source": [
    "Running this next line of code will come up with a dialog box with text entry,\n",
    "and I'm now using the `@thebballdave025` for Hugging Face stuff.\n",
    "\n",
    "<b>Make sure to use the READ token, here.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a022c0-760d-4fcb-bb71-a05e322598cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read token. Will bring up text entry to paste token string\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207707c8-1920-446a-bfa9-9c8ee9480a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "!date +'%s_%Y%m%dT%H%M%S%z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b4db3b-4516-4900-b640-ac498f0308bc",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e79207-23e2-4d73-ac6f-90a40a489574",
   "metadata": {},
   "source": [
    "(If you have problems that note `data_files` or `dataset` or `prompt_instruction_format`, make sure that the cells where these are defined have been run, i.e. the kernel hasn't been restarted since they were initialized.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca447ad1-943e-420a-9898-8404fbb8b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My trained model from Hugging Face\n",
    "\n",
    "new_model_name = \"thebballdave025/dwb-flan-t5-small-lora-ft-colab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d1a19-78ae-4e67-9125-ae81a0f4d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_load_tic = timeit.default_timer()\n",
    "\n",
    "#  Maybe the problem is that we need to change from Seq2Seq to Causal\n",
    "#+ but I think I only use the new_model_name (at least in inference;\n",
    "#+ I do use the actual model to look at the LoRA-ified architecture).\n",
    "\n",
    "new_model = AutoModelForSeq2SeqLM.from_pretrained(new_model_name)\n",
    "#new_model = AutoModelForCausalLM.from_pretrained(new_model_name)\n",
    "\n",
    "new_model_load_toc = timeit.default_timer()\n",
    "\n",
    "new_model_load_duration = new_model_load_toc - new_model_load_tic\n",
    "\n",
    "print(f\"Loading the LoRA-fine-tuned model, {new_model_name}\")\n",
    "print(f\"took {new_model_load_toc - new_model_load_tic:0.4f} seconds.\")\n",
    "\n",
    "new_model_load_time_str = format_timespan(new_model_load_duration)\n",
    "\n",
    "print(f\"which equates to {new_model_load_time_str}\")\n",
    "\n",
    "#  Next line makes training faster but a little less accurate\n",
    "new_model.config.pretraining_tp = 1\n",
    "\n",
    "new_tokenizer_tic = timeit.default_timer()\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(\n",
    "                                   new_model_name, \n",
    "                                   trust_remote_code=True)\n",
    "new_tokenizer_toc = timeit.default_timer()\n",
    "\n",
    "new_tokenizer_duration = new_tokenizer_toc - new_tokenizer_tic\n",
    "\n",
    "print()\n",
    "print(\"Getting fine-turned tokenizer\")\n",
    "print(f\"took {new_tokenizer_toc - new_tokenizer_tic:0.4f} seconds.\")\n",
    "\n",
    "new_tokenizer_time_str = format_timespan(new_tokenizer_duration)\n",
    "\n",
    "print(f\"which equates to {new_tokenizer_time_str}\")\n",
    "\n",
    "new_tokenizer.pad_token = new_tokenizer.eos_token\n",
    "new_tokenizer.padding_side = \"right\"\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "#--------------------------------------------------------\n",
    "# Got some weird results, so I'm doing the old tokenizer\n",
    "old_model_name = \"google/flan-t5-small\"\n",
    "\n",
    "old_model_load_tic = timeit.default_timer()\n",
    "old_model = AutoModelForSeq2SeqLM.from_pretrained(old_model_name)\n",
    "old_model_load_toc = timeit.default_timer()\n",
    "\n",
    "old_model_load_duration = \\\n",
    "           old_model_load_toc - old_model_load_tic\n",
    "\n",
    "print(f\"Loading the old model, {old_model_name}\")\n",
    "print(\"took \" + \\\n",
    "      f\"{old_model_load_toc - old_model_load_tic:0.4f}\" + \\\n",
    "      \" seconds.\"\n",
    ")\n",
    "\n",
    "old_model_load_time_str = format_timespan(old_model_load_duration)\n",
    "\n",
    "print(f\"which equates to {old_model_load_time_str}\")\n",
    "\n",
    "#  Next line makes training faster but a little less accurate\n",
    "old_model.config.pretraining_tp = 1\n",
    "\n",
    "old_tokenizer_tic = timeit.default_timer()\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\n",
    "                                       old_model_name, \n",
    "                                       trust_remote_code=True\n",
    ")\n",
    "old_tokenizer_toc = timeit.default_timer()\n",
    "\n",
    "old_tokenizer_duration = old_tokenizer_toc - old_tokenizer_tic\n",
    "\n",
    "print()\n",
    "print(\"Getting old tokenizer\")\n",
    "print( \"took \" + \\\n",
    "      f\"{old_tokenizer_toc - old_tokenizer_tic:0.4f}\"\n",
    "       \" seconds.\"\n",
    ")\n",
    "\n",
    "old_tokenizer_time_str = format_timespan(old_tokenizer_duration)\n",
    "\n",
    "print(f\"which equates to {tokenizer_time_str}\")\n",
    "\n",
    "#  padding instructions for the tokenizer\n",
    "old_tokenizer.pad_token = tokenizer.eos_token\n",
    "old_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc8db7f-873f-4feb-ab77-53d9c41351d2",
   "metadata": {},
   "source": [
    "#### Stuff for model architecture - post-LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8792fdf-eb40-4b73-b810-770227db7476",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b45927-f379-40b8-bf9a-e18171037c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_arch_str = str(new_model)\n",
    "\n",
    "with open(\n",
    "  \"dwb-flan-t5-small-lora-ft-colab.model-architecture.txt\", \n",
    "  'w', \n",
    "  encoding='utf-8') as fhn:\n",
    "    fhn.write(new_model_arch_str)\n",
    "##endof:  with open ... fhn\n",
    "\n",
    "objects_to_pickle.append(new_model_arch_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bd1c76-f3b8-44ff-8d6e-1b735155d546",
   "metadata": {},
   "source": [
    "@todo : get some Python version of `diff` going on here. I'm just using Cygwin/bash to see the LoRA additions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87701971-8342-470b-8f23-66f4494206e8",
   "metadata": {},
   "source": [
    "### Let's start by doing the single-dialogue summaries we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9549f-cb19-4bab-bc82-5cbe696b1ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to keep it consistent, use these. If not, change them at will\n",
    "model_to_use = new_model\n",
    "tokenizer_to_use = new_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70b7022-54f8-4efa-a8d6-16fe1c9a204f",
   "metadata": {},
   "source": [
    "#### Try one picked at random\n",
    "\n",
    "##### Well, not so randomly, anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99bcc51-f9a4-41f0-b7bf-1d5398cac38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Just one summarization to begin with, randomly picked ... but\n",
    "#+ now with th possibility of a known seed, to allow visual \n",
    "#+ comparison with after-training results.\n",
    "#+ I'M NOT GOING TO USE THIS REPEATED SEED, I'm just going to\n",
    "#+ use the datum at the first index to compare.\n",
    "#\n",
    "#  User repeatability when sharing with Pat\n",
    "\n",
    "\n",
    "do_seed_for_repeatable = True\n",
    "\n",
    "#  Gupta doesn't have a `tokenizer` argument. I seem to remember getting\n",
    "#+ an error when I tried that.\n",
    "summarizer = pipeline('summarization', \n",
    "                      model=model_to_use) #,\n",
    "                      #tokenizer=tokenizer_to_use)\n",
    "\n",
    "## Trials to fix weirdness.\n",
    "## model=old_model, tokenizer=old_tokenizer : matches baseline \n",
    "##                                            (quick)\n",
    "## model=new_model, tokenizer=new_tokenizer : weird results\n",
    "##                                            (takes significantly longer, too)\n",
    "## model=new_model, tokenizer=old_tokenizer : weird results\n",
    "##                                            (takes significantly longer, too)\n",
    "## model=old_model, tokenizer=new_tokenizer : actually matches baseline, which\n",
    "##                                            would seem to require a change in\n",
    "##                                            hypothesis as to why the\n",
    "##                                            weirdness and longer inference are\n",
    "##                                            happening. (Likely not tokenizer.)\n",
    "##                                            (quick)\n",
    "##\n",
    "##  I had thought that doing 'old_model' and 'new_tokenizer' gave me weird\n",
    "##+ results, too. Good thing to come back and check things.\n",
    "##  Still, the training \n",
    "\n",
    "if do_seed_for_repeatable:\n",
    "    rand_seed_for_randrange = 137\n",
    "    random.seed(rand_seed_for_randrange)\n",
    "##endof:  if do_seed_for_repeatable\n",
    "\n",
    "sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    "\n",
    "res = summarizer(sample[\"dialogue\"])\n",
    "\n",
    "print(f\"dwb-flan-t5-small-lora-ft-colab summary:\\n{res[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e7a31-de22-4dca-be9c-bff1f8705c45",
   "metadata": {},
   "source": [
    "#### Now, a couple summarizations with comparison to ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e1948-8eb4-41e2-84eb-be2cd71d9b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline('summarization', \n",
    "                      model=new_model) #,\n",
    "                      #tokenizer=new_tokenizer)\n",
    "\n",
    "pred_test_list = []\n",
    "ref_test_list = []\n",
    "\n",
    "sample_num = 0\n",
    "\n",
    "this_sample = dataset['test'][sample_num]\n",
    "\n",
    "print(f\"dialogue: \\n{this_sample['dialogue']}\\n---------------\")\n",
    "\n",
    "ground_summary = this_sample['summary']\n",
    "res = summarizer(this_sample['dialogue'])\n",
    "res_summary = res[0]['summary_text']\n",
    "\n",
    "print(f\"by-some-human-generated summary:\\n{ground_summary}\")\n",
    "print(f\"dwb-flan-t5-small-lora-ft-colab:\\n{res_summary}\")\n",
    "\n",
    "ref_test_list.append(ground_summary)\n",
    "pred_test_list.append(res_summary)\n",
    "\n",
    "# deprecated, blah blah blah\n",
    "#rouge = load_metric('rouge', trust_remote_code=True)\n",
    "\n",
    "#  Yes, I have just one datum, but I'm setting things up to\n",
    "#+ work well with a loop (meaning lists for pred and ref).\n",
    "results_test_0 = compute_google_rouge_score(\n",
    "                            predictions=pred_test_list,\n",
    "                            references=ref_test_list,\n",
    "                            use_aggregator=False\n",
    ")\n",
    "\n",
    "# >>> print(list(results_test.keys()))\n",
    "# ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b13b61-4b8a-484f-b3c6-1128ec6f6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rouge_scores(results_test_0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fff7a1-ca74-4000-8fec-9d4ed1d011fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline('summarization', \n",
    "                      model=model_to_use) #,\n",
    "                      #tokenizer=tokenizer_to_use)\n",
    "\n",
    "# I don't want to aggregate, yet\n",
    "pred_test_list = []\n",
    "ref_test_list = []\n",
    "\n",
    "sample_num = 224\n",
    "\n",
    "this_sample = dataset['test'][sample_num]\n",
    "\n",
    "print(f\"dialogue: \\n{this_sample['dialogue']}\\n---------------\")\n",
    "\n",
    "ground_summary = this_sample['summary']\n",
    "res = summarizer(this_sample['dialogue'])\n",
    "res_summary = res[0]['summary_text']\n",
    "\n",
    "print(f\"by-some-human-generated summary:\\n{ground_summary}\")\n",
    "print(f\"dwb-flan-t5-small-lora-ft-colab:\\n{res_summary}\")\n",
    "\n",
    "ref_test_list.append(ground_summary)\n",
    "pred_test_list.append(res_summary)\n",
    "\n",
    "results_test_224 = compute_google_rouge_score(\n",
    "                              predictions=pred_test_list,\n",
    "                              references=ref_test_list,\n",
    "                              use_aggregator=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bebf17-d3ca-46b7-afb3-941418ab36a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rouge_scores(results_test_224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88281b15-7070-44dd-84e4-aedefa00293d",
   "metadata": {},
   "source": [
    "## Evaluation on the Test Set and Comparison to Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d123c-412a-4195-8820-838590b814a2",
   "metadata": {},
   "source": [
    "### Verbosity stuff - get rid of the nice advice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a7666e-206c-418a-aec8-27b0fd4de3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "!date +'%s_%Y%m%dT%H%M%S%z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f710cdea-0123-4f96-8f29-e2ea8144e318",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde59af6-c15a-4494-b49e-87019a70715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_verbosity_is_critical = \\\n",
    "  logging.get_verbosity() == logging.CRITICAL # alias FATAL, 50\n",
    "log_verbosity_is_error = \\\n",
    "  logging.get_verbosity() == logging.ERROR # 40\n",
    "log_verbosity_is_warn = \\\n",
    "  logging.get_verbosity() == logging.WARNING # alias WARN, 30\n",
    "log_verbosity_is_info = \\\n",
    "  logging.get_verbosity() == logging.INFO # 20\n",
    "log_verbosity_is_debug = \\\n",
    "  logging.get_verbosity() == logging.DEBUG # 10\n",
    "\n",
    "print( \"The statement, 'logging verbosity is CRITICAL' \" + \\\n",
    "      f\"is {log_verbosity_is_critical}\")\n",
    "print( \"The statement, 'logging verbosity is    ERROR' \" + \\\n",
    "      f\"is {log_verbosity_is_error}\")\n",
    "print( \"The statement, 'logging verbosity is  WARNING' \" + \\\n",
    "      f\"is {log_verbosity_is_warn}\")\n",
    "print( \"The statement, 'logging verbosity is     INFO' \" + \\\n",
    "      f\"is {log_verbosity_is_info}\")\n",
    "print( \"The statement, 'logging verbosity is    DEBUG' \" + \\\n",
    "      f\"is {log_verbosity_is_debug}\")\n",
    "\n",
    "print()\n",
    "\n",
    "init_log_verbosity = logging.get_verbosity()\n",
    "print(f\"The value of logging.get_verbosity() is: {init_log_verbosity}\")\n",
    "\n",
    "print()\n",
    "\n",
    "init_t_n_a_w = os.environ.get('TRANSFORMERS_NO_ADVISORY_WARNINGS')\n",
    "print(f\"TRANSFORMERS_NO_ADIVSORY_WARNINGS: {init_t_n_a_w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf8db54-8003-429f-8abd-cfb09a1af80b",
   "metadata": {},
   "source": [
    "### Here's the actual evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d3eeb-e238-43bf-ada2-3d7ccaf10b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this again\n",
    "!date +'%s_%Y%m%dT%H%M%S%z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d0f187-2402-44a4-8322-f899df42fab4",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a522d7-6285-4b9d-b619-ea0f454f2cae",
   "metadata": {},
   "source": [
    "<b>!!! NOTE !!!</b> I'm going to use `tat` (with an underscore\n",
    "or undescores before, after, or surrounding the variable names)\n",
    "to indicate 'testing-after-training'.\n",
    "\n",
    "I guess I could have used `inference`, but I didn't."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523785e0-4ece-49cc-bb8a-64d7c3b0bb01",
   "metadata": {},
   "source": [
    "<b>!!! another NOTE</b> You'd better <b>make \n",
    "dang sure you want the lots of output</b> \n",
    "before you set this next boolean to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a638f1-5abb-4c3e-8683-dcc49557a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_have_lotta_output_from_all_dialogs_summaries_2 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdfd04c-675b-4e8c-b293-560617b5651a",
   "metadata": {},
   "source": [
    "# Are you sure about the value of that last boolean? 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9357f5e1-a59c-4b4e-a374-fbd47d341546",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"That last boolean has the value:\")\n",
    "print(f\"{do_have_lotta_output_from_all_dialogs_summaries_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec12c114-8f82-4cbd-aef3-fa1b5c8c237b",
   "metadata": {},
   "source": [
    "There could be up to megabytes worth of text output if you've changed it to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ad3bd-0921-4753-8c6c-175bffc23b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()\n",
    "\n",
    "tat_summarizer = pipeline('summarization', \n",
    "                          model=new_model) #, \n",
    "                          #tokenizer=new_tokenizer)\n",
    "\n",
    "tat_sample_dialog_list = [] ##  Keeping for comparison, later\n",
    "                            ##+ Not needed for scores.\n",
    "prediction_tat_list = []\n",
    "reference_tat_list = []\n",
    "\n",
    "tat_tic = timeit.default_timer()\n",
    "\n",
    "for sample_num in range(len(dataset['test'])):\n",
    "    this_sample = dataset['test'][sample_num]\n",
    "    \n",
    "    if do_have_lotta_output_from_all_dialogs_summaries_2:\n",
    "        print(\"=\"*75)\n",
    "        print(f\"dialogue: \\n{this_sample['dialogue']}\\n---------------\")\n",
    "    ##endof:  if do_have_lotta_output_from_all_dialogs_summaries_2\n",
    "    \n",
    "    ground_tat_summary = this_sample['summary']\n",
    "    res_tat = summarizer(this_sample['dialogue'])\n",
    "    res_tat_summary = res_tat[0]['summary_text']\n",
    "    \n",
    "    if do_have_lotta_output_from_all_dialogs_summaries_2:\n",
    "        print(\"-\"*70)\n",
    "        print( \"by-some-human-generated summary:\" + \\\n",
    "              f\"\\n{ground_tat_summary}\")\n",
    "        print(\"-\"*70)\n",
    "        print( \"dwb-flan-t5-small-lora-ft-colab:\" + \\\n",
    "              f\"\\n{res_tat_summary}\")\n",
    "        print(\"-\"*70)\n",
    "    ##endof:  if do_have_lotta_output_from_all_dialogs_summaries_2\n",
    "\n",
    "    tat_sample_dialog_list.append(this_sample['dialogue'])\n",
    "    reference_tat_list.append(ground_tat_summary)\n",
    "    prediction_tat_list.append(res_tat_summary)\n",
    "##endof:  for sample_num in range(len(dataset['test']))\n",
    "\n",
    "tat_toc = timeit.default_timer()\n",
    "\n",
    "tat_duration = tat_toc = tat_tic\n",
    "\n",
    "print( \"Getting things ready for scoring (after training)\")\n",
    "print(f\"took {tat_toc - tat_tic:0.4f} seconds.\")\n",
    "\n",
    "tat_time_str = format_timespan(tat_duration)\n",
    "\n",
    "print(f\"which equates to {tat_time_str}\")\n",
    "\n",
    "results_tat = compute_google_rouge_score(\n",
    "                         predictions=prediction_tat_list,\n",
    "                         references=reference_tat_list,\n",
    "                         use_aggregator=True\n",
    ")\n",
    "\n",
    "objects_to_pickle.append(tat_sample_dialog_list)\n",
    "objects_to_pickle.append(prediction_tat_list)\n",
    "objects_to_pickle.append(reference_tat_list)\n",
    "objects_to_pickle.append(results_tat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e708b7-ab43-4610-9e7c-f7b7ae2fd188",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Haven't tried this, because the logging seemed easier,\n",
    "##+ and the logging worked\n",
    "# os.environ(\"TRANSFORMERS_NO_ADVISORY_WARNINGS\") = init_t_n_a_w\n",
    "\n",
    "logging.set_verbosity(init_log_verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc2b004-b8af-4213-b406-cab89cd98466",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_rouge_scores(results_tat, \"TEST AFTER TRAINING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4a32a-65e7-4b6d-9928-399af69231d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Don't need this anymore\n",
    "!date +'%s_%Y%m%dT%H%M%S%z'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f1ee84-63d7-4f5a-a761-41782ea704f2",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f33df-d2fb-48e8-b6c0-e0f3d2d84b21",
   "metadata": {},
   "source": [
    "### Any comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02621ea6-f47d-4910-bf5a-966b2ac0c54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any comparison code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb63f34-c6d8-4a64-8712-99137b5a9ddb",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr/>\n",
    "\n",
    "### Pickle things to pickle save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5526ab-3d35-4b79-a18a-0cca17e5f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects_to_pickle_var_names = []\n",
    "\n",
    "objects_to_pickle_var_names.append('model_arch_str')\n",
    "objects_to_pickle_var_names.append('my_cool_str')\n",
    "objects_to_pickle_var_names.append('my_cool_list')\n",
    "objects_to_pickle_var_names.append('baseline_sample_dialog_list')\n",
    "objects_to_pickle_var_names.append('baseline_prediction_list')\n",
    "objects_to_pickle_var_names.append('baseline_reference_list')\n",
    "objects_to_pickle_var_names.append('baseline_results')\n",
    "objects_to_pickle_var_names.append('new_model_arch_str')\n",
    "objects_to_pickle_var_names.append('tat_sample_dialog_list')\n",
    "objects_to_pickle_var_names.append('prediction_tat_list')\n",
    "objects_to_pickle_var_names.append('reference_tat_list')\n",
    "objects_to_pickle_var_names.append('results_tat')\n",
    "objects_to_pickle_var_names.append('objects_to_picle_var_names')\n",
    "\n",
    "objects_to_pickle.append(objects_to_pickle_var_names)\n",
    "\n",
    "with open(pickle_filename, 'wb') as pfh:\n",
    "    pickle.dump(objects_to_pickle , pfh)\n",
    "##endof:  with open ... as pfh # (pickle file handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5af6893-b04d-42d1-8694-aeca3b33f993",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5918882a-0df9-4f3e-bb42-9b4ed492ad0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Notes Looking Forward to LoRA on RWKV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66346d54-06bb-47da-8d54-a9dee43b408a",
   "metadata": {},
   "source": [
    "Hugging Face Community, seems to have a good portion of their models\n",
    "\n",
    "https://huggingface.co/RWKV\n",
    "\n",
    "https://web.archive.org/web/20240530232509/https://huggingface.co/RWKV\n",
    "\n",
    "<br/>\n",
    "\n",
    "GitHub has even more versions/models, including the `v4-neo` that\n",
    "I think will be important (the LoRA project)\n",
    "\n",
    "https://github.com/BlinkDL/RWKV-LM/tree/main\n",
    "\n",
    "https://web.archive.org/web/20240530232637/https://github.com/BlinkDL/RWKV-LM/tree/main\n",
    "\n",
    "<br/>\n",
    "\n",
    "The main RWKV website (?!)\n",
    "\n",
    "https://www.rwkv.com/\n",
    "\n",
    "https://web.archive.org/web/20240529120904/https://www.rwkv.com/\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "GOOD STUFF. A project doing LoRA with RWKV\n",
    "\n",
    "https://github.com/Blealtan/RWKV-LM-LoRA/\n",
    "\n",
    "https://web.archive.org/web/20240530232823/https://github.com/Blealtan/RWKV-LM-LoRA\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "The official blog, I guess, with some good coding examples\n",
    "\n",
    "https://huggingface.co/blog/rwkv\n",
    "\n",
    "https://web.archive.org/web/20240530233025/https://huggingface.co/blog/rwkv\n",
    "\n",
    "It includes something that's similar to what I'm doing here in the\n",
    "`First_Full_LoRA_Trial_with_Transformer_Again.ipynb` tutorial, etc.\n",
    "\n",
    "```\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"RWKV/rwkv-raven-1b5\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "```\n",
    "\n",
    "The `AutoModelForCausalLM` is the same as the tutorial I'm following,\n",
    "but I don't know what the `.to(0)` is for.\n",
    "\n",
    "Really quickly, also looking at\n",
    "\n",
    "https://huggingface.co/RWKV/rwkv-4-world-7b\n",
    "\n",
    "https://web.archive.org/web/20240530234438/https://huggingface.co/RWKV/rwkv-4-world-7b\n",
    "\n",
    "I see an example for CPU.\n",
    "\n",
    "```\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "              \"RWKV/rwkv-4-world-7b\",\n",
    "              trust_remote_code=True\n",
    ").to(torch.float32)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "              \"RWKV/rwkv-4-world-7b\",\n",
    "              trust_remote_code=True)\n",
    "```\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "(Old version? Unofficial, it seems)\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/model_doc/rwkv\n",
    "\n",
    "https://web.archive.org/web/20240530232341/https://huggingface.co/docs/transformers/en/model_doc/rwkv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
