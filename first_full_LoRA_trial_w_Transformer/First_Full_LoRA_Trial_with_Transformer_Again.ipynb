{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d7b071-3c52-4189-81e4-2fe25bb2f61d",
   "metadata": {},
   "source": [
    "# First Full LoRA Trial with Transformer\n",
    "\n",
    "## peft (for LoRA) and FLAN-T5-small for the LLM\n",
    "\n",
    "I'm following what seems to be a great tutorial from Mehul Gupta,\n",
    "\n",
    "> https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578\n",
    "> \n",
    "> https://web.archive.org/web/20240522140323/https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578\n",
    "\n",
    "I'm doing this to prepare creating a LoRA for RWKV ( @todo  put links in here ) so as to fine-tune it for Pat's OLECT-LM stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ab0319e-1d1e-4fa3-9153-e8c26344e652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1717091264_20240530T174744-0600\n"
     ]
    }
   ],
   "source": [
    "# # Don't need this again\n",
    "# !powershell -c (Get-Date -UFormat \\\"%s_%Y%m%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8476aae1-0e49-4094-8eb8-6bf0d51acbc7",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`1717091264_20240530T174744-0600`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb51da5-6c05-4870-931c-7068e575ac99",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af7c2afe-9b5f-4ee6-9db6-9f8d2cf6b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "import torch\n",
    "from transformers import AutoTokenizer, \\\n",
    "                         AutoModelForSeq2SeqLM, \\\n",
    "                         TrainingArguments, \\\n",
    "                         pipeline\n",
    "from transformers.utils import logging\n",
    "from peft import LoraConfig, \\\n",
    "                 prepare_model_for_kbit_training, \\\n",
    "                 get_peft_model, \\\n",
    "                 AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login, notebook_login\n",
    "\n",
    "from datasets import load_metric\n",
    "import nltk\n",
    "import rouge_score\n",
    "\n",
    "import pickle\n",
    "import pprint\n",
    "import timeit\n",
    "from humanfriendly import format_timespan\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bfe2f4-b1b4-4158-b3c6-048c487514d2",
   "metadata": {},
   "source": [
    "## Load the training and test dataset along with the LLM with its tokenizer\n",
    "\n",
    "The LLM will be fine-tuned. It seems the tokenizer will also be fine-tuned, \n",
    "but I'm not sure \n",
    "\n",
    "<b>Why aren't we loading the validation set?</b> (I don't know; that's not a teaching question.)\n",
    "\n",
    "I've tried to make use of it with the `trainer`. We'll see how it goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6043d426-bcee-4539-a9ea-4b7c4c3dccd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f4a12ae7ac4f4bbc906c874b7ff5ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8ef8e438ad54ad7a1e7249a3b51d4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating evaluation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3faf675f15476a947b273e455938c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afd510ddbd44afc924fdc3d2193b21f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anast\\.conda\\envs\\rwkv-lora-pat\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Anast\\.cache\\huggingface\\hub\\models--google--flan-t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0686e730baf44d21a413a860bcae0c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b18ac904f946f3bda2b02d057f994a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad810de917e4737b07244bdf77ef8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f28e438e88e4e1a899ac503975da655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53a58ecaf944f79bdf6e3c7ee6d5b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3097ebb3d45a466da413b1e39a0bb36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Need to install  datasets  from pip, not conda. I'll do all from pip. \n",
    "#+ I'll get rid of the current conda environment and make it anew.\n",
    "#+ Actually, I'll make sure  conda  and  pip  are updated, then do what\n",
    "#+ I discussed above.\n",
    "#+\n",
    "#+ cf. \n",
    "#+     arch_ref_1 = \"https://web.archive.org/web/20240522150357/\" + \\\n",
    "#+                  \"https://stackoverflow.com/questions/77433096/\" + \\\n",
    "#+                  \"notimplementederror-loading-a-dataset-\" + \\\n",
    "#+                  \"cached-in-a-localfilesystem-is-not-suppor\"\n",
    "#+\n",
    "#+ Also useful might be\n",
    "#+     arch_ref_2 = \"https://web.archive.org/web/20240522150310/\" + \\\n",
    "#+                  \"https://stackoverflow.com/questions/76340743/\" + \\\n",
    "#+                  \"huggingface-load-datasets-gives-\" + \\\n",
    "#+                  \"notimplementederror-cannot-error\"\n",
    "#\n",
    "data_files = {'train':'samsum-train.json', \n",
    "              'evaluation':'samsum-validation.json',\n",
    "              'test':'samsum-test.json'}\n",
    "dataset = load_dataset('json', data_files=data_files)\n",
    "\n",
    "model_name = \"google/flan-t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "#  Next line makes training faster but a little less accurate\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                          trust_remote_code=True)\n",
    "\n",
    "#  padding instructions for the tokenizer\n",
    "#+   ??? !!! What about for RWKV !!! ???\n",
    "#+ Will it be the same?\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc6b24-d617-48a1-8236-c3be16402c4a",
   "metadata": {},
   "source": [
    "#### Trying some things I've been learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a107cbdc-5a73-4371-8afe-faa2dc623c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 512)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62aaf9b8-19fe-4255-81bb-47dfa40cfcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"google_-flan-t5-small.model-architecture.txt\", 'w', encoding='utf-8') as fh:\n",
    "    fh.write(str(model))\n",
    "##endof:  with open ... fh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59156663-4b5b-4b60-8b37-627a9f297fc9",
   "metadata": {},
   "source": [
    "## Prompt and Trainer\n",
    "\n",
    "For our SFT (<b>S</b>upervised <b>F</b>ine <b>T</b>uning) model, we use the `class trl.SFTTrainer`.\n",
    "\n",
    "I want to research this a bit, especially the `formatting_func` that we'll be passing to the `SFTTrainer`.\n",
    "\n",
    "First, though, some information about SFT. From the Hugging Face Documentation at https://huggingface.co/docs/trl/en/sft_trainer ([archived](https://web.archive.org/web/20240529140717/https://huggingface.co/docs/trl/en/sft_trainer))\n",
    "\n",
    "> Supervised fine-tuning (or SFT for short) is a crucial step in RLHF. In TRL we provide an easy-to-use API to create your SFT models and train them with few lines of code on your dataset.\n",
    "\n",
    "Though I won't be using the examples unless I get even more stuck, the next paragraph _has_ examples, and I'll put the paragraph here.\n",
    "\n",
    "> Check out a complete flexible example at [examples/scripts/sft.py](https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py) \\[[archived](https://web.archive.org/web/20240529140740/https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py)\\]. Experimental support for Vision Language Models is also included in the example [examples/scripts/vsft_llava.py](https://github.com/huggingface/trl/blob/main/examples/scripts/vsft_llava.py) \\[[archived](https://web.archive.org/web/20240529140738/https://github.com/huggingface/trl/blob/main/examples/scripts/vsft_llava.py)\\].\n",
    "\n",
    "RLHF ([archived wikipedia page](https://web.archive.org/web/20240529142205/https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback)) is <b>R</b>einforcement <b>L</b>earning from <b>H</b>uman <b>F</b>eedback. [TRL](https://huggingface.co/docs/trl/en/index#:~:text=TRL%20is%20a%20full%20stack,Policy%20Optimization%20(PPO)%20step.) ([archived]())       <b>T</b>ransfer <b>R</b>einforcement <b>L</b>earning, a library from Hugging Face.\n",
    "\n",
    "For the parameter, `formatting_func`, I can look ath the documentation site above (specifically [here](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=formatting_func%20(Optional)), at the GitHub repo for [the code](https://github.com/huggingface/trl/blob/main/trl/trainer/sft_trainer.py) (in the docstrings), or from my local `conda` environment, at `C:\\Users\\bballdave025\\.conda\\envs\\rwkv-lora-pat\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py`.\n",
    "\n",
    "Pulling code from the last one, I get\n",
    "\n",
    ">         formatting_func (`Optional[Callable]`):\n",
    ">            The formatting function to be used for creating the `ConstantLengthDataset`.\n",
    "\n",
    "That matches the first very well\n",
    "\n",
    "> <b>formatting_func</b> (`Optional[Callable]`) â€” The formatting function to be used for creating the `ConstantLengthDataset`.\n",
    "\n",
    "(A quick note: In this Jupyter Notebook environment, I could have typed `trainer = SFTTrainer(` and then <kbd>Shift</kbd> + <kbd>Tab</kbd> to find that same documentation.\n",
    "\n",
    "However, I think that more clarity is found at the [documentation for `ConstantLengthDataset](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=class%20trl.trainer.ConstantLengthDataset)\n",
    "\n",
    "> <b>formatting_func</b> (`Callable`, <b>optional</b>) â€” Function that formats the text before tokenization. Usually it is recommended to have follows a certain pattern such as `\"### Question: {question} ### Answer: {answer}\"`\n",
    "\n",
    "So, as we'll see the next code from  the tutorial, it basically is a prompt templater/formatter that matches the JSON. For example, we use `sample['dialogue']` to access the `dialogue` key/pair. That's what I got from all this stuff.\n",
    "\n",
    "Mehul Gupta himself stated\n",
    "\n",
    "> Next, using the Input and Output, we will create a prompt template which is a requirement by the SFTTrainer we will be using later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cdd3ac-f714-41ed-a1fb-c27395b9251d",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fcb36fc-783f-4f19-b213-69cc3dbf05af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_instruction_format(sample):\n",
    "    return f\"\"\" Instruction:\n",
    "      Use the Task below and the Input given to write the Response:\n",
    "\n",
    "      ### Task:\n",
    "      Summarize the Input\n",
    "\n",
    "      ### Input:\n",
    "      {sample['dialogue']}\n",
    "\n",
    "      ### Response:\n",
    "      {sample['summary']}\n",
    "      \"\"\"\n",
    "##endof:  prompt_instruction_format(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3622673e-df80-43da-9d5c-efd6085d301d",
   "metadata": {},
   "source": [
    "### Trainer - the LoRA Setup Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da39f5e-4e14-4eb5-ac66-c90a77472c64",
   "metadata": {},
   "source": [
    "#### Arguments and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ac040cf-2eb5-4eb5-91c7-2e47c54b990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Some arguments to pass to the trainer\n",
    "training_args = TrainingArguments( \n",
    "                        output_dir='output',\n",
    "                        num_train_epochs=1,\n",
    "                        per_device_train_batch_size=4,\n",
    "                        save_strategy='epoch',\n",
    "                        learning_rate=2e-4,\n",
    "                        do_eval=True,\n",
    "                        per_device_eval_batch_size=4,\n",
    "                        eval_strategy='epoch',\n",
    "                        hub_model_id=\"dwb-flan-t5-small-lora-finetune\",\n",
    ")\n",
    "\n",
    "# the fine-tuning (peft for LoRA) stuff\n",
    "peft_config = LoraConfig( lora_alpha=16,\n",
    "                          lora_dropout=0.1,\n",
    "                          r=64,\n",
    "                          bias='none',\n",
    "                          task_type='CAUSAL_LM'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa2a84-c5f6-45ad-9d09-fb9f25fae9da",
   "metadata": {},
   "source": [
    "`task_type`, cf. https://github.com/huggingface/peft/blob/main/src/peft/config.py#L222\n",
    "\n",
    ">        Args:\n",
    ">            peft_type (Union[[`~peft.utils.config.PeftType`], `str`]): The type of Peft method to use.\n",
    ">            task_type (Union[[`~peft.utils.config.TaskType`], `str`]): The type of task to perform.\n",
    ">            inference_mode (`bool`, defaults to `False`): Whether to use the Peft model in inference mode.\n",
    "\n",
    "After some searching using Cygwin\n",
    "\n",
    "```\n",
    "bballdave025@MYMACHINE /cygdrive/c/Users/bballdave025/.conda/envs/rwkv-lora-pat/Lib/site-packages/peft/utils\n",
    "$ ls -lah\n",
    "total 116K\n",
    "drwx------+ 1 bballdave025 bballdave025    0 May 28 21:09 .\n",
    "drwx------+ 1 bballdave025 bballdave025    0 May 28 21:09 ..\n",
    "-rwx------+ 1 bballdave025 bballdave025 2.0K May 28 21:09 __init__.py\n",
    "drwx------+ 1 bballdave025 bballdave025    0 May 28 21:09 __pycache__\n",
    "-rwx------+ 1 bballdave025 bballdave025 8.0K May 28 21:09 constants.py\n",
    "-rwx------+ 1 bballdave025 bballdave025 3.8K May 28 21:09 integrations.py\n",
    "-rwx------+ 1 bballdave025 bballdave025  17K May 28 21:09 loftq_utils.py\n",
    "-rwx------+ 1 bballdave025 bballdave025 9.7K May 28 21:09 merge_utils.py\n",
    "-rwx------+ 1 bballdave025 bballdave025  25K May 28 21:09 other.py\n",
    "-rwx------+ 1 bballdave025 bballdave025 2.2K May 28 21:09 peft_types.py\n",
    "-rwx------+ 1 bballdave025 bballdave025  21K May 28 21:09 save_and_load.py\n",
    "\n",
    "bballdave025@MYMACHINE /cygdrive/c/Users/bballdave025/.conda/envs/rwkv-lora-pat/Lib/site-packages/peft/utils\n",
    "$ grep -iIRHn \"TaskType\" .\n",
    "peft_types.py:60:class TaskType(str, enum.Enum):\n",
    "__init__.py:20:# from .config import PeftConfig, PeftType, PromptLearningConfig, TaskType\n",
    "__init__.py:22:from .peft_types import PeftType, TaskType\n",
    "\n",
    "bballdave025@MYMACHINE /cygdrive/c/Users/bballdave025/.conda/envs/rwkv-lora-pat/Lib/site-packages/peft/utils\n",
    "$\n",
    "```\n",
    "\n",
    "So, let's look at the `peft_types.py` file.\n",
    "\n",
    "The docstring for `class TaskType(str, enum.Enum)` is\n",
    "\n",
    "```\n",
    "    Enum class for the different types of tasks supported by PEFT.\n",
    "    \n",
    "    Overview of the supported task types:\n",
    "    - SEQ_CLS: Text classification.\n",
    "    - SEQ_2_SEQ_LM: Sequence-to-sequence language modeling.\n",
    "    - CAUSAL_LM: Causal language modeling.\n",
    "    - TOKEN_CLS: Token classification.\n",
    "    - QUESTION_ANS: Question answering.\n",
    "    - FEATURE_EXTRACTION: Feature extraction. Provides the hidden states which can be used as embeddings or features\n",
    "      for downstream tasks.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e178f3f-9dd6-409e-9a4d-c1c85611a8b9",
   "metadata": {},
   "source": [
    "### We're going to start timing stuff, so here's some system info\n",
    "\n",
    "`win_system_info_as_script.py` is a script I wrote with the help\n",
    "of a variety of StackOverflow and documentation sources.\n",
    "It should be in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b22e51b0-8f09-4693-b991-e853ca3cfccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#########################  System Information  #########################\n",
      "System: Windows\n",
      "Node Name: NOT-FOR-NOW\n",
      "Release: 10\n",
      "Version: 10.0.19045\n",
      "Machine: AMD64\n",
      "Processor: Intel64 Family 6 Model 165 Stepping 3, GenuineIntel\n",
      "Processor: Intel(R) Core(TM) i3-10100 CPU @ 3.60GHz\n",
      "Ip-Address: NOT-FOR-NOW\n",
      "Mac-Address: NOT-FOR-NOW\n",
      "\n",
      "#############################  Boot Time  ##############################\n",
      "Boot Time (date and time of last boot) was\n",
      "Boot Time: 2024-5-26T14:29:0\n",
      "\n",
      "##############################  CPU Info  ##############################\n",
      "Physical cores: 4\n",
      "Total cores: 8\n",
      "CPU Usage Per Core:\n",
      "Core 0: 3.1%\n",
      "Core 1: 1.6%\n",
      "Core 2: 6.2%\n",
      "Core 3: 0.0%\n",
      "Core 4: 3.1%\n",
      "Core 5: 0.0%\n",
      "Core 6: 6.2%\n",
      "Core 7: 0.0%\n",
      "Total CPU Usage: 5.7%\n",
      "Max Frequency: 3600.00Mhz\n",
      "Min Frequency: 0.00Mhz\n",
      "Current Frequency: 3600.00Mhz\n",
      "\n",
      "##############################  GPU Info  ##############################\n",
      "Information on GPU(s)/Graphics Card(s)\n",
      " (if any such information is to be found)\n",
      "\n",
      "Using  wmi , we get the following  win32_VideoController  names.\n",
      "   Trigger 6 External Graphics\n",
      "   Intel(R) UHD Graphics 630\n",
      "Using  PyTorch  and the  torch.cuda.is_available()  method.\n",
      "The statement, 'There is CUDA and an appropriate GPU',\n",
      "  is ...  False\n",
      "Using  TensorFlow  with several of its methods.\n",
      "  Attempting to get GPU Device List\n",
      "No GPU Devices.\n",
      "Tensorflow can give us CPU (and/or GPU) info.\n",
      "The info here might help you know if we're running on a CPU.\n",
      "\n",
      "Trying to use some nvidia code ( nvidia-smi ) to find information\n",
      "  That's the end of the nvidia try.\n",
      "\n",
      "Those are all our chances to find out about any GPU/Graphics Cards.\n",
      "\n",
      "######################  Memory (RAM) Information  ######################\n",
      "Total: 31.67GbB\n",
      "Available: 17.95GbB\n",
      "Used: 13.72GbB\n",
      "Percentage: 43.3%\n",
      "      =============== SWAP Memory ===============      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  That nvidia stuff didn't work\n",
      "  The error information is:\n",
      "[WinError 2] The system cannot find the file specified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 4.75GbB\n",
      "Free: 4.64GbB\n",
      "Used: 108.27MbB\n",
      "Percentage: 2.2%\n",
      "\n",
      "#############################  Disk Info  ##############################\n",
      "Partitions and Usage:\n",
      "=== Device: C:\\ ===\n",
      "  Mountpoint: C:\\\n",
      "  File system type: NTFS\n",
      "  Total Size: 915.94GbB\n",
      "  Used: 587.01GbB\n",
      "  Free: 328.93GbB\n",
      "  Percentage: 64.1%\n",
      "=== Device: D:\\ ===\n",
      "  Mountpoint: D:\\\n",
      "  File system type: exFAT\n",
      "  Total Size: 12.73TbB\n",
      "  Used: 1.99TbB\n",
      "  Free: 10.75TbB\n",
      "  Percentage: 15.6%\n",
      "=== Device: E:\\ ===\n",
      "  Mountpoint: E:\\\n",
      "  File system type: FAT32\n",
      "  Total Size: 115.31GbB\n",
      "  Used: 46.08GbB\n",
      "  Free: 69.23GbB\n",
      "  Percentage: 40.0%\n",
      "  Since last boot,\n",
      "Total read: 158.10GbB\n",
      "Total write: 204.07GbB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import win_system_info_as_script as winsysinfo\n",
    "winsysinfo.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864b9483-f8ff-47d7-ad94-b833f27d6e9e",
   "metadata": {},
   "source": [
    "### ROUGE Metrics\n",
    "\n",
    "Some references from the Microsoft/Google (who?) implementation\n",
    "\n",
    "https://pypi.org/project/rouge-score/\n",
    "\n",
    "https://web.archive.org/web/20240530231357/https://pypi.org/project/rouge-score/\n",
    "\n",
    "<br/>\n",
    "\n",
    "https://github.com/google-research/google-research/tree/master/rouge\n",
    "\n",
    "https://web.archive.org/web/20240530231412/https://github.com/google-research/google-research/tree/master/rouge\n",
    "\n",
    "<br/>\n",
    "\n",
    "Not the one I used:\n",
    "\n",
    "https://github.com/microsoft/nlp-recipes/blob/master/examples/text_summarization/summarization_evaluation.ipynb\n",
    "\n",
    "https://web.archive.org/web/20240530231709/https://github.com/microsoft/nlp-recipes/blob/master/examples/text_summarization/summarization_evaluation.ipynb\n",
    "\n",
    "<br/>\n",
    "\n",
    "Someone else made this other one, which I inspected but didn't use.\n",
    "\n",
    "https://pypi.org/project/rouge/\n",
    "\n",
    "https://web.archive.org/web/20240530232029/https://pypi.org/project/rouge/\n",
    "\n",
    "https://github.com/pltrdy/rouge\n",
    "\n",
    "https://web.archive.org/web/20240530232023/https://github.com/pltrdy/rouge\n",
    "\n",
    "but I think he defers to the rouge_score from Google."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907cdf67-55ac-41c6-a81c-7e89b4ddade8",
   "metadata": {},
   "source": [
    "#### My ROUGE Metrics\n",
    "\n",
    "I want to use the skip-grams score. Thanks to\n",
    "\n",
    "https://www.bomberbot.com/machine-learning/skip-bigrams-in-system/\n",
    "\n",
    "https://web.archive.org/web/20240530230949/https://www.bomberbot.com/machine-learning/skip-bigrams-in-system/\n",
    "\n",
    "I can do this as well as writing the code for the other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9287dd25-f5fe-45a2-b47c-7f8baaea0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# def rouge_n(system, reference, n):\n",
    "    # '''\n",
    "    # ROUGE-N : N-Grams implementation\n",
    "\n",
    "    # ref = \"https://web.archive.org/web/20240530230949/\" + \\\n",
    "          # \"https://www.bomberbot.com/machine-learning/\" + \\\n",
    "          # \"skip-bigrams-in-system/\"\n",
    "\n",
    "    # @param  system      string   The hypothesis\n",
    "    # @param  reference   string   The truth\n",
    "    # @param  n           string   The \"n\" in \"n-gram\", i.e.\n",
    "                                  # the number of words in\n",
    "                                  # each grouping\n",
    "\n",
    "    # @returns  dict in form {\"recall\": recall,\n",
    "                            # \"precision\": precision,\n",
    "                            # \"f-measure\": f_measure}\n",
    "\n",
    "    # Example:\n",
    "      # >>> import rouge_n\n",
    "      # >>>\n",
    "      # >>> system = \"The cat was found under the bed.\"\n",
    "      # >>> reference = \"The cat was hidden under the bed.\"\n",
    "      # >>>\n",
    "      # >>> print(rouge_n(system, reference, 1)) # ROUGE-1\n",
    "      # >>> print(rouge_n(system, reference, 2)) # ROUGE-2\n",
    "      # {â€˜recallâ€˜: 0.8571428571428571, â€˜precisionâ€˜: 1.0, â€˜f-measureâ€˜: 0.9230769230769231}\n",
    "      # {â€˜recallâ€˜: 0.6, â€˜precisionâ€˜: 0.5, â€˜f-measureâ€˜: 0.5454545454545455}\n",
    "    # '''\n",
    "    \n",
    "    # sys_ngrams = list(itertools.ngrams(system.split(), n))\n",
    "    # ref_ngrams = list(itertools.ngrams(reference.split(), n))\n",
    "    \n",
    "    # overlaps = set(sys_ngrams) & set(ref_ngrams)\n",
    "    # recall = len(overlaps) / len(ref_ngrams)\n",
    "    # precision = len(overlaps) / len(sys_ngrams)\n",
    "    \n",
    "    # if precision + recall == 0:\n",
    "        # f_measure = 0\n",
    "    # else:\n",
    "        # f_measure = 2  precision  recall / (precision + recall)\n",
    "    # ##endof:  if/else precision + recall == 0\n",
    "    # return {\"recall\": recall, \"precision\": precision, \"f-measure\": f_measure}\n",
    "# ##endof:  rouge_n(system, reference, n)\n",
    "\n",
    "\n",
    "\n",
    "# def lcs(X, Y): \n",
    "    # '''\n",
    "    # Longest common subsequence\n",
    "    # '''\n",
    "    \n",
    "    # m = len(X) \n",
    "    # n = len(Y) \n",
    "    \n",
    "    # L = [[None]*(n+1) for i in range(m+1)] \n",
    "    \n",
    "    # for i in range(m+1): \n",
    "        # for j in range(n+1): \n",
    "            # if i == 0 or j == 0: \n",
    "                # L[i][j] = 0\n",
    "            # elif X[i-1] == Y[j-1]: \n",
    "                # L[i][j] = L[i-1][j-1]+1\n",
    "            # else: \n",
    "                # L[i][j] = max(L[i-1][j], L[i][j-1])\n",
    "            # ##endof:  if\n",
    "        # ##endof:  for j\n",
    "    # ##endof:  for i\n",
    "    # return L[m][n] \n",
    "# ##endof:  lcs(X, Y>\n",
    "\n",
    "# def rouge_l(system, reference):\n",
    "   # '''\n",
    "   # ROUGE-L : Longest Common Subsequence implementation\n",
    "\n",
    "   # ref = \"https://web.archive.org/web/20240530230949/\" + \\\n",
    "          # \"https://www.bomberbot.com/machine-learning/\" + \\\n",
    "          # \"skip-bigrams-in-system/\"\n",
    "    \n",
    "    # @param  system      string   The hypothesis\n",
    "    # @param  reference   string   The truth\n",
    "\n",
    "    # @returns  dict in form {\"recall\": recall,\n",
    "                            # \"precision\": precision,\n",
    "                            # \"f-measure\": f_measure}\n",
    "\n",
    "\n",
    "    # Example:\n",
    "      # >>> import rouge_l, lcs\n",
    "      # >>>\n",
    "      # >>> system = \"The quick dog jumps over the lazy fox.\"\n",
    "      # >>>reference = \"The quick brown fox jumps over the lazy dog.\"\n",
    "      # >>>\n",
    "      # >>> print(rouge_l(system, reference))\n",
    "      # {â€˜recallâ€˜: 0.7777777777777778, â€˜precisionâ€˜: 0.875, â€˜f-measureâ€˜: 0.823529411764706}\n",
    "    # '''\n",
    "    \n",
    "    # sys_len = len(system.split())\n",
    "    # ref_len = len(reference.split())\n",
    "    # lcs_len = lcs(system.split(), reference.split())\n",
    "    \n",
    "    # recall = lcs_len / ref_len\n",
    "    # precision = lcs_len / sys_len\n",
    "\n",
    "    # if precision + recall == 0:\n",
    "        # f_measure = 0\n",
    "    # else:\n",
    "        # f_measure = 2  precision  recall / (precision + recall)\n",
    "    # ##endof:  if/else\n",
    "    \n",
    "    # return {\"recall\": recall, \"precision\": precision, \"f-measure\": f_measure}\n",
    "# ##endof:  rouge_l(system, reference)\n",
    "\n",
    "\n",
    "\n",
    "# from itertools import combinations\n",
    "\n",
    "# def skipbigrams(sequence, n):\n",
    "    # '''\n",
    "    # Returns the set of skip n-grams\n",
    "    \n",
    "    # @param  sequence\n",
    "    # @param  n\n",
    "    # '''\n",
    "    \n",
    "    # return set(combinations(sequence, n))\n",
    "    \n",
    "# ##endof:  skipbigrams(sequence, n=2)\n",
    "\n",
    "# def rouge_s(system, reference, n=2):\n",
    "    # '''\n",
    "    # ROUGE-S : Skip Bigrams implementation\n",
    "    \n",
    "    # @param\n",
    "    # @param\n",
    "    # @param\n",
    "    \n",
    "    # @returns\n",
    "    \n",
    "    \n",
    "    # Example\n",
    "      # >>> import skipbigrams, rouge_s\n",
    "      # >>>\n",
    "      # >>> system = \"The quick dog jumps over the lazy fox.\"\n",
    "      # >>> reference = \"The quick brown fox jumps over the lazy dog.\"  \n",
    "      # >>>\n",
    "      # >>> print(rouge_s(system, reference))\n",
    "      # {â€˜recallâ€˜: 0.35, â€˜precisionâ€˜: 0.4166666666666667, â€˜f-measureâ€˜: 0.38095238095238093}\n",
    "    # '''\n",
    "    # sys_skipbigrams = skipbigrams(system.split(), n)\n",
    "    # ref_skipbigrams = skipbigrams(reference.split(), n)\n",
    "    \n",
    "    # overlaps = sys_skipbigrams & ref_skipbigrams\n",
    "    # recall = len(overlaps) / len(ref_skipbigrams)\n",
    "    # precision = len(overlaps) / len(sys_skipbigrams)\n",
    "    \n",
    "    # if precision + recall == 0:\n",
    "        # f_measure = 0\n",
    "    # else:\n",
    "        # f_measure = 2  precision  recall / (precision + recall)\n",
    "    # ##endof:  if/else\n",
    "    \n",
    "    # return {\"recall\": recall, \"precision\": precision, \"f-measure\": f_measure}\n",
    "# ##endof:  rouge_s(system, reference, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5cd858-f6e7-4dba-b689-75e3d950f26d",
   "metadata": {},
   "source": [
    "### Try for a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba0b4db-cd1c-466c-91be-cc0e2a8a14c1",
   "metadata": {},
   "source": [
    "#### Just one summarization to begin with, randomly picked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a4e54f9-6693-454d-b4a1-fca681baf39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1717094554_20240530T184234-0600\n"
     ]
    }
   ],
   "source": [
    "# # Don't need this again\n",
    "#!powershell -c (Get-Date -UFormat \\\"%s_%Y%m%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d892f4f-d28f-4371-bcb1-9ce214c6f3b3",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`1717094554_20240530T184234-0600`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68250f44-daf6-436e-a7d2-dcb7a8d97ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue: \n",
      "Harry: and? have you listened to it?\n",
      "Jacob: listened to what?\n",
      "Harry: to the song i sent you 3 days ago -.-\n",
      "Jacob: oh shit, i completely forgot...\n",
      "Harry: ofc again\n",
      "Jacob: don't be like this :* i'll do that later tonight\n",
      "Harry: heh, okay\n",
      "Harry: i'm really curious what you'll think about it\n",
      "Jacob: i'll let you know, a bit busy right now, speak to you later!\n",
      "Harry: okay\n",
      "---------------\n",
      "flan-t5-small summary:\n",
      "Jacob forgot to listen to the song he sent Jacob 3 days ago. Harry will let Jacob know later tonight. Jacob will talk to Harry later.\n"
     ]
    }
   ],
   "source": [
    "#  Just one summarization to begin with, randomly picked ... but\n",
    "#+ now with th possibility of a known seed, to allow visual \n",
    "#+ comparison with after-training results.\n",
    "#+ I'M NOT GOING TO USE THIS REPEATED SEED, I'm just going to\n",
    "#+ use the datum at the first index to compare.\n",
    "\n",
    "do_seed_for_repeatable = False\n",
    "\n",
    "summarizer = pipeline('summarization', model=model, tokenizer=tokenizer)\n",
    "\n",
    "if do_seed_for_repeatable:\n",
    "    rand_seed_for_randrange = 137\n",
    "    random.seed(rand_seed_for_randrange)\n",
    "##endof:  if do_seed_for_repeatable\n",
    "\n",
    "sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n",
    "\n",
    "res = summarizer(sample[\"dialogue\"])\n",
    "\n",
    "print(f\"flan-t5-small summary:\\n{res[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21170b7-0685-4e39-b9d6-2eaf5624d476",
   "metadata": {},
   "source": [
    "#### Now, one summarization with comparison to ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0a760e7-6270-4e65-93f0-1af0c3dd559e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but your input_length is only 133. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=66)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialogue: \n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Hannah: <file_gif>\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Hannah: <file_gif>\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him ðŸ™‚\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "---------------\n",
      "human-genratd summary:\n",
      "Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
      "flan-t5-small summary:\n",
      "Larry called Hannah last time she was at the park together. Hannah doesn't know Larry well. Larry called her last time they were at a park. Hannah will text Larry.\n",
      "\n",
      "\n",
      "---------- ROUGE SCORES ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anast\\AppData\\Local\\Temp\\ipykernel_8400\\2790818686.py:26: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric('rouge', trust_remote_code=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3857662c21b4975b3ae7672b7736ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROUGE-1 results\n",
      "AggregateScore(low=Score(precision=0.16129032258064516, recall=0.3125, fmeasure=0.2127659574468085), mid=Score(precision=0.16129032258064516, recall=0.3125, fmeasure=0.2127659574468085), high=Score(precision=0.16129032258064516, recall=0.3125, fmeasure=0.2127659574468085))\n",
      "\n",
      "ROUGE-2 results\n",
      "AggregateScore(low=Score(precision=0.03333333333333333, recall=0.06666666666666667, fmeasure=0.04444444444444444), mid=Score(precision=0.03333333333333333, recall=0.06666666666666667, fmeasure=0.04444444444444444), high=Score(precision=0.03333333333333333, recall=0.06666666666666667, fmeasure=0.04444444444444444))\n",
      "\n",
      "ROUGE-L results\n",
      "AggregateScore(low=Score(precision=0.12903225806451613, recall=0.25, fmeasure=0.1702127659574468), mid=Score(precision=0.12903225806451613, recall=0.25, fmeasure=0.1702127659574468), high=Score(precision=0.12903225806451613, recall=0.25, fmeasure=0.1702127659574468))\n",
      "\n",
      "ROUGE-Lsum results\n",
      "AggregateScore(low=Score(precision=0.12903225806451613, recall=0.25, fmeasure=0.1702127659574468), mid=Score(precision=0.12903225806451613, recall=0.25, fmeasure=0.1702127659574468), high=Score(precision=0.12903225806451613, recall=0.25, fmeasure=0.1702127659574468))\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline('summarization', model=model, tokenizer=tokenizer)\n",
    "\n",
    "pred_test_list = []\n",
    "ref_test_list = []\n",
    "\n",
    "sample_num = 0\n",
    "\n",
    "this_sample = dataset['test'][sample_num]\n",
    "\n",
    "print(f\"dialogue: \\n{this_sample['dialogue']}\\n---------------\")\n",
    "\n",
    "grnd_summary = this_sample['summary']\n",
    "res = summarizer(this_sample['dialogue'])\n",
    "res_summary = res[0]['summary_text']\n",
    "\n",
    "# humgen is for human-generated\n",
    "\n",
    "print(f\"human-genratd summary:\\n{grnd_summary}\")\n",
    "print(f\"flan-t5-small summary:\\n{res_summary}\")\n",
    "\n",
    "ref_test_list.append(grnd_summary)\n",
    "pred_test_list.append(res_summary)\n",
    "\n",
    "print(\"\\n\\n---------- ROUGE SCORES ----------\")\n",
    "\n",
    "rouge = load_metric('rouge', trust_remote_code=True)\n",
    "\n",
    "results = rouge.compute(predictions=pred_test_list,\n",
    "                        references=ref_test_list,\n",
    "                        use_aggregator=True)\n",
    "\n",
    "# >>> print(list(results.keys()))\n",
    "# ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "\n",
    "print()\n",
    "print(\"ROUGE-1 results\")\n",
    "pprint.pp(results['rouge1'])\n",
    "print()\n",
    "print(\"ROUGE-2 results\")\n",
    "pprint.pp(results['rouge2'])\n",
    "print()\n",
    "print(\"ROUGE-L results\")\n",
    "pprint.pp(results['rougeL'])\n",
    "print()\n",
    "print(\"ROUGE-Lsum results\")\n",
    "pprint.pp(results['rougeLsum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f099cc0c-bce3-49e5-8ca8-afd212adbc70",
   "metadata": {},
   "source": [
    "#### Verbosity stuff - get rid of the nice advice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ea7a763-7427-4054-ab79-ee8d0931f25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1717094688_2024-05-30T184448-0600\n"
     ]
    }
   ],
   "source": [
    "# # Don't need this again\n",
    "# !powershell -c (Get-Date -UFormat \\\"%s_%Y-%m-%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb761d-d836-49a4-b585-29aae74f0c86",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`1717094688_2024-05-30T184448-0600`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb49d3a5-b58e-468a-836a-84a3908fd27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The statement, 'logging verbosity is CRITICAL' is False\n",
      "The statement, 'logging verbosity is    ERROR' is False\n",
      "The statement, 'logging verbosity is  WARNING' is True\n",
      "The statement, 'logging verbosity is     INFO' is False\n",
      "The statement, 'logging verbosity is    DEBUG' is False\n",
      "\n",
      "The value of logging.get_verbosity() is: 30\n",
      "\n",
      "TRANSFORMERS_NO_ADIVSORY_WARNINGS: None\n"
     ]
    }
   ],
   "source": [
    "log_verbosity_is_critical = \\\n",
    "  logging.get_verbosity() == logging.CRITICAL # alias FATAL, 50\n",
    "log_verbosity_is_error = \\\n",
    "  logging.get_verbosity() == logging.ERROR # 40\n",
    "log_verbosity_is_warn = \\\n",
    "  logging.get_verbosity() == logging.WARNING # alias WARN, 30\n",
    "log_verbosity_is_info = \\\n",
    "  logging.get_verbosity() == logging.INFO # 20\n",
    "log_verbosity_is_debug = \\\n",
    "  logging.get_verbosity() == logging.DEBUG # 10\n",
    "\n",
    "print( \"The statement, 'logging verbosity is CRITICAL' \" + \\\n",
    "      f\"is {log_verbosity_is_critical}\")\n",
    "print( \"The statement, 'logging verbosity is    ERROR' \" + \\\n",
    "      f\"is {log_verbosity_is_error}\")\n",
    "print( \"The statement, 'logging verbosity is  WARNING' \" + \\\n",
    "      f\"is {log_verbosity_is_warn}\")\n",
    "print( \"The statement, 'logging verbosity is     INFO' \" + \\\n",
    "      f\"is {log_verbosity_is_info}\")\n",
    "print( \"The statement, 'logging verbosity is    DEBUG' \" + \\\n",
    "      f\"is {log_verbosity_is_debug}\")\n",
    "\n",
    "print()\n",
    "\n",
    "init_log_verbosity = logging.get_verbosity()\n",
    "print(f\"The value of logging.get_verbosity() is: {init_log_verbosity}\")\n",
    "\n",
    "print()\n",
    "\n",
    "init_t_n_a_w = os.environ.get('TRANSFORMERS_NO_ADVISORY_WARNINGS')\n",
    "print(f\"TRANSFORMERS_NO_ADIVSORY_WARNINGS: {init_t_n_a_w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a66b11-e35e-4663-9bcc-602bd3cd060d",
   "metadata": {},
   "source": [
    "### Actual Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c671b535-247a-4510-9a09-d8db4fbeab31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1717094729_2024-05-30T184529-0600\n"
     ]
    }
   ],
   "source": [
    "# # Don't need this again\n",
    "# !powershell -c (Get-Date -UFormat \\\"%s_%Y-%m-%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5dca6b-d4ca-440e-bdd2-71869263c6d7",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`1717094729_2024-05-30T184529-0600`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26c550c2-eee7-4393-9f92-1a25f4fcc2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting things ready for scoring\n",
      "took 1162.5236 seconds.\n",
      "\n",
      "\n",
      "---------- ROUGE SCORES ----------\n",
      "\n",
      "ROUGE-1 results\n",
      "AggregateScore(low=Score(precision=0.3623620420489957, recall=0.5387757354848512, fmeasure=0.4120367260545498), mid=Score(precision=0.37354505712494124, recall=0.5519205917207157, fmeasure=0.42157981166777414), high=Score(precision=0.38488859010129967, recall=0.5656367969330082, fmeasure=0.4313347594247768))\n",
      "\n",
      "ROUGE-2 results\n",
      "AggregateScore(low=Score(precision=0.15911356172159186, recall=0.2428538196441747, fmeasure=0.18143374370228235), mid=Score(precision=0.16776558103745187, recall=0.256702553043478, fmeasure=0.1902802753555807), high=Score(precision=0.176789440182549, recall=0.26994546288441695, fmeasure=0.19927249226979166))\n",
      "\n",
      "ROUGE-L results\n",
      "AggregateScore(low=Score(precision=0.2804766356825748, recall=0.4221646517546973, fmeasure=0.31994624442237346), mid=Score(precision=0.2892456873611609, recall=0.43475844856226864, fmeasure=0.32792714750993834), high=Score(precision=0.2984044560730355, recall=0.44750618279601273, fmeasure=0.33706771629160553))\n",
      "\n",
      "ROUGE-Lsum results\n",
      "AggregateScore(low=Score(precision=0.28022879848567583, recall=0.4220608213922142, fmeasure=0.319655889478565), mid=Score(precision=0.289591524472137, recall=0.43469242939551495, fmeasure=0.32826109350219757), high=Score(precision=0.29852002937109373, recall=0.44804213486471733, fmeasure=0.33687287121399573))\n"
     ]
    }
   ],
   "source": [
    "#  ref1 = \"https://web.archive.org/web/20240530051418/\" + \\\n",
    "#+        \"https://stackoverflow.com/questions/73221277/\" + \\\n",
    "#+        \"python-hugging-face-warning\"\n",
    "#  ref2 = \"https://web.archive.org/web/20240530051559/\" + \\\n",
    "#+        \"https://huggingface.co/docs/transformers/en/\" + \\\n",
    "#+        \"main_classes/logging\"\n",
    "\n",
    "##  Haven't tried this, because the logging seemed easier,\n",
    "##+ and the logging worked\n",
    "#os.environ(\"TRANSFORMERS_NO_ADVISORY_WARNINGS\") = 1\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "summarizer = pipeline('summarization', \n",
    "                      model=model, \n",
    "                      tokenizer=tokenizer)\n",
    "\n",
    "prediction_list = []\n",
    "reference_list = []\n",
    "\n",
    "tic = timeit.default_timer()\n",
    "\n",
    "for sample_num in range(len(dataset['test'])):\n",
    "  this_sample = dataset['test'][sample_num]\n",
    "  \n",
    "  #print(f\"dialogue: \\n{this_sample['dialogue']}\\n---------------\")\n",
    "\n",
    "  grnd_summary = this_sample['summary']\n",
    "  res = summarizer(this_sample['dialogue'])\n",
    "  res_summary = res[0]['summary_text']\n",
    "  \n",
    "  #print(f\"human-genratd summary:\\n{grnd_summary}\")\n",
    "  #print(f\"flan-t5-small summary:\\n{res_summary}\")\n",
    "  \n",
    "  reference_list.append(grnd_summary)\n",
    "  prediction_list.append(res_summary)\n",
    "##endof:  for sample_num in range(len(dataset['test']))\n",
    "\n",
    "toc = timeit.default_timer()\n",
    "\n",
    "baseline_duration = toc - tic\n",
    "\n",
    "print( \"Getting things ready for scoring\")\n",
    "print(f\"took {toc - tic:0.4f} seconds.\")\n",
    "\n",
    "print(\"\\n\\n---------- ROUGE SCORES ----------\")\n",
    "\n",
    "rouge = load_metric('rouge', trust_remote_code=True)\n",
    "  #  Set trust_remote_code=False to see the warning,\n",
    "  #+ deprecation, and what to change to.\n",
    "\n",
    "results = rouge.compute(predictions=prediction_list,\n",
    "                        references=reference_list,\n",
    "                        use_aggregator=True)\n",
    "\n",
    "# >>> print(list(results.keys()))\n",
    "# ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "\n",
    "print()\n",
    "print(\"ROUGE-1 results\")\n",
    "pprint.pp(results['rouge1'])\n",
    "print()\n",
    "print(\"ROUGE-2 results\")\n",
    "pprint.pp(results['rouge2'])\n",
    "print()\n",
    "print(\"ROUGE-L results\")\n",
    "pprint.pp(results['rougeL'])\n",
    "print()\n",
    "print(\"ROUGE-Lsum results\")\n",
    "pprint.pp(results['rougeLsum'])\n",
    "\n",
    "##  Haven't tried this, because the logging seemed easier,\n",
    "##+ and the logging worked\n",
    "# os.environ(\"TRANSFORMERS_NO_ADVISORY_WARNINGS\") = init_t_n_a_w\n",
    "\n",
    "logging.set_verbosity(init_log_verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd4df533-4ce4-45d6-9966-50cc2d862108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running baseline inference (using the test set)\n",
      "took 19 minutes and 22.52 seconds\n"
     ]
    }
   ],
   "source": [
    "do_enter_duration_manually = False\n",
    "\n",
    "if do_enter_duration_manually:\n",
    "    pass\n",
    "    #baseline_duration = # remember to type in your number, if needed\n",
    "##endof:  if do_enter_duration_manually\n",
    "\n",
    "print(\"Running baseline inference (using the test set)\")\n",
    "print(f\"took {format_timespan(baseline_duration)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df187315-53a9-4f66-a9f3-17db67512ddd",
   "metadata": {},
   "source": [
    "### Trainer - the Actual Trainer Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebc41e49-b3b8-4ea0-a3e0-aa40e4860be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1717096214_2024-05-30T191014-0600\n"
     ]
    }
   ],
   "source": [
    "# # Don't need this again\n",
    "# !powershell -c (Get-Date -UFormat \\\"%s_%Y-%m-%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5503fa3b-09b3-4b6e-b18c-3fb9791e715f",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`1717096214_2024-05-30T191014-0600`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb4feb5c-bb3c-444d-8814-6efa2317ee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "C:\\Users\\Anast\\.conda\\envs\\rwkv-lora-pat\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 512\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a5b6ae30d74d5eab0a50e2402268c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e5fc06377d4dbaaeb63e6706d5e4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer( model=model,\n",
    "                      train_dataset=dataset['train'],\n",
    "                      eval_dataset=dataset['evaluation'],\n",
    "                      peft_config=peft_config,\n",
    "                      tokenizer=tokenizer,\n",
    "                      packing=True,\n",
    "                      formatting_func=prompt_instruction_format,\n",
    "                      args=training_args,\n",
    "                    )\n",
    "##  Warnings are below output.\n",
    "\n",
    "##  Ended up not using this.\n",
    "#                      max_seq_length=675\n",
    "#          )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25010c2f-3f36-47b8-aad4-d5beec083d7c",
   "metadata": {},
   "source": [
    "First time warnings from the code above (as it still is).\n",
    "\n",
    "        \n",
    ">        WARNING:bitsandbytes.cextension:The installed version of bitsandbytes \\\n",
    ">         was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, \\\n",
    ">         and GPU quantization are unavailable.\n",
    ">        C:\\Users\\bballdave025\\.conda\\envs\\rwkv-lora-pat\\lib\\site-packages\\trl\\\\\n",
    ">         trainer\\sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` \\\n",
    ">        argument to the SFTTrainer, this will default to 512\n",
    ">         warnings.warn(\n",
    ">        \n",
    ">        [ > Generating train split: 6143/0 [00:04<00:00, 2034.36 examples/s] ]\n",
    ">        \n",
    ">        Token indices sequence length is longer than the specified maximum sequence \\\n",
    ">         length for this model (657 > 512). Running this sequence through the model \\\n",
    ">         will result in indexing errors\n",
    ">        \n",
    ">        [ > Generating train split: 355/0 [00:00<00:00, 6.10 examples/s] ]\n",
    "\n",
    "DWB Note\n",
    "\n",
    "<strike>So, I'm changing the `max_seq_length`.</strike> \n",
    "Maybe I should just throw out the offender(s) \n",
    "(along with the blank one that's in there somewhere),\n",
    "but I'll just continue as is.\n",
    "\n",
    "Actually, it appears I didn't run the updated cell, \n",
    "(with `max_seq_length=675`), since the\n",
    "Warning and Advice are still there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d556f5-92e7-4bdc-bbd0-73d643050446",
   "metadata": {},
   "source": [
    "## Let's Train This LoRA Thing and See How It Does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "376a82f8-7f45-454c-b301-a591298171c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1717096271_2024-05-30T191111-0600\n"
     ]
    }
   ],
   "source": [
    "# # Don't need this again\n",
    "# !powershell -c (Get-Date -UFormat \\\"%s_%Y-%m-%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd1014-d324-4293-930d-0ead8fdba381",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`1717096271_2024-05-30T191111-0600`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c6264-aab2-4a36-aa8b-71aa69c9719f",
   "metadata": {},
   "source": [
    "At about `1717063394_2024-05-30T100314-0600`, DWB went in and \n",
    "renamed `profile.ps1` to `NOT-USING_-_pro_file_-_now.ps1.bak`\n",
    "That should get rid of our errors from `powershell`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c15de9-adb4-433d-8a01-e633571bb076",
   "metadata": {},
   "source": [
    "### The long-time-taking training code is just below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4971f967-9d95-4606-9cc7-748834836b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1536' max='1536' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1536/1536 3:04:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.068500</td>\n",
       "      <td>0.022573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anast\\.conda\\envs\\rwkv-lora-pat\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tic: 362634.7966071\n",
      "toc: 373716.499057\n",
      "Training took 11081.7024 seconds.\n"
     ]
    }
   ],
   "source": [
    "tic = timeit.default_timer()\n",
    "trainer.train()\n",
    "toc = timeit.default_timer()\n",
    "print(f\"tic: {tic}\")\n",
    "print(f\"toc: {toc}\")\n",
    "training_duration = toc - tic\n",
    "print(f\"Training took {toc - tic:0.4f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "749bd2db-709b-45bb-b9c4-acb820cbd5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with LoRA (and with the other info as above)\n",
      "took 3 hours, 4 minutes and 41.7 seconds.\n"
     ]
    }
   ],
   "source": [
    "do_by_hand = False\n",
    "\n",
    "if do_by_hand:\n",
    "    pass\n",
    "    #training_duration = # make sure to enter your value, if necessary\n",
    "##endof:  if do_by_hand\n",
    "print( \"Training with LoRA (and with the other info as above)\")\n",
    "print(f\"took {format_timespan(training_duration)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "52aaea49-d34a-4ab1-89f1-65c7de627ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1717107458_2024-05-30T221738-0600\n"
     ]
    }
   ],
   "source": [
    "# # Don't need this again\n",
    "# !powershell -c (Get-Date -UFormat \\\"%s_%Y-%m-%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0520fa-6516-42cf-888f-08b0717e7d83",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`1717107458_2024-05-30T221738-0600`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5127700-78f2-4f17-b677-8d29789d30db",
   "metadata": {},
   "source": [
    "#### @todo : consolidate \"the other info as above\"\n",
    "\n",
    "I'm talking about the numbers of data points, tokens, whatever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ca0bc-360c-4708-a1a8-c33c5f3f731f",
   "metadata": {},
   "source": [
    "#### Any Comments / Things to Try (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d76966d-3466-463a-83fa-e21381b5365a",
   "metadata": {},
   "source": [
    "We passed an evaluation set (parameter ``) to the `trainer`.\n",
    "How can we see information about that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934771bd-3201-4393-be21-7036e49b0843",
   "metadata": {},
   "source": [
    "#### How to get the evaluation set used by the trainer\n",
    "\n",
    "I added the following parameters to the \n",
    "`training_args = TrainingArguments(<args>)`\n",
    "call.\n",
    "\n",
    "- `do_eval=True`\n",
    "- `per_device_eval_batch_size=4`\n",
    "- `eval_strategy='epoch'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c056cac-441f-42ae-9600-8db0bc991f82",
   "metadata": {},
   "source": [
    "#### How to specify your repo name\n",
    "\n",
    "I also added this next parameter to the arguments for\n",
    "`training_args = TrainingArguments(<args>)`\n",
    "\n",
    "- `hub_model_id=\"dwb-flan-t5-small-lora-finetune\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0b246e-c7bd-4fd7-8eab-b0ba46a2a922",
   "metadata": {},
   "source": [
    "#### The final TrainingArguments call - with parameter list\n",
    "\n",
    "```\n",
    "training_args = TrainingArguments( \n",
    "                        output_dir='output',\n",
    "                        num_train_epochs=1,\n",
    "                        per_device_train_batch_size=4,\n",
    "                        save_strategy='epoch',\n",
    "                        learning_rate=2e-4,\n",
    "                        do_eval=True,\n",
    "                        per_device_eval_batch_size=4,\n",
    "                        eval_strategy='epoch',\n",
    "                        hub_model_id=\"dwb-flan-t5-small-lora-finetune\",\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e62ce34-a826-4a83-8b2d-9acede4ff2a2",
   "metadata": {},
   "source": [
    "## Save the Trainer to Hugging Face and Get Our Updated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80265d1c-b65b-4863-8ac3-34e066c86522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1717145367_2024-05-31T084927-0600\n"
     ]
    }
   ],
   "source": [
    "# # Don't need this again\n",
    "# !powershell -c (Get-Date -UFormat \\\"%s_%Y-%m-%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c105c8-8182-4c12-9ce6-1016ee4787c5",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`1717145367_2024-05-31T084927-0600`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e0693e-579b-4a82-a2c2-2d3dc1e6f900",
   "metadata": {},
   "source": [
    "I'm following the [(archived) tutorial from Mehul Gupta on Medium](https://web.archive.org/web/20240522140323/https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578); since it's archived, you can follow exactly what I'm doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43e5a45e-1890-415c-9da3-9f43fa0969df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5150a502f643a1b5b097e53a7f6f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  This will come up with a dialog box with text entry.\n",
    "#+ and I'm now using the `thebballdave025@gmail.com`\n",
    "#+ ( @thebballdave025 for Hugging Face ) HF stuff.\n",
    "\n",
    "# Use the write token, here.\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77c99a5c-2739-49f6-9a4b-02c087e4c656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anast\\.conda\\envs\\rwkv-lora-pat\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aca152437474a919b1f2ba54afde15a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/11.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5688612f72564f6cba99dea5e27e8bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1717084743.DESKTOP-O7KM5A5.8272.0:   0%|          | 0.00/12.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a3a8a4b72443b18e0a105a9948d8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b1f9f26ed04cc9a9b1ad30979e10c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1717117975.DESKTOP-O7KM5A5.8400.0:   0%|          | 0.00/7.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1237aa6b99b84dcbba87c01469b614a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/thebballdave025/dwb-flan-t5-small-lora-finetune/commit/c87d34b398f3801ceb1e18c819a7c8fc894989c7', commit_message='End of training', commit_description='', oid='c87d34b398f3801ceb1e18c819a7c8fc894989c7', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save tokenizer and create a tokenizer model card\n",
    "tokenizer.save_pretrained('testing')\n",
    "  #  used 'testing' first - I think I can make a repo according\n",
    "  #+ to the first getting-started cli instructions, but let's\n",
    "  #+ use what Mehul Gupta used, first\n",
    "  #  Actually, I think 'testing' is the local directory\n",
    "\n",
    "# Create the trainer model card\n",
    "trainer.create_model_card()\n",
    "\n",
    "# Push the results to the Hugging Face Hub\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe8898-7a51-4826-8328-4db9cea25886",
   "metadata": {},
   "source": [
    "Part of the output included the URL,\n",
    "\n",
    "https://huggingface.co/thebballdave025/dwb-flan-t5-small-lora-finetune/commit/c87d34b398f3801ceb1e18c819a7c8fc894989c7\n",
    "\n",
    "Hooray! The repo name I used in constructing the trainer worked!\n",
    "\n",
    "I can get to the general repo with the URL,\n",
    "\n",
    "https://huggingface.co/thebballdave025/dwb-flan-t5-small-lora-finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511b4c50-2882-4fdd-afa5-3daefbb78cf3",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "## Info on the Fine-Tuned Model from the Repo's README - Model Card(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1121b2f-91df-4912-b42f-a3e6803180a9",
   "metadata": {},
   "source": [
    "### [thebballdave025/dwb-flan-t5-small-lora-finetune](https://huggingface.co/thebballdave025/dwb-flan-t5-small-lora-finetune)\n",
    "\n",
    "\\[archived\\] The archiving attempt at archive.org (Wayback Machine) failed.\n",
    "I'm not sure why, as the model is set as public.\n",
    "\n",
    "`PEFT  TensorBoard  Safetensors       generator  trl  sft  generated_from_trainer       License: apache-2.0`\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<b>@todo</b> : [Edit Model Card](https://huggingface.co/thebballdave025/dwb-flan-t5-small-lora-finetune/edit/main/README.md)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;\n",
    "Unable to determine this modelâ€™s pipeline type. Check the docs \n",
    "[(i)](https://huggingface.co/docs/hub/models-widgets#enabling-a-widget).\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Adapter for\n",
    "[google/flan-t5-small](https://huggingface.co/google/flan-t5-small)\n",
    "\n",
    "#### dwb-flan-t5-small-lora-finetune\n",
    "\n",
    "This model is a fine-tuned version of \n",
    "[google/flan-t5-small](https://huggingface.co/google/flan-t5-small) on the \n",
    "generator dataset \\[DWB note: I don't know why it says \"generator dataset\".\n",
    "I used the samsum dataset, which I will link here and on the\n",
    "model card, eventually\\]. \n",
    "\n",
    "It achieves the following results on the evaluation set:\n",
    "\n",
    "- Loss: 0.0226\n",
    "- <i>DWB Note: I don't know which metric was used to calculate loss. If this were more important, I'd dig through code to find out and evaluate with the same metric. If I'm really lucky, they somehow used the ROUGE scores in the loss function, so we match.</i>\n",
    "\n",
    "#### Model description\n",
    "\n",
    "More information needed\n",
    "\n",
    "#### Intended uses & limitations\n",
    "\n",
    "More information needed\n",
    "\n",
    "#### Training and evaluation data\n",
    "\n",
    "More information needed\n",
    "\n",
    "#### Training procedure\n",
    "\n",
    "#### Training hyperparameters\n",
    "\n",
    "The following hyperparameters were used during training:\n",
    "\n",
    "- learning_rate: 0.0002\n",
    "- train_batch_size: 4\n",
    "- eval_batch_size: 4\n",
    "- seed: 42\n",
    "- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n",
    "- lr_scheduler_type: linear\n",
    "- num_epochs: 1\n",
    "\n",
    "#### Training results\n",
    "\n",
    "```\n",
    "\n",
    "  Training Loss | Epoch | Step | Validation Loss\n",
    " ---------------+-------+------+-----------------\n",
    "      0.0685    |  1.0  | 1536 |     0.0226\n",
    "```\n",
    "\n",
    "#### Framework versions\n",
    "\n",
    "- PEFT 0.11.2.dev0\n",
    "- Transformers 4.41.1\n",
    "- Pytorch 2.3.0+cpu\n",
    "- Datasets 2.19.1\n",
    "- Tokenizers 0.19.1\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b44f56-58df-4abb-b279-72d469d75b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a022c0-760d-4fcb-bb71-a05e322598cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207707c8-1920-446a-bfa9-9c8ee9480a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73a6d8-1a9c-4f37-85c8-f97a9664804e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55b4db3b-4516-4900-b640-ac498f0308bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88281b15-7070-44dd-84e4-aedefa00293d",
   "metadata": {},
   "source": [
    "## Evaluation on the Test Set and Comparison to Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d123c-412a-4195-8820-838590b814a2",
   "metadata": {},
   "source": [
    "#### Verbosity stuff - get rid of the nice advice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a7666e-206c-418a-aec8-27b0fd4de3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!powershell -c (Get-Date -UFormat \\\"%s_%Y-%m-%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f710cdea-0123-4f96-8f29-e2ea8144e318",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde59af6-c15a-4494-b49e-87019a70715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bballdave025@MYMACHINE /cygdrive/c/Users/bballdave025/.conda/envs/rwkv-lora-pat/Lib/site-packages/peft/utils\n",
    "# $ date +'%s_%Y-%m-%dT%H%M%S%z'\n",
    "# 1717049876_2024-05-30T001756-0600\n",
    "\n",
    "log_verbosity_is_critical = \\\n",
    "  logging.get_verbosity() == logging.CRITICAL # alias FATAL, 50\n",
    "log_verbosity_is_error = \\\n",
    "  logging.get_verbosity() == logging.ERROR # 40\n",
    "log_verbosity_is_warn = \\\n",
    "  logging.get_verbosity() == logging.WARNING # alias WARN, 30\n",
    "log_verbosity_is_info = \\\n",
    "  logging.get_verbosity() == logging.INFO # 20\n",
    "log_verbosity_is_debug = \\\n",
    "  logging.get_verbosity() == logging.DEBUG # 10\n",
    "\n",
    "print( \"The statement, 'logging verbosity is CRITICAL' \" + \\\n",
    "      f\"is {log_verbosity_is_critical}\")\n",
    "print( \"The statement, 'logging verbosity is    ERROR' \" + \\\n",
    "      f\"is {log_verbosity_is_error}\")\n",
    "print( \"The statement, 'logging verbosity is  WARNING' \" + \\\n",
    "      f\"is {log_verbosity_is_warn}\")\n",
    "print( \"The statement, 'logging verbosity is     INFO' \" + \\\n",
    "      f\"is {log_verbosity_is_info}\")\n",
    "print( \"The statement, 'logging verbosity is    DEBUG' \" + \\\n",
    "      f\"is {log_verbosity_is_debug}\")\n",
    "\n",
    "print()\n",
    "\n",
    "init_log_verbosity = logging.get_verbosity()\n",
    "print(f\"The value of logging.get_verbosity() is: {init_log_verbosity}\")\n",
    "\n",
    "print()\n",
    "\n",
    "init_t_n_a_w = os.environ.get('TRANSFORMERS_NO_ADVISORY_WARNINGS')\n",
    "print(f\"TRANSFORMERS_NO_ADIVSORY_WARNINGS: {init_t_n_a_w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf8db54-8003-429f-8abd-cfb09a1af80b",
   "metadata": {},
   "source": [
    "### Here's the actual evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52d3eeb-e238-43bf-ada2-3d7ccaf10b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!powershell -c (Get-Date -UFormat \\\"%s_%Y-%m-%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d0f187-2402-44a4-8322-f899df42fab4",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`timestamp`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a522d7-6285-4b9d-b619-ea0f454f2cae",
   "metadata": {},
   "source": [
    "<b>!!! NOTE !!!</b> I'm going to use `tat` (with an underscore\n",
    "or undescores before, after, or surrounding the variable names)\n",
    "to indicate 'testing-after-training'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ad3bd-0921-4753-8c6c-175bffc23b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  I'm going to use 'tat' for testing-after-training\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "summarizer = pipeline('summarization', model=model, tokenizer=tokenizer)\n",
    "\n",
    "prediction_tat_list = []\n",
    "reference_tat_list = []\n",
    "\n",
    "tic = timeit.default_timer()\n",
    "\n",
    "for sample_num in range(len(dataset['test'])):\n",
    "  this_sample = dataset['test'][sample_num]\n",
    "  \n",
    "  #print(f\"dialogue: \\n{this_sample['dialogue']}\\n---------------\")\n",
    "\n",
    "  grnd_tat_summary = this_sample['summary']\n",
    "  res_tat = summarizer(this_sample['dialogue'])\n",
    "  res_tat_summary = res_tat[0]['summary_text']\n",
    "  \n",
    "  #print(f\"human-genratd summary:\\n{grnd_tat_summary}\")\n",
    "  #print(f\"flan-t5-small summary:\\n{res_tat_summary}\")\n",
    "  \n",
    "  reference_tat_list.append(grnd_tat_summary)\n",
    "  prediction_tat_list.append(res_tat_summary)\n",
    "##endof:  for sample_num in range(len(dataset['test']))\n",
    "\n",
    "toc = timeit.default_timer()\n",
    "\n",
    "print( \"Getting things ready for scoring (after training)\")\n",
    "print(f\"took {toc - tic:0.4f} seconds.\")\n",
    "\n",
    "print(\"\\n\\n---------- ROUGE SCORES ----------\")\n",
    "\n",
    "rouge = load_metric('rouge', trust_remote_code=True)\n",
    "  #  Set trust_remote_code=False to see the warning,\n",
    "  #+ deprecation, and what to change to.\n",
    "\n",
    "results_tat = rouge.compute(\n",
    "                  predictions=prediction_tat_list,\n",
    "                  references=reference_tat_list,\n",
    "                  use_aggregator=True\n",
    ")\n",
    "\n",
    "# >>> print(list(results_tat.keys()))\n",
    "# ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "\n",
    "print()\n",
    "print(\"ROUGE-1 results\")\n",
    "pprint.pp(results_tat['rouge1'])\n",
    "print()\n",
    "print(\"ROUGE-2 results\")\n",
    "pprint.pp(results_tat['rouge2'])\n",
    "print()\n",
    "print(\"ROUGE-L results\")\n",
    "pprint.pp(results_tat['rougeL'])\n",
    "print()\n",
    "print(\"ROUGE-Lsum results\")\n",
    "pprint.pp(results_tat['rougeLsum'])\n",
    "\n",
    "logging.set_verbosity(init_log_verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61791378-d26a-4399-a2eb-dba17715791f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff65b106-c9d2-4936-9d5a-fa1d5fa1d10d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b8950-a5ba-4270-9053-5b154d1529bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5526ab-3d35-4b79-a18a-0cca17e5f7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22be8033-0a75-4941-aa99-dddc3f972b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66346d54-06bb-47da-8d54-a9dee43b408a",
   "metadata": {},
   "source": [
    "## Notes Looking Forward to LoRA on RWKV\n",
    "\n",
    "Hugging Face Community, seems to have a good portion of their models\n",
    "\n",
    "https://huggingface.co/RWKV\n",
    "\n",
    "https://web.archive.org/web/20240530232509/https://huggingface.co/RWKV\n",
    "\n",
    "<br/>\n",
    "\n",
    "GitHub has even more versions/models, including the `v4-neo` that\n",
    "I think will be important (the LoRA project)\n",
    "\n",
    "https://github.com/BlinkDL/RWKV-LM/tree/main\n",
    "\n",
    "https://web.archive.org/web/20240530232637/https://github.com/BlinkDL/RWKV-LM/tree/main\n",
    "\n",
    "<br/>\n",
    "\n",
    "The main RWKV website (?!)\n",
    "\n",
    "https://www.rwkv.com/\n",
    "\n",
    "https://web.archive.org/web/20240529120904/https://www.rwkv.com/\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "GOOD STUFF. A project doing LoRA with RWKV\n",
    "\n",
    "https://github.com/Blealtan/RWKV-LM-LoRA/\n",
    "\n",
    "https://web.archive.org/web/20240530232823/https://github.com/Blealtan/RWKV-LM-LoRA\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "The official blog, I guess, with some good coding examples\n",
    "\n",
    "https://huggingface.co/blog/rwkv\n",
    "\n",
    "https://web.archive.org/web/20240530233025/https://huggingface.co/blog/rwkv\n",
    "\n",
    "It includes something that's similar to what I'm doing here in the\n",
    "`First_Full_LoRA_Trial_with_Transformer_Again.ipynb` tutorial, etc.\n",
    "\n",
    "```\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"RWKV/rwkv-raven-1b5\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "```\n",
    "\n",
    "The `AutoModelForCausalLM` is the same as the tutorial I'm following,\n",
    "but I don't know what the `.to(0)` is for.\n",
    "\n",
    "Really quickly, also looking at\n",
    "\n",
    "https://huggingface.co/RWKV/rwkv-4-world-7b\n",
    "\n",
    "https://web.archive.org/web/20240530234438/https://huggingface.co/RWKV/rwkv-4-world-7b\n",
    "\n",
    "I see an example for CPU.\n",
    "\n",
    "```\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "              \"RWKV/rwkv-4-world-7b\",\n",
    "              trust_remote_code=True\n",
    ").to(torch.float32)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "              \"RWKV/rwkv-4-world-7b\",\n",
    "              trust_remote_code=True)\n",
    "```\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "(Old version? Unofficial, it seems)\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/model_doc/rwkv\n",
    "\n",
    "https://web.archive.org/web/20240530232341/https://huggingface.co/docs/transformers/en/model_doc/rwkv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e92f0e-4c74-44b5-bec1-f0161ee84266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
