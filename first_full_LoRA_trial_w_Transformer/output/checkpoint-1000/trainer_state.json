{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9532888465204957,
  "eval_steps": 250,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0009532888465204957,
      "grad_norm": 0.307153582572937,
      "learning_rate": 0.00019980934223069591,
      "loss": 2.7623,
      "step": 1
    },
    {
      "epoch": 0.004766444232602479,
      "grad_norm": 0.36438095569610596,
      "learning_rate": 0.0001990467111534795,
      "loss": 2.7967,
      "step": 5
    },
    {
      "epoch": 0.009532888465204958,
      "grad_norm": 0.5419382452964783,
      "learning_rate": 0.00019809342230695903,
      "loss": 2.6901,
      "step": 10
    },
    {
      "epoch": 0.014299332697807437,
      "grad_norm": 0.6505125761032104,
      "learning_rate": 0.0001971401334604385,
      "loss": 2.3337,
      "step": 15
    },
    {
      "epoch": 0.019065776930409915,
      "grad_norm": 0.6815025806427002,
      "learning_rate": 0.00019618684461391803,
      "loss": 2.1854,
      "step": 20
    },
    {
      "epoch": 0.023832221163012392,
      "grad_norm": 0.47881466150283813,
      "learning_rate": 0.00019523355576739752,
      "loss": 1.9753,
      "step": 25
    },
    {
      "epoch": 0.028598665395614873,
      "grad_norm": 0.5425577759742737,
      "learning_rate": 0.00019428026692087705,
      "loss": 1.7498,
      "step": 30
    },
    {
      "epoch": 0.03336510962821735,
      "grad_norm": 0.44173935055732727,
      "learning_rate": 0.00019332697807435654,
      "loss": 1.4839,
      "step": 35
    },
    {
      "epoch": 0.03813155386081983,
      "grad_norm": 0.43801501393318176,
      "learning_rate": 0.00019237368922783604,
      "loss": 1.3203,
      "step": 40
    },
    {
      "epoch": 0.042897998093422304,
      "grad_norm": 0.36845311522483826,
      "learning_rate": 0.00019142040038131554,
      "loss": 1.0272,
      "step": 45
    },
    {
      "epoch": 0.047664442326024785,
      "grad_norm": 0.38564759492874146,
      "learning_rate": 0.00019046711153479506,
      "loss": 0.9353,
      "step": 50
    },
    {
      "epoch": 0.052430886558627265,
      "grad_norm": 0.28672581911087036,
      "learning_rate": 0.00018951382268827456,
      "loss": 0.7365,
      "step": 55
    },
    {
      "epoch": 0.057197330791229746,
      "grad_norm": 0.3096940815448761,
      "learning_rate": 0.00018856053384175406,
      "loss": 0.6141,
      "step": 60
    },
    {
      "epoch": 0.06196377502383222,
      "grad_norm": 0.2108839601278305,
      "learning_rate": 0.00018760724499523355,
      "loss": 0.5234,
      "step": 65
    },
    {
      "epoch": 0.0667302192564347,
      "grad_norm": 0.17616945505142212,
      "learning_rate": 0.00018665395614871305,
      "loss": 0.4497,
      "step": 70
    },
    {
      "epoch": 0.07149666348903717,
      "grad_norm": 0.17396551370620728,
      "learning_rate": 0.00018570066730219258,
      "loss": 0.4387,
      "step": 75
    },
    {
      "epoch": 0.07626310772163966,
      "grad_norm": 0.16254766285419464,
      "learning_rate": 0.00018474737845567207,
      "loss": 0.3736,
      "step": 80
    },
    {
      "epoch": 0.08102955195424213,
      "grad_norm": 0.13059984147548676,
      "learning_rate": 0.0001837940896091516,
      "loss": 0.3412,
      "step": 85
    },
    {
      "epoch": 0.08579599618684461,
      "grad_norm": 0.1389453411102295,
      "learning_rate": 0.0001828408007626311,
      "loss": 0.3306,
      "step": 90
    },
    {
      "epoch": 0.0905624404194471,
      "grad_norm": 0.12244414538145065,
      "learning_rate": 0.0001818875119161106,
      "loss": 0.3064,
      "step": 95
    },
    {
      "epoch": 0.09532888465204957,
      "grad_norm": 0.12522269785404205,
      "learning_rate": 0.0001809342230695901,
      "loss": 0.3067,
      "step": 100
    },
    {
      "epoch": 0.10009532888465204,
      "grad_norm": 0.11474015563726425,
      "learning_rate": 0.0001799809342230696,
      "loss": 0.2625,
      "step": 105
    },
    {
      "epoch": 0.10486177311725453,
      "grad_norm": 0.13086295127868652,
      "learning_rate": 0.0001790276453765491,
      "loss": 0.271,
      "step": 110
    },
    {
      "epoch": 0.109628217349857,
      "grad_norm": 0.11956343054771423,
      "learning_rate": 0.0001780743565300286,
      "loss": 0.2525,
      "step": 115
    },
    {
      "epoch": 0.11439466158245949,
      "grad_norm": 0.13015320897102356,
      "learning_rate": 0.0001771210676835081,
      "loss": 0.2535,
      "step": 120
    },
    {
      "epoch": 0.11916110581506197,
      "grad_norm": 0.1198173314332962,
      "learning_rate": 0.0001761677788369876,
      "loss": 0.2615,
      "step": 125
    },
    {
      "epoch": 0.12392755004766444,
      "grad_norm": 0.10572207719087601,
      "learning_rate": 0.00017521448999046713,
      "loss": 0.2457,
      "step": 130
    },
    {
      "epoch": 0.12869399428026693,
      "grad_norm": 0.08601856231689453,
      "learning_rate": 0.00017426120114394662,
      "loss": 0.2422,
      "step": 135
    },
    {
      "epoch": 0.1334604385128694,
      "grad_norm": 0.10068266838788986,
      "learning_rate": 0.00017330791229742615,
      "loss": 0.2333,
      "step": 140
    },
    {
      "epoch": 0.13822688274547187,
      "grad_norm": 0.09995309263467789,
      "learning_rate": 0.00017235462345090562,
      "loss": 0.2359,
      "step": 145
    },
    {
      "epoch": 0.14299332697807435,
      "grad_norm": 0.10637041181325912,
      "learning_rate": 0.00017140133460438514,
      "loss": 0.2292,
      "step": 150
    },
    {
      "epoch": 0.14775977121067682,
      "grad_norm": 0.1177048608660698,
      "learning_rate": 0.00017044804575786464,
      "loss": 0.2133,
      "step": 155
    },
    {
      "epoch": 0.15252621544327932,
      "grad_norm": 0.09634800255298615,
      "learning_rate": 0.00016949475691134416,
      "loss": 0.2143,
      "step": 160
    },
    {
      "epoch": 0.1572926596758818,
      "grad_norm": 0.09355175495147705,
      "learning_rate": 0.00016854146806482366,
      "loss": 0.2139,
      "step": 165
    },
    {
      "epoch": 0.16205910390848427,
      "grad_norm": 0.42484909296035767,
      "learning_rate": 0.00016758817921830313,
      "loss": 0.2069,
      "step": 170
    },
    {
      "epoch": 0.16682554814108674,
      "grad_norm": 0.1165904626250267,
      "learning_rate": 0.00016663489037178265,
      "loss": 0.1984,
      "step": 175
    },
    {
      "epoch": 0.17159199237368922,
      "grad_norm": 0.09310045093297958,
      "learning_rate": 0.00016568160152526215,
      "loss": 0.1941,
      "step": 180
    },
    {
      "epoch": 0.17635843660629172,
      "grad_norm": 0.12128669023513794,
      "learning_rate": 0.00016472831267874168,
      "loss": 0.188,
      "step": 185
    },
    {
      "epoch": 0.1811248808388942,
      "grad_norm": 0.0815657377243042,
      "learning_rate": 0.00016377502383222117,
      "loss": 0.184,
      "step": 190
    },
    {
      "epoch": 0.18589132507149667,
      "grad_norm": 0.09277266263961792,
      "learning_rate": 0.00016282173498570067,
      "loss": 0.1871,
      "step": 195
    },
    {
      "epoch": 0.19065776930409914,
      "grad_norm": 0.10186007618904114,
      "learning_rate": 0.00016186844613918017,
      "loss": 0.1786,
      "step": 200
    },
    {
      "epoch": 0.1954242135367016,
      "grad_norm": 0.08006875216960907,
      "learning_rate": 0.0001609151572926597,
      "loss": 0.1758,
      "step": 205
    },
    {
      "epoch": 0.2001906577693041,
      "grad_norm": 0.09366367757320404,
      "learning_rate": 0.0001599618684461392,
      "loss": 0.1788,
      "step": 210
    },
    {
      "epoch": 0.2049571020019066,
      "grad_norm": 0.08705569058656693,
      "learning_rate": 0.0001590085795996187,
      "loss": 0.1705,
      "step": 215
    },
    {
      "epoch": 0.20972354623450906,
      "grad_norm": 0.11188893765211105,
      "learning_rate": 0.00015805529075309818,
      "loss": 0.1718,
      "step": 220
    },
    {
      "epoch": 0.21448999046711154,
      "grad_norm": 0.08841504156589508,
      "learning_rate": 0.0001571020019065777,
      "loss": 0.1677,
      "step": 225
    },
    {
      "epoch": 0.219256434699714,
      "grad_norm": 0.1058618426322937,
      "learning_rate": 0.0001561487130600572,
      "loss": 0.1679,
      "step": 230
    },
    {
      "epoch": 0.22402287893231648,
      "grad_norm": 0.09331177175045013,
      "learning_rate": 0.0001551954242135367,
      "loss": 0.1613,
      "step": 235
    },
    {
      "epoch": 0.22878932316491898,
      "grad_norm": 0.09750111401081085,
      "learning_rate": 0.00015424213536701623,
      "loss": 0.1602,
      "step": 240
    },
    {
      "epoch": 0.23355576739752146,
      "grad_norm": 0.10616492480039597,
      "learning_rate": 0.00015328884652049572,
      "loss": 0.1758,
      "step": 245
    },
    {
      "epoch": 0.23832221163012393,
      "grad_norm": 0.09010384231805801,
      "learning_rate": 0.00015233555767397522,
      "loss": 0.1609,
      "step": 250
    },
    {
      "epoch": 0.23832221163012393,
      "eval_loss": 0.057427018880844116,
      "eval_runtime": 301.1636,
      "eval_samples_per_second": 0.757,
      "eval_steps_per_second": 0.189,
      "step": 250
    },
    {
      "epoch": 0.2430886558627264,
      "grad_norm": 0.09244062006473541,
      "learning_rate": 0.00015138226882745472,
      "loss": 0.1473,
      "step": 255
    },
    {
      "epoch": 0.24785510009532888,
      "grad_norm": 0.09245975315570831,
      "learning_rate": 0.00015042897998093424,
      "loss": 0.1557,
      "step": 260
    },
    {
      "epoch": 0.2526215443279314,
      "grad_norm": 0.08932986855506897,
      "learning_rate": 0.00014947569113441374,
      "loss": 0.1563,
      "step": 265
    },
    {
      "epoch": 0.25738798856053385,
      "grad_norm": 0.09238795936107635,
      "learning_rate": 0.00014852240228789324,
      "loss": 0.1448,
      "step": 270
    },
    {
      "epoch": 0.2621544327931363,
      "grad_norm": 0.08765724301338196,
      "learning_rate": 0.00014756911344137273,
      "loss": 0.1513,
      "step": 275
    },
    {
      "epoch": 0.2669208770257388,
      "grad_norm": 0.08241435140371323,
      "learning_rate": 0.00014661582459485226,
      "loss": 0.1464,
      "step": 280
    },
    {
      "epoch": 0.2716873212583413,
      "grad_norm": 0.0862584114074707,
      "learning_rate": 0.00014566253574833175,
      "loss": 0.1317,
      "step": 285
    },
    {
      "epoch": 0.27645376549094375,
      "grad_norm": 0.07371104508638382,
      "learning_rate": 0.00014470924690181125,
      "loss": 0.1259,
      "step": 290
    },
    {
      "epoch": 0.2812202097235462,
      "grad_norm": 0.10184313356876373,
      "learning_rate": 0.00014375595805529078,
      "loss": 0.1326,
      "step": 295
    },
    {
      "epoch": 0.2859866539561487,
      "grad_norm": 0.11007644981145859,
      "learning_rate": 0.00014280266920877025,
      "loss": 0.151,
      "step": 300
    },
    {
      "epoch": 0.29075309818875117,
      "grad_norm": 0.09713533520698547,
      "learning_rate": 0.00014184938036224977,
      "loss": 0.1322,
      "step": 305
    },
    {
      "epoch": 0.29551954242135364,
      "grad_norm": 0.08833512663841248,
      "learning_rate": 0.00014089609151572927,
      "loss": 0.1347,
      "step": 310
    },
    {
      "epoch": 0.30028598665395617,
      "grad_norm": 0.09118528664112091,
      "learning_rate": 0.0001399428026692088,
      "loss": 0.1452,
      "step": 315
    },
    {
      "epoch": 0.30505243088655865,
      "grad_norm": 0.08868779242038727,
      "learning_rate": 0.0001389895138226883,
      "loss": 0.1416,
      "step": 320
    },
    {
      "epoch": 0.3098188751191611,
      "grad_norm": 0.09563098102807999,
      "learning_rate": 0.00013803622497616779,
      "loss": 0.1351,
      "step": 325
    },
    {
      "epoch": 0.3145853193517636,
      "grad_norm": 0.09023477137088776,
      "learning_rate": 0.00013708293612964728,
      "loss": 0.1423,
      "step": 330
    },
    {
      "epoch": 0.31935176358436607,
      "grad_norm": 0.09140109270811081,
      "learning_rate": 0.0001361296472831268,
      "loss": 0.1302,
      "step": 335
    },
    {
      "epoch": 0.32411820781696854,
      "grad_norm": 0.10137658566236496,
      "learning_rate": 0.0001351763584366063,
      "loss": 0.1344,
      "step": 340
    },
    {
      "epoch": 0.328884652049571,
      "grad_norm": 0.11035141348838806,
      "learning_rate": 0.0001342230695900858,
      "loss": 0.1319,
      "step": 345
    },
    {
      "epoch": 0.3336510962821735,
      "grad_norm": 0.06996501982212067,
      "learning_rate": 0.0001332697807435653,
      "loss": 0.1246,
      "step": 350
    },
    {
      "epoch": 0.33841754051477596,
      "grad_norm": 0.09154362231492996,
      "learning_rate": 0.0001323164918970448,
      "loss": 0.1381,
      "step": 355
    },
    {
      "epoch": 0.34318398474737843,
      "grad_norm": 0.07508058845996857,
      "learning_rate": 0.00013136320305052432,
      "loss": 0.1311,
      "step": 360
    },
    {
      "epoch": 0.3479504289799809,
      "grad_norm": 0.08949723094701767,
      "learning_rate": 0.00013040991420400382,
      "loss": 0.125,
      "step": 365
    },
    {
      "epoch": 0.35271687321258344,
      "grad_norm": 0.08451508730649948,
      "learning_rate": 0.00012945662535748334,
      "loss": 0.126,
      "step": 370
    },
    {
      "epoch": 0.3574833174451859,
      "grad_norm": 0.09587414562702179,
      "learning_rate": 0.0001285033365109628,
      "loss": 0.119,
      "step": 375
    },
    {
      "epoch": 0.3622497616777884,
      "grad_norm": 0.08391518145799637,
      "learning_rate": 0.00012755004766444234,
      "loss": 0.118,
      "step": 380
    },
    {
      "epoch": 0.36701620591039086,
      "grad_norm": 0.07747770845890045,
      "learning_rate": 0.00012659675881792183,
      "loss": 0.1209,
      "step": 385
    },
    {
      "epoch": 0.37178265014299333,
      "grad_norm": 0.0934198647737503,
      "learning_rate": 0.00012564346997140136,
      "loss": 0.1172,
      "step": 390
    },
    {
      "epoch": 0.3765490943755958,
      "grad_norm": 0.06033097952604294,
      "learning_rate": 0.00012469018112488085,
      "loss": 0.109,
      "step": 395
    },
    {
      "epoch": 0.3813155386081983,
      "grad_norm": 0.07851578295230865,
      "learning_rate": 0.00012373689227836035,
      "loss": 0.1137,
      "step": 400
    },
    {
      "epoch": 0.38608198284080075,
      "grad_norm": 0.07289399951696396,
      "learning_rate": 0.00012278360343183985,
      "loss": 0.1163,
      "step": 405
    },
    {
      "epoch": 0.3908484270734032,
      "grad_norm": 0.0792764350771904,
      "learning_rate": 0.00012183031458531935,
      "loss": 0.1232,
      "step": 410
    },
    {
      "epoch": 0.3956148713060057,
      "grad_norm": 0.07738808542490005,
      "learning_rate": 0.00012087702573879887,
      "loss": 0.119,
      "step": 415
    },
    {
      "epoch": 0.4003813155386082,
      "grad_norm": 0.08896267414093018,
      "learning_rate": 0.00011992373689227835,
      "loss": 0.1071,
      "step": 420
    },
    {
      "epoch": 0.4051477597712107,
      "grad_norm": 0.08688221126794815,
      "learning_rate": 0.00011897044804575788,
      "loss": 0.1154,
      "step": 425
    },
    {
      "epoch": 0.4099142040038132,
      "grad_norm": 0.08612383902072906,
      "learning_rate": 0.00011801715919923738,
      "loss": 0.1007,
      "step": 430
    },
    {
      "epoch": 0.41468064823641565,
      "grad_norm": 0.1371576189994812,
      "learning_rate": 0.00011706387035271689,
      "loss": 0.1199,
      "step": 435
    },
    {
      "epoch": 0.4194470924690181,
      "grad_norm": 0.09108061343431473,
      "learning_rate": 0.00011611058150619638,
      "loss": 0.1073,
      "step": 440
    },
    {
      "epoch": 0.4242135367016206,
      "grad_norm": 0.08890456706285477,
      "learning_rate": 0.0001151572926596759,
      "loss": 0.1124,
      "step": 445
    },
    {
      "epoch": 0.42897998093422307,
      "grad_norm": 0.09171269834041595,
      "learning_rate": 0.00011420400381315539,
      "loss": 0.1127,
      "step": 450
    },
    {
      "epoch": 0.43374642516682554,
      "grad_norm": 0.099912129342556,
      "learning_rate": 0.00011325071496663489,
      "loss": 0.1153,
      "step": 455
    },
    {
      "epoch": 0.438512869399428,
      "grad_norm": 0.06703298538923264,
      "learning_rate": 0.0001122974261201144,
      "loss": 0.1111,
      "step": 460
    },
    {
      "epoch": 0.4432793136320305,
      "grad_norm": 0.08093548566102982,
      "learning_rate": 0.0001113441372735939,
      "loss": 0.114,
      "step": 465
    },
    {
      "epoch": 0.44804575786463297,
      "grad_norm": 0.07145562022924423,
      "learning_rate": 0.00011039084842707341,
      "loss": 0.1102,
      "step": 470
    },
    {
      "epoch": 0.45281220209723544,
      "grad_norm": 0.06859105080366135,
      "learning_rate": 0.0001094375595805529,
      "loss": 0.1021,
      "step": 475
    },
    {
      "epoch": 0.45757864632983797,
      "grad_norm": 0.07280439883470535,
      "learning_rate": 0.00010848427073403243,
      "loss": 0.105,
      "step": 480
    },
    {
      "epoch": 0.46234509056244044,
      "grad_norm": 0.07414188235998154,
      "learning_rate": 0.00010753098188751191,
      "loss": 0.1079,
      "step": 485
    },
    {
      "epoch": 0.4671115347950429,
      "grad_norm": 0.07863271981477737,
      "learning_rate": 0.00010657769304099144,
      "loss": 0.1048,
      "step": 490
    },
    {
      "epoch": 0.4718779790276454,
      "grad_norm": 0.07220739871263504,
      "learning_rate": 0.00010562440419447093,
      "loss": 0.1048,
      "step": 495
    },
    {
      "epoch": 0.47664442326024786,
      "grad_norm": 0.07650351524353027,
      "learning_rate": 0.00010467111534795044,
      "loss": 0.0954,
      "step": 500
    },
    {
      "epoch": 0.47664442326024786,
      "eval_loss": 0.03319983929395676,
      "eval_runtime": 301.4162,
      "eval_samples_per_second": 0.756,
      "eval_steps_per_second": 0.189,
      "step": 500
    },
    {
      "epoch": 0.48141086749285034,
      "grad_norm": 0.08413530886173248,
      "learning_rate": 0.00010371782650142994,
      "loss": 0.1129,
      "step": 505
    },
    {
      "epoch": 0.4861773117254528,
      "grad_norm": 0.10630720108747482,
      "learning_rate": 0.00010276453765490944,
      "loss": 0.0969,
      "step": 510
    },
    {
      "epoch": 0.4909437559580553,
      "grad_norm": 0.09628598392009735,
      "learning_rate": 0.00010181124880838895,
      "loss": 0.104,
      "step": 515
    },
    {
      "epoch": 0.49571020019065776,
      "grad_norm": 0.07287771254777908,
      "learning_rate": 0.00010085795996186845,
      "loss": 0.1011,
      "step": 520
    },
    {
      "epoch": 0.5004766444232602,
      "grad_norm": 0.09723581373691559,
      "learning_rate": 9.990467111534796e-05,
      "loss": 0.1164,
      "step": 525
    },
    {
      "epoch": 0.5052430886558628,
      "grad_norm": 0.07850062102079391,
      "learning_rate": 9.895138226882747e-05,
      "loss": 0.1018,
      "step": 530
    },
    {
      "epoch": 0.5100095328884652,
      "grad_norm": 0.0767829567193985,
      "learning_rate": 9.799809342230697e-05,
      "loss": 0.0941,
      "step": 535
    },
    {
      "epoch": 0.5147759771210677,
      "grad_norm": 0.08408347517251968,
      "learning_rate": 9.704480457578646e-05,
      "loss": 0.0963,
      "step": 540
    },
    {
      "epoch": 0.5195424213536701,
      "grad_norm": 0.08184600621461868,
      "learning_rate": 9.609151572926597e-05,
      "loss": 0.1005,
      "step": 545
    },
    {
      "epoch": 0.5243088655862727,
      "grad_norm": 0.0835418775677681,
      "learning_rate": 9.513822688274547e-05,
      "loss": 0.1015,
      "step": 550
    },
    {
      "epoch": 0.5290753098188751,
      "grad_norm": 0.08370465040206909,
      "learning_rate": 9.418493803622498e-05,
      "loss": 0.1015,
      "step": 555
    },
    {
      "epoch": 0.5338417540514776,
      "grad_norm": 0.08304113149642944,
      "learning_rate": 9.323164918970448e-05,
      "loss": 0.1088,
      "step": 560
    },
    {
      "epoch": 0.5386081982840801,
      "grad_norm": 0.056209322065114975,
      "learning_rate": 9.227836034318399e-05,
      "loss": 0.0929,
      "step": 565
    },
    {
      "epoch": 0.5433746425166825,
      "grad_norm": 0.06184670701622963,
      "learning_rate": 9.13250714966635e-05,
      "loss": 0.0981,
      "step": 570
    },
    {
      "epoch": 0.5481410867492851,
      "grad_norm": 0.09814276546239853,
      "learning_rate": 9.0371782650143e-05,
      "loss": 0.0918,
      "step": 575
    },
    {
      "epoch": 0.5529075309818875,
      "grad_norm": 0.06809117645025253,
      "learning_rate": 8.941849380362251e-05,
      "loss": 0.0999,
      "step": 580
    },
    {
      "epoch": 0.55767397521449,
      "grad_norm": 0.07826395332813263,
      "learning_rate": 8.8465204957102e-05,
      "loss": 0.1023,
      "step": 585
    },
    {
      "epoch": 0.5624404194470924,
      "grad_norm": 0.060574278235435486,
      "learning_rate": 8.751191611058152e-05,
      "loss": 0.0959,
      "step": 590
    },
    {
      "epoch": 0.567206863679695,
      "grad_norm": 0.08655744045972824,
      "learning_rate": 8.655862726406101e-05,
      "loss": 0.1017,
      "step": 595
    },
    {
      "epoch": 0.5719733079122974,
      "grad_norm": 0.08474034070968628,
      "learning_rate": 8.560533841754051e-05,
      "loss": 0.0999,
      "step": 600
    },
    {
      "epoch": 0.5767397521448999,
      "grad_norm": 0.07126858085393906,
      "learning_rate": 8.465204957102002e-05,
      "loss": 0.0887,
      "step": 605
    },
    {
      "epoch": 0.5815061963775023,
      "grad_norm": 0.10694766044616699,
      "learning_rate": 8.369876072449953e-05,
      "loss": 0.0994,
      "step": 610
    },
    {
      "epoch": 0.5862726406101049,
      "grad_norm": 0.0936119481921196,
      "learning_rate": 8.274547187797903e-05,
      "loss": 0.0959,
      "step": 615
    },
    {
      "epoch": 0.5910390848427073,
      "grad_norm": 0.09766944497823715,
      "learning_rate": 8.179218303145854e-05,
      "loss": 0.1008,
      "step": 620
    },
    {
      "epoch": 0.5958055290753098,
      "grad_norm": 0.06628921627998352,
      "learning_rate": 8.083889418493804e-05,
      "loss": 0.0985,
      "step": 625
    },
    {
      "epoch": 0.6005719733079123,
      "grad_norm": 0.07903910428285599,
      "learning_rate": 7.988560533841755e-05,
      "loss": 0.0897,
      "step": 630
    },
    {
      "epoch": 0.6053384175405148,
      "grad_norm": 0.0707167536020279,
      "learning_rate": 7.893231649189706e-05,
      "loss": 0.0945,
      "step": 635
    },
    {
      "epoch": 0.6101048617731173,
      "grad_norm": 0.0843275859951973,
      "learning_rate": 7.797902764537655e-05,
      "loss": 0.0956,
      "step": 640
    },
    {
      "epoch": 0.6148713060057197,
      "grad_norm": 0.06989369541406631,
      "learning_rate": 7.702573879885607e-05,
      "loss": 0.0979,
      "step": 645
    },
    {
      "epoch": 0.6196377502383222,
      "grad_norm": 0.0871494859457016,
      "learning_rate": 7.607244995233556e-05,
      "loss": 0.0949,
      "step": 650
    },
    {
      "epoch": 0.6244041944709247,
      "grad_norm": 0.07951338589191437,
      "learning_rate": 7.511916110581506e-05,
      "loss": 0.092,
      "step": 655
    },
    {
      "epoch": 0.6291706387035272,
      "grad_norm": 0.08589603751897812,
      "learning_rate": 7.416587225929457e-05,
      "loss": 0.0957,
      "step": 660
    },
    {
      "epoch": 0.6339370829361296,
      "grad_norm": 0.07756780833005905,
      "learning_rate": 7.321258341277407e-05,
      "loss": 0.0978,
      "step": 665
    },
    {
      "epoch": 0.6387035271687321,
      "grad_norm": 0.057922087609767914,
      "learning_rate": 7.225929456625358e-05,
      "loss": 0.0887,
      "step": 670
    },
    {
      "epoch": 0.6434699714013346,
      "grad_norm": 0.0770292654633522,
      "learning_rate": 7.130600571973308e-05,
      "loss": 0.0939,
      "step": 675
    },
    {
      "epoch": 0.6482364156339371,
      "grad_norm": 0.0627807080745697,
      "learning_rate": 7.035271687321259e-05,
      "loss": 0.0875,
      "step": 680
    },
    {
      "epoch": 0.6530028598665396,
      "grad_norm": 0.058099448680877686,
      "learning_rate": 6.93994280266921e-05,
      "loss": 0.0893,
      "step": 685
    },
    {
      "epoch": 0.657769304099142,
      "grad_norm": 0.0661325678229332,
      "learning_rate": 6.84461391801716e-05,
      "loss": 0.0909,
      "step": 690
    },
    {
      "epoch": 0.6625357483317446,
      "grad_norm": 0.10172414034605026,
      "learning_rate": 6.74928503336511e-05,
      "loss": 0.091,
      "step": 695
    },
    {
      "epoch": 0.667302192564347,
      "grad_norm": 0.08219598233699799,
      "learning_rate": 6.65395614871306e-05,
      "loss": 0.0897,
      "step": 700
    },
    {
      "epoch": 0.6720686367969495,
      "grad_norm": 0.08377191424369812,
      "learning_rate": 6.55862726406101e-05,
      "loss": 0.0922,
      "step": 705
    },
    {
      "epoch": 0.6768350810295519,
      "grad_norm": 0.08934938162565231,
      "learning_rate": 6.463298379408961e-05,
      "loss": 0.0967,
      "step": 710
    },
    {
      "epoch": 0.6816015252621545,
      "grad_norm": 0.08117896318435669,
      "learning_rate": 6.367969494756911e-05,
      "loss": 0.0884,
      "step": 715
    },
    {
      "epoch": 0.6863679694947569,
      "grad_norm": 0.06442361325025558,
      "learning_rate": 6.272640610104862e-05,
      "loss": 0.087,
      "step": 720
    },
    {
      "epoch": 0.6911344137273594,
      "grad_norm": 0.06424592435359955,
      "learning_rate": 6.177311725452813e-05,
      "loss": 0.0894,
      "step": 725
    },
    {
      "epoch": 0.6959008579599618,
      "grad_norm": 0.08800038695335388,
      "learning_rate": 6.0819828408007626e-05,
      "loss": 0.0832,
      "step": 730
    },
    {
      "epoch": 0.7006673021925643,
      "grad_norm": 0.08648484945297241,
      "learning_rate": 5.9866539561487136e-05,
      "loss": 0.0992,
      "step": 735
    },
    {
      "epoch": 0.7054337464251669,
      "grad_norm": 0.06415580958127975,
      "learning_rate": 5.891325071496664e-05,
      "loss": 0.0867,
      "step": 740
    },
    {
      "epoch": 0.7102001906577693,
      "grad_norm": 0.07679440081119537,
      "learning_rate": 5.7959961868446144e-05,
      "loss": 0.0884,
      "step": 745
    },
    {
      "epoch": 0.7149666348903718,
      "grad_norm": 0.07506895065307617,
      "learning_rate": 5.700667302192565e-05,
      "loss": 0.084,
      "step": 750
    },
    {
      "epoch": 0.7149666348903718,
      "eval_loss": 0.027996815741062164,
      "eval_runtime": 301.4217,
      "eval_samples_per_second": 0.756,
      "eval_steps_per_second": 0.189,
      "step": 750
    },
    {
      "epoch": 0.7197330791229742,
      "grad_norm": 0.10420194268226624,
      "learning_rate": 5.605338417540515e-05,
      "loss": 0.0844,
      "step": 755
    },
    {
      "epoch": 0.7244995233555768,
      "grad_norm": 0.0650736466050148,
      "learning_rate": 5.510009532888466e-05,
      "loss": 0.0857,
      "step": 760
    },
    {
      "epoch": 0.7292659675881792,
      "grad_norm": 0.0888633280992508,
      "learning_rate": 5.414680648236415e-05,
      "loss": 0.0904,
      "step": 765
    },
    {
      "epoch": 0.7340324118207817,
      "grad_norm": 0.09198494255542755,
      "learning_rate": 5.319351763584366e-05,
      "loss": 0.0913,
      "step": 770
    },
    {
      "epoch": 0.7387988560533841,
      "grad_norm": 0.06222865357995033,
      "learning_rate": 5.224022878932317e-05,
      "loss": 0.0876,
      "step": 775
    },
    {
      "epoch": 0.7435653002859867,
      "grad_norm": 0.06799767911434174,
      "learning_rate": 5.128693994280267e-05,
      "loss": 0.0781,
      "step": 780
    },
    {
      "epoch": 0.7483317445185891,
      "grad_norm": 0.10448454320430756,
      "learning_rate": 5.0333651096282176e-05,
      "loss": 0.0827,
      "step": 785
    },
    {
      "epoch": 0.7530981887511916,
      "grad_norm": 0.09155669808387756,
      "learning_rate": 4.938036224976168e-05,
      "loss": 0.0853,
      "step": 790
    },
    {
      "epoch": 0.7578646329837941,
      "grad_norm": 0.0716058537364006,
      "learning_rate": 4.8427073403241184e-05,
      "loss": 0.0869,
      "step": 795
    },
    {
      "epoch": 0.7626310772163966,
      "grad_norm": 0.07382109016180038,
      "learning_rate": 4.747378455672069e-05,
      "loss": 0.0977,
      "step": 800
    },
    {
      "epoch": 0.7673975214489991,
      "grad_norm": 0.0775313451886177,
      "learning_rate": 4.652049571020019e-05,
      "loss": 0.091,
      "step": 805
    },
    {
      "epoch": 0.7721639656816015,
      "grad_norm": 0.07375241070985794,
      "learning_rate": 4.5567206863679695e-05,
      "loss": 0.0909,
      "step": 810
    },
    {
      "epoch": 0.776930409914204,
      "grad_norm": 0.08515474200248718,
      "learning_rate": 4.46139180171592e-05,
      "loss": 0.0846,
      "step": 815
    },
    {
      "epoch": 0.7816968541468065,
      "grad_norm": 0.06191980838775635,
      "learning_rate": 4.36606291706387e-05,
      "loss": 0.0874,
      "step": 820
    },
    {
      "epoch": 0.786463298379409,
      "grad_norm": 0.09430816769599915,
      "learning_rate": 4.2707340324118214e-05,
      "loss": 0.0835,
      "step": 825
    },
    {
      "epoch": 0.7912297426120114,
      "grad_norm": 0.08136894553899765,
      "learning_rate": 4.175405147759772e-05,
      "loss": 0.1013,
      "step": 830
    },
    {
      "epoch": 0.7959961868446139,
      "grad_norm": 0.05765584111213684,
      "learning_rate": 4.0800762631077215e-05,
      "loss": 0.0765,
      "step": 835
    },
    {
      "epoch": 0.8007626310772163,
      "grad_norm": 0.088172048330307,
      "learning_rate": 3.984747378455672e-05,
      "loss": 0.0893,
      "step": 840
    },
    {
      "epoch": 0.8055290753098189,
      "grad_norm": 0.060408372431993484,
      "learning_rate": 3.889418493803623e-05,
      "loss": 0.0842,
      "step": 845
    },
    {
      "epoch": 0.8102955195424214,
      "grad_norm": 0.07619817554950714,
      "learning_rate": 3.7940896091515734e-05,
      "loss": 0.0827,
      "step": 850
    },
    {
      "epoch": 0.8150619637750238,
      "grad_norm": 0.08119814097881317,
      "learning_rate": 3.698760724499524e-05,
      "loss": 0.084,
      "step": 855
    },
    {
      "epoch": 0.8198284080076264,
      "grad_norm": 0.0741816982626915,
      "learning_rate": 3.603431839847474e-05,
      "loss": 0.083,
      "step": 860
    },
    {
      "epoch": 0.8245948522402288,
      "grad_norm": 0.06458922475576401,
      "learning_rate": 3.508102955195424e-05,
      "loss": 0.0872,
      "step": 865
    },
    {
      "epoch": 0.8293612964728313,
      "grad_norm": 0.07800101488828659,
      "learning_rate": 3.412774070543375e-05,
      "loss": 0.0798,
      "step": 870
    },
    {
      "epoch": 0.8341277407054337,
      "grad_norm": 0.08263973146677017,
      "learning_rate": 3.317445185891325e-05,
      "loss": 0.0783,
      "step": 875
    },
    {
      "epoch": 0.8388941849380362,
      "grad_norm": 0.055068932473659515,
      "learning_rate": 3.222116301239276e-05,
      "loss": 0.083,
      "step": 880
    },
    {
      "epoch": 0.8436606291706387,
      "grad_norm": 0.10707294195890427,
      "learning_rate": 3.126787416587226e-05,
      "loss": 0.0903,
      "step": 885
    },
    {
      "epoch": 0.8484270734032412,
      "grad_norm": 0.10159040242433548,
      "learning_rate": 3.0314585319351762e-05,
      "loss": 0.0862,
      "step": 890
    },
    {
      "epoch": 0.8531935176358436,
      "grad_norm": 0.09627002477645874,
      "learning_rate": 2.936129647283127e-05,
      "loss": 0.0859,
      "step": 895
    },
    {
      "epoch": 0.8579599618684461,
      "grad_norm": 0.06380047649145126,
      "learning_rate": 2.8408007626310773e-05,
      "loss": 0.0823,
      "step": 900
    },
    {
      "epoch": 0.8627264061010487,
      "grad_norm": 0.08126873522996902,
      "learning_rate": 2.7454718779790277e-05,
      "loss": 0.085,
      "step": 905
    },
    {
      "epoch": 0.8674928503336511,
      "grad_norm": 0.06333344429731369,
      "learning_rate": 2.6501429933269784e-05,
      "loss": 0.0751,
      "step": 910
    },
    {
      "epoch": 0.8722592945662536,
      "grad_norm": 0.055938780307769775,
      "learning_rate": 2.5548141086749288e-05,
      "loss": 0.0806,
      "step": 915
    },
    {
      "epoch": 0.877025738798856,
      "grad_norm": 0.08502792567014694,
      "learning_rate": 2.4594852240228792e-05,
      "loss": 0.0866,
      "step": 920
    },
    {
      "epoch": 0.8817921830314586,
      "grad_norm": 0.07828785479068756,
      "learning_rate": 2.3641563393708293e-05,
      "loss": 0.0926,
      "step": 925
    },
    {
      "epoch": 0.886558627264061,
      "grad_norm": 0.085783950984478,
      "learning_rate": 2.26882745471878e-05,
      "loss": 0.0895,
      "step": 930
    },
    {
      "epoch": 0.8913250714966635,
      "grad_norm": 0.06811841577291489,
      "learning_rate": 2.1734985700667304e-05,
      "loss": 0.0829,
      "step": 935
    },
    {
      "epoch": 0.8960915157292659,
      "grad_norm": 0.07478311657905579,
      "learning_rate": 2.0781696854146808e-05,
      "loss": 0.0848,
      "step": 940
    },
    {
      "epoch": 0.9008579599618685,
      "grad_norm": 0.06913737952709198,
      "learning_rate": 1.9828408007626312e-05,
      "loss": 0.085,
      "step": 945
    },
    {
      "epoch": 0.9056244041944709,
      "grad_norm": 0.099596306681633,
      "learning_rate": 1.8875119161105816e-05,
      "loss": 0.0928,
      "step": 950
    },
    {
      "epoch": 0.9103908484270734,
      "grad_norm": 0.05176394060254097,
      "learning_rate": 1.792183031458532e-05,
      "loss": 0.0799,
      "step": 955
    },
    {
      "epoch": 0.9151572926596759,
      "grad_norm": 0.06541625410318375,
      "learning_rate": 1.6968541468064824e-05,
      "loss": 0.0847,
      "step": 960
    },
    {
      "epoch": 0.9199237368922784,
      "grad_norm": 0.16322921216487885,
      "learning_rate": 1.601525262154433e-05,
      "loss": 0.0842,
      "step": 965
    },
    {
      "epoch": 0.9246901811248809,
      "grad_norm": 0.06831394881010056,
      "learning_rate": 1.5061963775023832e-05,
      "loss": 0.0853,
      "step": 970
    },
    {
      "epoch": 0.9294566253574833,
      "grad_norm": 0.09201756864786148,
      "learning_rate": 1.4108674928503337e-05,
      "loss": 0.0848,
      "step": 975
    },
    {
      "epoch": 0.9342230695900858,
      "grad_norm": 0.07203473150730133,
      "learning_rate": 1.315538608198284e-05,
      "loss": 0.0942,
      "step": 980
    },
    {
      "epoch": 0.9389895138226882,
      "grad_norm": 0.06643937528133392,
      "learning_rate": 1.2202097235462345e-05,
      "loss": 0.0828,
      "step": 985
    },
    {
      "epoch": 0.9437559580552908,
      "grad_norm": 0.08530429005622864,
      "learning_rate": 1.124880838894185e-05,
      "loss": 0.0809,
      "step": 990
    },
    {
      "epoch": 0.9485224022878932,
      "grad_norm": 0.06851990520954132,
      "learning_rate": 1.0295519542421355e-05,
      "loss": 0.09,
      "step": 995
    },
    {
      "epoch": 0.9532888465204957,
      "grad_norm": 0.047927021980285645,
      "learning_rate": 9.342230695900859e-06,
      "loss": 0.0819,
      "step": 1000
    },
    {
      "epoch": 0.9532888465204957,
      "eval_loss": 0.02679048478603363,
      "eval_runtime": 301.3419,
      "eval_samples_per_second": 0.757,
      "eval_steps_per_second": 0.189,
      "step": 1000
    }
  ],
  "logging_steps": 5,
  "max_steps": 1049,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 250,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1138747392000000.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
