{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d7b071-3c52-4189-81e4-2fe25bb2f61d",
   "metadata": {},
   "source": [
    "# First Full LoRA Trial with Transformer\n",
    "\n",
    "## peft (for LoRA) and FLAN-T5-small for the LLM\n",
    "\n",
    "I'm following what seems to be a great tutorial from Mehul Gupta,\n",
    "\n",
    "> https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578\n",
    "> \n",
    "> https://web.archive.org/web/20240522140323/https://medium.com/data-science-in-your-pocket/lora-for-fine-tuning-llms-explained-with-codes-and-example-62a7ac5a3578\n",
    "\n",
    "I'm doing this to prepare creating a LoRA for RWKV ( @todo  put links in here ) so as to fine-tune it for Pat's OLECT-LM stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab0319e-1d1e-4fa3-9153-e8c26344e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  No need to run this again\n",
    "# !powershell -c (Get-Date -UFormat \\\"%s_%Y%m%dT%H%M%S%Z00\\\") -replace '[.][0-9]*_', '_'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8476aae1-0e49-4094-8eb8-6bf0d51acbc7",
   "metadata": {},
   "source": [
    "Output was:\n",
    "\n",
    "`1716367147_20240522T083907-0600`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb51da5-6c05-4870-931c-7068e575ac99",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7c2afe-9b5f-4ee6-9db6-9f8d2cf6b1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "import torch\n",
    "from transformers import AutoTokenizer, \\\n",
    "                         AutoModelForSeq2SeqLM, \\\n",
    "                         TrainingArguments, \\\n",
    "                         pipeline\n",
    "from peft import LoraConfig, \\\n",
    "                 prepare_model_for_kbit_training, \\\n",
    "                 get_peft_model, \\\n",
    "                 AutoPeftModelForCausalLM\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import login, notebook_login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bfe2f4-b1b4-4158-b3c6-048c487514d2",
   "metadata": {},
   "source": [
    "## Load the training and test dataset along with the LLM with its tokenizer\n",
    "\n",
    "The LLM will be fine-tuned. It seems the tokenizer will also be fine-tuned, \n",
    "but I'm not sure \n",
    "\n",
    "<b>Why aren't we loading the validation set?</b> (I don't know; that's not a teaching question.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6043d426-bcee-4539-a9ea-4b7c4c3dccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Need to install  datasets  from pip, not conda. I'll do all from pip. \n",
    "#+ I'll get rid of the current conda environment and make it anew.\n",
    "#+ Actually, I'll make sure  conda  and  pip  are updated, then do what\n",
    "#+ I discussed above.\n",
    "#+\n",
    "#+ cf. \n",
    "#+     arch_ref_1 = \"https://web.archive.org/web/20240522150357/\" + \\\n",
    "#+                  \"https://stackoverflow.com/questions/77433096/\" + \\\n",
    "#+                  \"notimplementederror-loading-a-dataset-\" + \\\n",
    "#+                  \"cached-in-a-localfilesystem-is-not-suppor\"\n",
    "#+\n",
    "#+ Also useful might be\n",
    "#+     arch_ref_2 = \"https://web.archive.org/web/20240522150310/\" + \\\n",
    "#+                  \"https://stackoverflow.com/questions/76340743/\" + \\\n",
    "#+                  \"huggingface-load-datasets-gives-\" + \\\n",
    "#+                  \"notimplementederror-cannot-error\"\n",
    "#\n",
    "data_files = {'train':'samsum-train.json', 'test':'samsum-test.json'}\n",
    "dataset = load_dataset('json', data_files=data_files)\n",
    "\n",
    "model_name = \"google/flan-t5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "#  Next line makes training faster but a little less accurate\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                          trust_remote_code=True)\n",
    "\n",
    "#  padding instructions for the tokenizer\n",
    "#+   ??? !!! What about for RWKV !!! ???\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc6b24-d617-48a1-8236-c3be16402c4a",
   "metadata": {},
   "source": [
    "#### Trying some things I've been learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a107cbdc-5a73-4371-8afe-faa2dc623c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 512)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 6)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-7): 7 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
      "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
      "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59156663-4b5b-4b60-8b37-627a9f297fc9",
   "metadata": {},
   "source": [
    "## Prompt and Trainer\n",
    "\n",
    "For our SFT (<b>S</b>upervised <b>F</b>ine <b>T</b>uning) model, we use the `class trl.SFTTrainer`.\n",
    "\n",
    "I want to research this a bit, especially the `formatting_func` that we'll be passing to the `SFTTrainer`.\n",
    "\n",
    "First, though, some information about SFT. From the Hugging Face Documentation at https://huggingface.co/docs/trl/en/sft_trainer ([archived](https://web.archive.org/web/20240529140717/https://huggingface.co/docs/trl/en/sft_trainer))\n",
    "\n",
    "> Supervised fine-tuning (or SFT for short) is a crucial step in RLHF. In TRL we provide an easy-to-use API to create your SFT models and train them with few lines of code on your dataset.\n",
    "\n",
    "Though I won't be using the examples unless I get even more stuck, the next paragraph _has_ examples, and I'll put the paragraph here.\n",
    "\n",
    "> Check out a complete flexible example at [examples/scripts/sft.py](https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py) \\[[archived](https://web.archive.org/web/20240529140740/https://github.com/huggingface/trl/blob/main/examples/scripts/sft.py)\\]. Experimental support for Vision Language Models is also included in the example [examples/scripts/vsft_llava.py](https://github.com/huggingface/trl/blob/main/examples/scripts/vsft_llava.py) \\[[archived](https://web.archive.org/web/20240529140738/https://github.com/huggingface/trl/blob/main/examples/scripts/vsft_llava.py)\\].\n",
    "\n",
    "RLHF ([archived wikipedia page](https://web.archive.org/web/20240529142205/https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback)) is <b>R/b>einforcement <b>L</b>earning from <b>H</b>uman <b>F</b>eedback. [TRL](https://huggingface.co/docs/trl/en/index#:~:text=TRL%20is%20a%20full%20stack,Policy%20Optimization%20(PPO)%20step.) ([archived]())       <b>T</b>ransfer <b>R</b>einforcement <b>L</b>earning, a library from Hugging Face.\n",
    "\n",
    "For the parameter, `formatting_func`, I can look ath the documentation site above (specifically [here](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=formatting_func%20(Optional)), at the GitHub repo for [the code](https://github.com/huggingface/trl/blob/main/trl/trainer/sft_trainer.py) (in the docstrings), or from my local `conda` environment, at `C:\\Users\\bballdave025\\.conda\\envs\\rwkv-lora-pat\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py`.\n",
    "\n",
    "Pulling code from the last one, I get\n",
    "\n",
    ">         formatting_func (`Optional[Callable]`):\n",
    ">            The formatting function to be used for creating the `ConstantLengthDataset`.\n",
    "\n",
    "That matches the first very well\n",
    "\n",
    "> <b>formatting_func</b> (`Optional[Callable]`) — The formatting function to be used for creating the `ConstantLengthDataset`.\n",
    "\n",
    "(A quick note: In this Jupyter Notebook environment, I could have typed `trainer = SFTTrainer(` and then <kbd>Shift</kbd> + <kbd>Tab</kbd> to find that same documentation.\n",
    "\n",
    "However, I think that more clarity is found at the [documentation for `ConstantLengthDataset](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=class%20trl.trainer.ConstantLengthDataset)\n",
    "\n",
    "> <b>formatting_func</b> (`Callable`, <b>optional</b>) — Function that formats the text before tokenization. Usually it is recommended to have follows a certain pattern such as `\"### Question: {question} ### Answer: {answer}\"`\n",
    "\n",
    "So, as we'll see the next code from  the tutorial, it basically is a prompt templater/formatter that matches the JSON. For example, we use `sample['dialogue']` to access the `dialogue` key/pair. That's what I got from all this stuff.\n",
    "\n",
    "Mehul Gupta himself stated\n",
    "\n",
    "> Next, using the Input and Output, we will create a prompt template which is a requirement by the SFTTrainer we will be using later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cdd3ac-f714-41ed-a1fb-c27395b9251d",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fcb36fc-783f-4f19-b213-69cc3dbf05af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_instruction_format(sample):\n",
    "    return f\"\"\" Instruction:\n",
    "      Use the Task below and the Input given to write the Response:\n",
    "\n",
    "      ### Task:\n",
    "      Summarize the Input\n",
    "\n",
    "      ### Input:\n",
    "      {sample['dialogue']}\n",
    "\n",
    "      ### Response:\n",
    "      {sample['summary']}\n",
    "      \"\"\"\n",
    "##endof:  prompt_instruction_format(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3622673e-df80-43da-9d5c-efd6085d301d",
   "metadata": {},
   "source": [
    "### Trainer - with LoRA setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da39f5e-4e14-4eb5-ac66-c90a77472c64",
   "metadata": {},
   "source": [
    "#### Arguments and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ac040cf-2eb5-4eb5-91c7-2e47c54b990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Some arguments to pass to the trainer\n",
    "training_args = TrainingArguments( output_dir='output',\n",
    "                                   num_train_epochs=1,\n",
    "                                   per_device_train_batch_size=4,\n",
    "                                   save_strategy='epoch',\n",
    "                                   learning_rate=2e-4\n",
    ")\n",
    "\n",
    "# the fine-tuning (peft for LoRA) stuff\n",
    "peft_config = LoraConfig( lora_alpha=16,\n",
    "                          lora_dropout=0.1,\n",
    "                          r=64,\n",
    "                          bias='none',\n",
    "                          task_type='CAUSAL_LM'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa2a84-c5f6-45ad-9d09-fb9f25fae9da",
   "metadata": {},
   "source": [
    "`task_type`, cf. https://github.com/huggingface/peft/blob/main/src/peft/config.py#L222\n",
    "\n",
    ">        Args:\n",
    ">            peft_type (Union[[`~peft.utils.config.PeftType`], `str`]): The type of Peft method to use.\n",
    ">            task_type (Union[[`~peft.utils.config.TaskType`], `str`]): The type of task to perform.\n",
    ">            inference_mode (`bool`, defaults to `False`): Whether to use the Peft model in inference mode.\n",
    "\n",
    "After some searching using Cygwin\n",
    "\n",
    "```\n",
    "bballdave025@MYMACHINE /cygdrive/c/Users/bballdave025/.conda/envs/rwkv-lora-pat/Lib/site-packages/peft/utils\n",
    "$ ls -lah\n",
    "total 116K\n",
    "drwx------+ 1 bballdave025 bballdave025    0 May 28 21:09 .\n",
    "drwx------+ 1 bballdave025 bballdave025    0 May 28 21:09 ..\n",
    "-rwx------+ 1 bballdave025 bballdave025 2.0K May 28 21:09 __init__.py\n",
    "drwx------+ 1 bballdave025 bballdave025    0 May 28 21:09 __pycache__\n",
    "-rwx------+ 1 bballdave025 bballdave025 8.0K May 28 21:09 constants.py\n",
    "-rwx------+ 1 bballdave025 bballdave025 3.8K May 28 21:09 integrations.py\n",
    "-rwx------+ 1 bballdave025 bballdave025  17K May 28 21:09 loftq_utils.py\n",
    "-rwx------+ 1 bballdave025 bballdave025 9.7K May 28 21:09 merge_utils.py\n",
    "-rwx------+ 1 bballdave025 bballdave025  25K May 28 21:09 other.py\n",
    "-rwx------+ 1 bballdave025 bballdave025 2.2K May 28 21:09 peft_types.py\n",
    "-rwx------+ 1 bballdave025 bballdave025  21K May 28 21:09 save_and_load.py\n",
    "\n",
    "bballdave025@MYMACHINE /cygdrive/c/Users/bballdave025/.conda/envs/rwkv-lora-pat/Lib/site-packages/peft/utils\n",
    "$ grep -iIRHn \"TaskType\"\n",
    "peft_types.py:60:class TaskType(str, enum.Enum):\n",
    "__init__.py:20:# from .config import PeftConfig, PeftType, PromptLearningConfig, TaskType\n",
    "__init__.py:22:from .peft_types import PeftType, TaskType\n",
    "\n",
    "bballdave025@MYMACHINE /cygdrive/c/Users/bballdave025/.conda/envs/rwkv-lora-pat/Lib/site-packages/peft/utils\n",
    "$\n",
    "```\n",
    "\n",
    "So, let's look at\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c6de9-8841-47b3-b337-d6f339c6e82d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c5cd858-f6e7-4dba-b689-75e3d950f26d",
   "metadata": {},
   "source": [
    "### Try for a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68250f44-daf6-436e-a7d2-dcb7a8d97ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a760e7-6270-4e65-93f0-1af0c3dd559e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c550c2-eee7-4393-9f92-1a25f4fcc2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c903b1fb-6fd2-4e63-b7cd-73862898f4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf26b8c-3103-4a4c-a3ff-80f6ad075655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f18fb-3541-4bf1-987d-9cbb73bd8ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4feb5c-bb3c-444d-8814-6efa2317ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "tic = timeit.default_timer()\n",
    "\n",
    "toc = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d523e8-54bb-4d24-9882-1381d2f6e9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d7177-b914-4c49-bdcb-56164a67b4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae11f63-6b3c-4f7c-ad05-3364314f46bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0b919c-5a80-49c2-bc12-efaae04eb228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13373d1f-d71f-49b8-b415-94a490e97fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6a9ff4-2540-4ca7-90e8-82509fe2531d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
